{"config":{"lang":["en","fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"administration/admin_walkthrough/","title":"User Interface Walkthrough for Administrators","text":""},{"location":"administration/admin_walkthrough/#user-interface-walkthrough-for-administrators","title":"User Interface Walkthrough for Administrators","text":"<p>In the user interface, there is a menu on the left-hand side. We will be walking through each option under the \"Administration\" topic as well as some other features:</p> <p></p>"},{"location":"administration/admin_walkthrough/#api-keys","title":"API keys","text":"<p>The \"API Keys\" page allows system administrators to effectively manage all user API keys through this search interface. Features like search, filters, and sorting options enable quick location and organization of keys. This comprehensive view facilitates the tracking and auditing of API key of all users across the entire system with minimal effort.</p> <p></p> <p>Using the detail page above, administrators can modify or delete existing API keys.</p> <p>On the API Key detail page, administrators access detailed information about a specific API key, with options to edit or delete it as necessary. This streamlined approach ensures each key is up-to-date and secure, and that it adheres to the organization's access requirements and security standards.</p>"},{"location":"administration/admin_walkthrough/#error-viewer","title":"Error Viewer","text":"<p>The \"Error Viewer\" page is an important feature in Assemblyline in terms of service management and development. The errors that you see here are for services only, so you won't see any errors taking place in core components on this page.</p> <p>You can see the distribution of errors over a given time range, you can filter for all errors from a specific service by using a Lucene query in the filter bar or by clicking on a service column in the \"Service Error Distribution\" graph, and you can do the same for a specific error type via the filter bar or the \"Error Type Distribution\" graph.</p> <p>The buttons located on the right of the filter bar can apply pre-defined queries such as unhandled exceptions, canceled tasks, and maximum depth, level, or retries errors. When you select an error card, a side-drawer appears with the details for that error:</p> <p></p> <p>By the looks of this error message, the NetRep service had difficulty analyzing the related file to the point where the service timed out. All Assemblyline services retry two times on service timeouts, after which a non-recoverable failure is raised with the phrase \"Task Pre-empted\". A service developer can look at this error and try to reproduce it locally in their development environment, or a service administrator can raise it to the service owners for them to debug.</p>"},{"location":"administration/admin_walkthrough/#file-identification","title":"File Identification","text":"<p>Assemblyline does not look at file names or file extensions to determine the file type of a file. Instead, it uses file class analysis to assign an Assemblyline file type to each file that it sees, so that this file can be routed to the specific service that is tailored for that file type. File identification is one of the most important features of Assemblyline since the entire analysis of a file relies on the type that it is identified as. The main file in charge of identification can be found here.</p> <p>Assemblyline's file class analysis can be broken into a few distinct aspects:</p> <ul> <li>Magic</li> <li>MIME</li> <li>Yara</li> </ul> <p>An example of where file identification lives in Assemblyline is on the \"File Details\" page:</p> <p></p> <p>You can see that Assemblyline's file type is present as the \"Type\": <code>document/office/onenote</code>. The MIME type is <code>application/octect-stream</code> and the Magic label is a custom value <code>custom: document/office/onenote</code>. Custom Magic labels will be explained later.</p> <p>In most cases, a combination of Magic and MIME is enough to produce an identification for a file. We use a subset of Magic labels and MIME types that are high-confidence and label them as \"trusted\", which you can see in the \"TRUSTED MIMES\" and \"MAGIC PATTERNS\" tabs.</p> <p>In the \"TRUSTED MIMES\" view, you can see the MIME types that are high-confidence and are mapped directly to the Assemblyline file type:</p> <p></p> <p>Trusted Magic labels are a little trickier, so we use high-confidence regular expressions to map labels to Assemblyline file types, as seen in the \"MAGIC PATTERNS\" view:</p> <p></p> <p>On top of this, we use custom Magic rules to apply labels to a file, as seen in the \"LIBMAGIC\" tab. This is where that <code>custom: document/office/onenote</code> label came from for the OneNote file mentioned above.</p> <p></p> <p>This is all fine and dandy, but some files do not have obvious metadata or headers where Magic labels and MIME types can be applied, like scripts. This is where the Yara rules come into play.</p> <p></p> <p>These Yara rules are designed to look for high-confidence pieces of code that are language-specific to determine the file type of the script.</p> <p>The best part about all these tabs on the \"File Identification\" page is that they are all customizable! If you see a file that is being identified incorrectly, you can easily add either a custom Magic label, a trusted MIME type, a regular expression for Magic labels, or a Yara rule.</p>"},{"location":"administration/admin_walkthrough/#post-process-actions","title":"Post-process actions","text":""},{"location":"administration/admin_walkthrough/#services","title":"Services","text":""},{"location":"administration/admin_walkthrough/#service-review","title":"Service Review","text":"<p>You can find the service review interface by clicking the Administration topic then choose Service Review subtopic.</p> <p></p> <p>This is a page where you can compare different versions of a service and how it behaved in terms of the number of results processed, the average score, the average number of extracted and supplementary files, etc.</p> <p>Here is an example of comparing different versions of the APIVector service:</p> <p></p>"},{"location":"administration/admin_walkthrough/#site-map","title":"Site Map","text":"<p>You can find the site map interface by clicking the Administration topic then choose the Site Map subtopic.</p> <p></p> <p>This page is useful for seeing how the URL endpoints map to Python client functions, the method types of these functions, and the RBAC roles required to use them.</p> <p></p>"},{"location":"administration/admin_walkthrough/#system-safelist","title":"System Safelist","text":""},{"location":"administration/admin_walkthrough/#users","title":"Users","text":"<p>We have made it through the Administration subtopics, now let's look at over useful features when managing Assemblyline.</p>"},{"location":"administration/admin_walkthrough/#signature-management","title":"Signature Management","text":""},{"location":"administration/admin_walkthrough/#heuristic-management","title":"Heuristic Management","text":""},{"location":"administration/admin_walkthrough/#source-management","title":"Source Management","text":""},{"location":"administration/admin_walkthrough/#workflow-management","title":"Workflow Management","text":"<p>Previously, we talked about workflows on the Alerts page, which could be applied a single time on all alerts currently in the system. The \"Workflows\" page is where you can create workflows that are persistent and will run on all alerts currently in the system as well as all new alerts that come in.</p> <p></p>"},{"location":"administration/admin_walkthrough/#dashboard","title":"Dashboard","text":"<p>Now let's look at one last feature that is incredibly useful for an administrator, the Dashboard page.</p> <p></p> <p>This is the ultimate visualization of what is going on with the Assemblyline system. To the point where there is so much information, we must hide it behind cute buttons with tooltips. It is overwhelming at first, but once you know what to look for, using this page is crucial to Assemblyline administration.</p> <p>This page can be separated into two main sections: Core and Services cards.</p> <p></p> <p></p> <p>Core cards represent core components described in Design and Architecture and Service cards represent services. Let's start by looking at the Ingestion component card.</p> <p></p> <p>Here are those cute buttons that I was talking about. The small box with a Q in it has a tooltip if you hover your mouse over it which explains what the value next to it represents:</p> <p></p> <p>You can hover over the other buttons to discover what everything means.</p> <p>For Service cards, the following graphic describes what is going on for each service:</p> <p></p> <p>The following are useful pages to manage and troubleshoot your Assemblyline deployment.</p>"},{"location":"administration/admin_walkthrough/#system-management","title":"System Management","text":""},{"location":"administration/admin_walkthrough/#troubleshooting","title":"Troubleshooting","text":""},{"location":"administration/architecture/","title":"System Architecture","text":""},{"location":"administration/architecture/#system-architecture","title":"System Architecture","text":"<p>The Assemblyline system architecture is somewhat complex to understand because there are many moving pieces. In this section, we will describe every component of Assemblyline and will give you visual examples of how the different components interact with each other. We will do that by first setting the scene by describing a scenario done in Assemblyline and then describing all the components in that scenario.</p>"},{"location":"administration/architecture/#submitting-files-to-assemblyline","title":"Submitting files to Assemblyline","text":"<p>Here's how submitting files to Assemblyline takes place. The user browses to the User Interface (UI) Frontend and submits a file through the web interface. The frontend takes that file and hands it off to the API Server. Alternatively, the user uses the Assemblyline client to connect directly to the API server and submits a file via the client. Once the API server receives the file, it saves the file to the Filestore and creates a tasking message in Redis (volatile) for the Dispatcher to pick up.</p> <p>The Dispatcher will be notified by Redis that a new task has come in, will identify the file, and will route that file to the appropriate services by pushing a new message for the services in its service queue.</p> <p>Services long poll the Service Server API, and when a new task enters their queue, they will get the file from the Service Server and start processing it. Once they are done, they will send their analysis results to the Service Server along with any embedded or supplementary files they have collected during analysis. The analysis results will be saved into the database by the Service Server and the new embedded files will be sent back to the Dispatcher via Redis so that it can dispatch them to a new set of services for reprocessing.</p> <p>Once all associated files are done processing, the Dispatcher will mark the submission as complete in the database.</p> <p></p>"},{"location":"administration/architecture/#alternate-submission-method","title":"Alternate Submission Method","text":"<p>If the ingestion API is used instead of the submit API, an asynchronous process is started. Instead of going directly to the Dispatcher, the Submission will be queued in Redis (persistent). The Ingester process will monitor that queue and will slowly feed the Dispatcher. This alternate method is for a high volume of submissions. Instead of having a million files processed at the same time in the system, they are queued and processed in batches by priority. This ensures the system can survive giant bursts of files at the same time without falling over.</p>"},{"location":"administration/architecture/#alternate-service-process","title":"Alternate Service Process","text":"<p>Some services may run in \"privileged\" mode. This means that they are allowed to bypass the Service Server (which makes them much faster) and pull their tasks directly from Redis. Privileged services can also save their analysis results directly into the Datastore and save their embedded and supplementary files directly into the Filestore.</p> <p>Running in privileged mode is only recommended for services that do not execute the files directly on their containers because these services have complete access to all core components instead of only having access to the Service Server.</p>"},{"location":"administration/architecture/#file-submission-components","title":"File Submission Components","text":""},{"location":"administration/architecture/#ui-frontend","title":"UI Frontend","text":"<p>This is the component that, as a user, you will be the most familiar with. This component provides the UI of Assemblyline. This is the first thing that you see once you deploy Assemblyline and the component that you interact with the most. Assemblyline's frontend is built using React with a Material UI theme sitting on top of it. It supports light and dark themes as well as internationalization.</p> <p></p>"},{"location":"administration/architecture/#ui","title":"UI","text":"<p>A previous iteration of the UI component provided both the User Interface and API to Assemblyline. The current iteration of the UI component does not provide the User Interface anymore. The User Interface code was moved to the UI Frontend component when the interface switched to React.</p> <p>Nowadays, the UI component provides the user-facing API server of Assemblyline as well as a SocketIO Server for live messaging. The API Server is responsible for identifying the user either with an internal list of users or by communicating with your LDAP or OAuth server. It keeps track of the user sessions to make sure users only get access to the APIs they are allowed to. It also does document-level access control to make sure that the data returned by the APIs is data that the user is allowed to see.</p>"},{"location":"administration/architecture/#dispatcher","title":"Dispatcher","text":"<p>The Dispatcher is the core tasking component of the system. It checks the type of each file submitted to the system and routes each file to the appropriate service depending on service availability and file type. It keeps track of children for a given file (ZIP extraction, etc.) and ensures that a submission is not completed until all the children have been processed and all files have been sent to the appropriate services. The Dispatcher keeps track of errors in the system and re-queues jobs if it detects a recoverable failure. It is the Dispatcher\u2019s job to mark a submission as completed when all work is done. The Dispatcher does all its queuing using non-persistent Redis queues. If the Dispatcher is restarted, all inflight submissions are restarted from the beginning. This is usually not a problem because Assemblyline has service-level caching.</p> <p>The Dispatcher also keeps metrics on how many files are being completed in the system over a given interval. This information can be observed from the Dashboard in the UI.</p>"},{"location":"administration/architecture/#ingester","title":"Ingester","text":"<p>The Ingester is Assemblyline\u2019s high-volume ingestion component. It takes all submissions created using the ingest API and sorts them into different priority queues (Critical, High, Medium, and Low). It then fills half of the Dispatcher's maximum inflight queue with submissions starting with the highest priority queues and continuing until all queues are exhausted.</p> <p>The priority queues are starving queues. All critical submissions are processed before starting high submissions and so on...</p> <p>The Ingester can also deal with impossible-to-finish backlogs. When the queues reach a certain threshold, the Ingester will start sampling the queues using a method that is increasingly aggressive in proportion to the size of the backlog to randomly remove submissions from the priority queues to ensure that the queues don't keep growing forever.</p> <p>The Ingester also ensures that work isn\u2019t duplicated by deduplicating submissions before it sends them to the Dispatcher. If provided, it can also apply regular expressions to the metadata of the submissions that perform safelisting.</p> <p>Metrics are reported on the number of duplicates, ingested, safelisted, and completed files as well as the number of bytes ingested and completed.</p>"},{"location":"administration/architecture/#service-server","title":"Service Server","text":"<p>The Service Server has been introduced to Assemblyline 4 to isolate the services from the core infrastructure. It includes APIs that the service \"Task Handler\" uses to get tasks, publish results, download files for analysis, publish embedded and supplementary files, and get access to the system safelist. This is everything that a service needs to process the file properly without knowing anything about the infrastructure. The Service Server APIs are only accessible by the services and nothing else in the system.</p>"},{"location":"administration/architecture/#remote-datatypes","title":"Remote Datatypes","text":"<p>Assemblyline uses a bunch of containers running on different hosts that need live access to shared data structures. This is where the remote datatypes come into play, these are data structures stored in Redis that are available to all processes in Assemblyline.</p> <p>Assemblyline instantiates connection to two different Redis instances:</p> <ul> <li>Redis persistent: Journal file backed-up database on a persistent drive, even if the system crashes or reboots, the content of this Redis version is always available.</li> <li>Redis volatile: In-memory instance is much faster to interface with but if the Redis volatile container crashes or restarts, all its content is lost.</li> </ul> <p>We have a wide range of supported data types to account for various scenarios:</p> <ul> <li>Counters: Metrics gathering</li> <li>Event dispatcher/handlers: Registers callback functions for events that happen system-wide</li> <li>Hash: Store currently processing items</li> <li>Global Locks: Concurrency locks</li> <li>Sets: Service and submission priority-based queues</li> <li>Quota trackers: Track user's submission and API quotas</li> <li>Queues (Pubsub/Fifo/Priority): Messaging between components</li> </ul>"},{"location":"administration/architecture/#datastore","title":"Datastore","text":"<p>The Assemblyline Datastore component is the database where we store analysis results. While the Datastore was originally built as generic as possible, it currently pretty much only works with Elasticsearch since we've tied in a lot of Elasticsearch-specific features to make it faster. It could be made to work on projects derived from Elasticsearch, like OpenSearch for example.</p> <p>The Datastore is there to ensure a stable connection to the Elasticsearch backend with auto keep-alive and retries, easy index management, and sync with the code as well as support for all the basic features like <code>get</code>, <code>put</code>, <code>update</code>, <code>search</code>, <code>facet</code>, <code>stats</code>, <code>histogram</code> ...</p>"},{"location":"administration/architecture/#filestore","title":"Filestore","text":"<p>Assemblyline's Filestore is where all files are stored. The Filestore implementation allows for multiple types of Filestores to be used such as:</p> <ul> <li>HTTP (read-only)</li> <li>FTP/SFTP</li> <li>Amazon S3 / Minio</li> <li>Azure Blob storage</li> <li>Local storage</li> </ul> <p>It also has the concept of a multi-level Filestore so the files can be written to multiple locations at the same time.</p> <p>By default, Assemblyline uses Minio, which is an Amazon S3-compatible file storage engine.</p>"},{"location":"administration/architecture/#generating-alerts","title":"Generating Alerts","text":"<p>Assemblyline doesn't just analyze files for malicious content; it can also function as a triage environment. In this mode, all files are sent to Assemblyline, and alerts are generated based on predefined rules, so analysts only focus on files that meet these alert criteria. This section explains the process of generating alerts within Assemblyline.</p>"},{"location":"administration/architecture/#how-alerts-are-generated","title":"How Alerts are Generated","text":"<p>Alerts are created based on Post-Process actions that define what constitutes an alert (default: a file with a score over 500). Here's how the alert generation process works step by step:</p> <ol> <li> <p>Submission Scoring and Rule Evaluation:</p> <ul> <li>While the Dispatcher writes results for a submission, it evaluates rules to determine if an alert should be generated.</li> </ul> </li> <li> <p>Alert Message Creation:</p> <ul> <li>If the criteria for an alert are met, the Dispatcher writes a message to Redis' persistent database to generate an alert for the submission.</li> </ul> </li> <li> <p>Alerter Component:</p> <ul> <li>The Alerter component picks up this message from Redis and converts the related submission and its results into an alert, saving the alert in the Datastore.</li> </ul> </li> <li> <p>Workflow Component:</p> <ul> <li>The Workflow component processes newly created alerts, applying labels, priority, and status to alerts that match user-defined workflow queries.</li> </ul> </li> </ol> <p></p> <p>Below are detailed descriptions of the components involved in generating alerts.</p>"},{"location":"administration/architecture/#alerter-component","title":"Alerter Component","text":"<ul> <li>Function: The Alerter monitors the Redis persistent alert input queue, transforms relevant submissions and results into alerts, and saves these alerts in the Datastore.</li> <li>Alert Contents: An alert consists of specific import tags with their type and verdict, heuristic triggers, ATT&amp;CK matrix categories, submission metadata, file information (hashes, size, type, name), and alert-specific information (timestamps, verdicts, owner, labels, priority, status).</li> </ul> <p>In certain scenarios, an alert can originate from another alert and be reprocessed with a different set of services. The criteria for reprocessing are defined in the Post-Process actions.</p>"},{"location":"administration/architecture/#workflow-component","title":"Workflow Component","text":"<ul> <li>Function: The Workflow component applies user-defined workflows to newly created alerts. When an alert matches a specified workflow, it assigns the corresponding labels, priority, and status to the alert in the Datastore.</li> <li>Usage: This allows for automatic categorization and prioritization of alerts based on predefined conditions, ensuring that critical alerts get immediate attention.</li> </ul>"},{"location":"administration/architecture/#housekeeping","title":"Housekeeping","text":"<p>Assemblyline also includes a bunch of housekeeping processes that perform different tasks in the system that are not immediately related to the file processing and alerting process.</p> <p>These processes perform tasks like:</p> <ul> <li>Remove data where TTL has expired</li> <li>Gather the system metrics</li> <li>Scale service instances to process the current load</li> <li>Update services</li> <li>Generate statistics on signatures and heuristics</li> <li>Clean up tasks from service queues when a service is disabled/deleted</li> </ul> <p>We'll describe all these housekeeping processes and show how they relate to the different infrastructure components.</p>"},{"location":"administration/architecture/#expiry","title":"Expiry","text":"<p>The Expiry process is tasked to monitor for documents that have past their expiry date and remove them from the system. It does this by searching in the Datastore for documents that have expiry dates (<code>expiry_ts</code>) earlier than the current date, then deletes the associated records from the Datastore. If there are also associated files to the document in the Filestore, Expiry will delete those files too.</p> <p></p>"},{"location":"administration/architecture/#heartbeat-and-metrics","title":"Heartbeat and Metrics","text":"<p>All core components in Assemblyline generate some sort of metrics so that we can track their performance and see if the system is alive and well. Those metrics are sent to a message queue in Redis volatile which is then read by the Metrics and Heartbeat containers. The Heartbeat container will aggregate those metrics and send them back to the SocketIO server for immediate consumption in the Frontend and the Metrics container will aggregate those metrics as well but will store them in the logging ELK (Elasticsearch + Logstash + Kibana) stack for consumption by administrators after the fact.</p> <p></p>"},{"location":"administration/architecture/#scaler","title":"Scaler","text":"<p>The Scaler looks at the busyness level and a service's queue of items to process in Redis volatile, checks the available resources on the host(s) then determines the optimal amount of each service that should run right now to get through the current load of files to process. This component instructs Kubernetes or Docker to load or unload service containers accordingly.</p> <p></p>"},{"location":"administration/architecture/#statistics","title":"Statistics","text":"<p>Every hour, the Statistics container runs facet queries in the Datastore to find out how many times heuristics/signatures were used and saves those stats to the respective signatures/heuristics in the Datastore.</p>"},{"location":"administration/architecture/#plumber","title":"Plumber","text":"<p>The Plumber process cleans up tasks from the service queues when a service is disabled/deleted. When a service is turned off by the Scaler or deleted by the user, the service task queue needs to be emptied. The status of all the services will be periodically checked and any service that is found to be disabled or deleted for which a service queue exists, the Dispatcher will be informed that the task(s) had an error.</p>"},{"location":"administration/architecture/#updating-the-system","title":"Updating the system","text":"<p>To keep the system up to date, two critical components come into play:</p> <ul> <li>Updater: to update the different containers in the system</li> <li>Service updater: to update the different signature sets of a given service</li> </ul>"},{"location":"administration/architecture/#updater","title":"Updater","text":"<p>The Updater checks external container registries (Azure Container Registry, Docker Hub, Harbor, ... ) for a new container image for a service based on the container tag. When new image versions are found, it launches the new ephemeral container to register the service. It then notifies the Scaler via Redis volatile that a new service version is available. The Scaler instructs Kubernetes / Docker to replace all service instances with the new service version by re-creating the service containers.</p> <p>The Updater also advertises if there are new services available that you can install.</p>"},{"location":"administration/architecture/#service-updater","title":"Service Updater","text":"<p>Each service that updates the signatures or safelist has its own updater. Only one updater is launched for all associated services. The Service Updater fetches updates from external links defined in its configuration (<code>git</code>, <code>HTTP</code>, ...). It then saves the updated signature/safelist to the database via the internal UI container. The internal UI container is an API server dedicated to processing requests for core components. This container's API isn't accessible outside of the system, unlike the UI container that the Frontend uses.</p> <p>When the services launch, they pull their signature set from the service updater which in turn asks the internal UI for the signatures committed to the database.</p> <p></p>"},{"location":"administration/architecture/#keeping-files-forever-malware-archive","title":"Keeping files forever (Malware Archive)","text":"<p>Malware Archive is a new feature for Assemblyline that allows users to preserve important documents forever. To accomplish this, archived filestore and datastore indices have been defined where the stored documents do not have an expiry date and will not be deleted by the expiry process. The new core component <code>Archiver</code> was added to move the file and the analysis over to the Malware Archive.</p>"},{"location":"administration/architecture/#archiver","title":"Archiver","text":"<p>The Archiver process is the core component that archives the files and documents. When a user or a system requests a submission to be archived, a message is created in the Redis (volatile) message broker. The Archiver listens to those messages and is tasked with copying the file to the archived filestore and the related submissions in the datastore to their archive indices.</p>"},{"location":"administration/architecture/#yara-back-in-time-retrohunt","title":"YARA back in time (Retrohunt)","text":"<p>Retrohunt is a new feature that allows users to scan the collection of submitted files in Assemblyline using their own YARA rules. When a user submits a request to create a new search job, the UI will task the new component called <code>Haunted House</code> to process the hunting of the files and inform the user on the progress of the job. The resulting file hits of the search job are stored in the Datastore which makes them easily accessible and minimizes the interaction with Haunted House.</p>"},{"location":"administration/architecture/#haunted-house","title":"Haunted House","text":"<p>Haunted House is composed of three components called the \"Python Client\", the \"Server\" and the \"Worker\", the latter two being written in Rust. The Python Client is the intermediary between the Assemblyline UI and the Rust Server. It receives the requests from the UI to either start a new search, repeat an existing search or receive status updates on an in-progress job. The Rust Server is responsible for tasking a multitude of Workers to match the YARA rules provided to the files.</p>"},{"location":"administration/architecture/#work-online-continue-offline","title":"Work online, continue offline","text":"<p>An Assemblyline instance can continue the work that another Assemblyline instance has started. This allows you to start a scan on an instance of Assemblyline that is online (connected to the internet) and gather as much information possible on a file using these internet sources. Then ship the full analysis bundle to another instance of Assemblyline that is on an offline network (NOT connected to the internet) and continue the analysis with extra service that you may not want to deploy on your online version.</p>"},{"location":"administration/architecture/#replay-creator","title":"Replay Creator","text":"<p>The Replay Creator component creates analysis bundles based on specific criteria and saves those bundles in a directory so they can be grabbed by any process that you must transfer files over from online to offline networks.</p>"},{"location":"administration/architecture/#replay-loader","title":"Replay Loader","text":"<p>The Replay Loader component loads bundle files from a directory and tells Dispatcher to continue the scan of the associated files with services that have a different version / configuration between online and offline or with services that were simply not on the server from which the bundle was created.</p>"},{"location":"administration/heuristic_management/","title":"Heuristic Management","text":""},{"location":"administration/heuristic_management/#heuristic-management","title":"Heuristic Management","text":"<p>Assemblyline's heuristic management interface lets you:</p> <ol> <li>List all heuristics in the system</li> <li>Filter and search the current set of heuristics</li> <li>View details about those heuristics</li> </ol> <p>The main difference between heuristics and signatures in Assemblyline is that heuristics are created by service writers, while signatures are written by trusted sources outside of Assemblyline such as CAPE, Suricata, Yara, and Sigma.</p> <p></p> <p>In this view, you can see the names of the heuristics, how often they have been seen, when they were last seen, and their classifications. By selecting one of these cards, you are brought to a detailed view of the heuristic:</p> <p></p> <p>Something interesting on this page, if you scroll down, is the \"Statistics\" section:</p> <p></p> <p>This graph allows you to visualize when certain files with the same features come into the system, which is useful in campaign attribution and monitoring.</p>"},{"location":"administration/service_management/","title":"Service Management","text":""},{"location":"administration/service_management/#service-management","title":"Service Management","text":"<p>Assemblyline's service management interface allows users to effectively manage services by performing actions such as listing, updating, adding, removing, and backing up services. This guide provides a step-by-step overview of how to manage services in Assemblyline.</p>"},{"location":"administration/service_management/#accessing-service-management","title":"Accessing Service Management","text":"<p>Navigate to the service management interface by selecting the Administration topic and choosing the Services subtopic.</p> <p></p>"},{"location":"administration/service_management/#service-list","title":"Service List","text":"<p>Upon loading the service management interface, you are presented with a list of all services in the system.</p> <p></p>"},{"location":"administration/service_management/#service-management-buttons","title":"Service Management Buttons","text":"<p>The top-right section of the service list page includes several important buttons:</p> <p></p> <ol> <li>Add Service: Add new services to the system.</li> <li>Update Services: Perform updates on existing services.</li> <li>Install All Services: Install all available services.</li> <li>Download Backup: Download a backup of the current services configurations.</li> <li>Restore Backup: Restore services configuration from a previously downloaded backup.</li> </ol>"},{"location":"administration/service_management/#adding-a-service","title":"Adding a Service","text":"<p>To add a new service:</p> <ol> <li>Click the green \"(+)\" button in the top-right corner.</li> <li>In the popup window, paste the new YAML-formatted service manifest contents.</li> <li>Click \"Add\" to incorporate the service into the system.</li> </ol> <p></p> <p>Tip</p> <p>The \"service-add\" API will automatically replace certain environment variables in your manifest:</p> <ul> <li>$SERVICE_TAG: Will be replaced by the latest tag for your current deployment type (dev/stable) found in the docker registry where the service container is hosted.</li> <li>$REGISTRY: Will be replaced by your local registry.</li> </ul>"},{"location":"administration/service_management/#updating-services","title":"Updating Services","text":"<p>As malware evolves, so too must the services that analyze and counteract these threats. Assemblyline's rapid development cycle ensures frequent updates for its supported services, enabling timely responses to new and emerging threats.</p> <p>The system automatically detects if a newer container version is available for your deployment type (dev/stable). An update button will be displayed on the service card for services that require updating.</p> <p></p> <p>Hover over the button to see the new version and click to kick off the update.</p>"},{"location":"administration/service_management/#installing-available-services","title":"Installing Available Services","text":"<p>The system tracks which services are installed and highlights additional services that are available for installation. To install all available services, click the \"Install all services\" button at the top right corner. This will prompt the system to install any services that you do not currently have.</p> <p>Alternatively, if you prefer to install services individually, scroll to the bottom of the service list page. Here, you will find a section listing all services that are available but not yet installed. Click the install button on the service card for each individual service you wish to add.</p> <p></p>"},{"location":"administration/service_management/#creating-and-restoring-service-config-backups","title":"Creating and Restoring Service Config Backups","text":"<p>Backup and restore functionality allows you to save and recover service configurations.</p> <p>To manage backups:</p> <ol> <li>Click the \"arrow pointing down\" button to download the current services configuration as a YAML file, named <code>&lt;FQDN&gt;_service_backup.yml</code>.</li> <li>To restore a backup, click the \"clock with counter-clockwise arrow\" button. Paste the backup content into the textbox and click \"Restore\" to reinstate the services configurations to their saved state.</li> </ol> <p></p>"},{"location":"administration/service_management/#service-listing-overview","title":"Service Listing Overview","text":"<p>The service listing table provides comprehensive information on each of the services configured in Assemblyline. Below is a detailed explanation of each column and the type of data it represents:</p> <p></p>"},{"location":"administration/service_management/#name","title":"Name","text":"<p>The Name column contains the name of the service. This is the identifier that you will use to refer to this service throughout the system.</p>"},{"location":"administration/service_management/#version","title":"Version","text":"<p>The Version column lists the current running version of the service.</p>"},{"location":"administration/service_management/#category","title":"Category","text":"<p>The Category column categorizes the services into predefined groups. This helps in organizing services and understanding the role each service plays. Available categories out-of-the-box include:</p> <ul> <li>Antivirus: Services that utilize built-in signatures to scan files for viruses and malware detection.</li> <li>Dynamic Analysis: Services that execute the file in a sandbox environment to observe behaviors.</li> <li>External: Services that rely on external data sources or submit data to external systems.</li> <li>Extraction: Services that extract files from archives or other compound file formats.</li> <li>Filtering: Services that eliminate certain files from being processed further.</li> <li>Internet Connected: Services that allow executed files to interact with the internet for additional analysis, such as receiving instructions or retrieving supplementary data.</li> <li>Networking: Services that analyze network traffic or related data.</li> <li>Static Analysis: Services that analyze files without executing them.</li> </ul>"},{"location":"administration/service_management/#stage","title":"Stage","text":"<p>The Stage column delineates at which point in the analysis pipeline a service will execute. The stages determine the sequence in which services operate, ensuring an organized and efficient workflow. Each service belongs to a single stage, executed in the following order:</p> <ul> <li>FILTER: Initial filtering of files (e.g., Safelist). This stage determines whether subsequent stages will analyze a file.</li> <li>EXTRACT: File extraction and unpacking (e.g., Extract, Unpacker). Services in this stage focus on extracting files from archives or other compound formats.</li> <li>CORE: Core analysis tasks. This stage is where the majority of services perform their analysis.</li> <li>SECONDARY: Additional analysis performed with context from previous stages (e.g., VirusTotal). These services offer further insights based on the initial core analysis.</li> <li>POST: Subsequent processing steps (e.g., URLCreator). Services in this stage act upon the results generated by earlier stages.</li> <li>REVIEW: This final stage involves services that review the entire context of the analysis before performing their own tasks (e.g., Ancestry).</li> </ul>"},{"location":"administration/service_management/#file-types","title":"File Types","text":"<p>The File Types column specifies the Assemblyline file types that the service will accept for processing. This is defined using regular expressions. For instance:</p> <ul> <li><code>.*</code> will accept all files for analysis.</li> <li>Specific regex patterns can be used to target certain Assemblyline file types, such as <code>executable/windows</code> for Windows executables.</li> </ul>"},{"location":"administration/service_management/#external","title":"External","text":"<p>The External column indicates whether the service sends data outside of Assemblyline's infrastructure. This is an important consideration for privacy and data security. An example is the VirusTotal service, which submits files to the VirusTotal platform for analysis.</p>"},{"location":"administration/service_management/#mode","title":"Mode","text":"<p>The Mode column specifies how the service operates, providing key information on its interaction with Assemblyline's core infrastructure. Here are the possible modes:</p> <ul> <li>Uses Service Server: These services utilize the Service Server, which isolates them from the core infrastructure. The Service Server provides APIs for task handling, result publishing, file downloading, and accessing the system safelist, ensuring that services have all necessary functionalities while remaining unaware of the underlying infrastructure.</li> <li>Runs in Privileged Mode: Services operating in this mode bypass the Service Server, directly pulling tasks from Redis. They can save analysis results into the Datastore and store embedded and supplementary files directly into the Filestore. This results in faster processing due to the direct access to core components.</li> </ul> <p>For more detailed information, please refer to the full documentation.</p>"},{"location":"administration/service_management/#classification","title":"Classification","text":"<p>The Classification column lists the classification level at which the service operates, which governs how the service handles and labels data.</p>"},{"location":"administration/service_management/#enabled","title":"Enabled","text":"<p>The Enabled column indicates if the service is currently enabled or disabled. An enabled service will have one or more instances running and actively process files, whereas a disabled service will not operate until re-enabled.</p>"},{"location":"administration/service_management/#service-details","title":"Service Details","text":"<p>The service details page allows you to visualize and customize various parameters of a service across multiple tabs.</p>"},{"location":"administration/service_management/#modifying-or-removing-a-service","title":"Modifying or Removing a Service","text":"<p>Click on any service from the service list to view its details. The service detail page allows you to:</p> <ul> <li>Delete the service: Red \"circled minus\" button.</li> <li>Toggle Enabled/Disabled state: Big square button above the tabs.</li> </ul>"},{"location":"administration/service_management/#general-tab","title":"General Tab","text":"<p>This tab allows you to view and modify general information about the service.</p> <p></p> <p>Here, you can adjust various general settings:</p> <ul> <li>Version: Change the service version.</li> <li>Description: Edit the service description.</li> <li>Execution Stage: Specify when the service should run.</li> <li>Category: Group the service under a specific category.</li> <li>Accepted/Rejected File Types: Define regular expressions for file types.</li> <li>Execution Timeout: Modify the time limit for service execution.</li> <li>Maximum Instances: Set the maximum number of service instances.</li> <li>Location: Configure the service location to be internal of external.</li> <li>Result Caching: Enable or disable result caching.</li> </ul> <p>Tip</p> <p>Refer to the service manifest documentation for more detailed information about these fields.</p>"},{"location":"administration/service_management/#container-tab","title":"Container Tab","text":"<p>The Container tab provides comprehensive information about the containers your service utilizes.</p> <p></p> <p>Within this tab, you can adjust the following settings:</p> <ul> <li>Update Channel: Select between Development or Stable channels for updates.</li> <li>Main Service Container: Modify the primary container that executes the service.</li> <li>Dependency Containers: Add, modify, or remove supporting containers.</li> </ul>"},{"location":"administration/service_management/#main-service-container","title":"Main Service Container","text":"<p>The main service container is the container housing and running the service code. By clicking on the main service container, you can modify the parameters used to launch it.</p> <p></p> <p>You can adjust the following settings:</p> <ul> <li>Container Image Name: Set the name of the container image.</li> <li>Container Registry Type: Specify the type of container registry.</li> <li>Resource Limits: Define CPU and RAM limits.</li> <li>Container Registry Credentials: Provide registry credentials (username/password).</li> <li>Command Execution: Change the command executed within the container.</li> <li>Internet Access: Enable or disable internet access for the container.</li> <li>Environment Variables: Set necessary environment variables before launching the container.</li> </ul> <p>Tip</p> <p>For more details on configuring the Docker block, refer to the docker config section in the service manifest documentation.</p>"},{"location":"administration/service_management/#dependency-containers","title":"Dependency Containers","text":"<p>Dependency containers support the main service by providing external resources (like a database) or facilitating updates. These containers are shared among multiple service instances.</p> <p>To add or modify a dependency container, click \"Add Dependency\" or select an existing container.</p> <p></p> <p>The dependency container configuration window allows you to modify similar settings as the main service container, with an added option for persistent storage.</p> <p>Tip</p> <p>Refer to the persistent volume section in the service manifest documentation to learn more about configuring persistent storage for dependency containers.</p>"},{"location":"administration/service_management/#updates-tab","title":"Updates Tab","text":"<p>The Updates tab displays information about the service's self-update mechanism or signature updates.</p> <p></p> <p>Warning</p> <p>This tab appears only if the service defines an update config block in its service manifest.</p> <p>In this tab, you can view and modify:</p> <ul> <li>Update Interval: Set the frequency of updates.</li> <li>Signature Generation: Indicate whether the service generates signatures.</li> <li>Startup Dependency: Specify if the service must wait for a successful update before starting instances.</li> <li>Update Sources: List the sources from which the service pulls its updates.</li> </ul> <p>Tip</p> <p>For more information on modifying signature sources, refer to the Modifying sources documentation.</p>"},{"location":"administration/service_management/#parameters-tab","title":"Parameters Tab","text":"<p>The Parameters tab allows customization of various service parameters.</p> <p></p> <p>Parameters are divided into two categories:</p> <ul> <li>User-Specified Parameters</li> <li>Service Variables</li> </ul>"},{"location":"administration/service_management/#user-specified-parameters","title":"User-Specified Parameters","text":"<p>These parameters can be adjusted by users for each specific submission in the system. They often include:</p> <ul> <li>Feature toggles for the service</li> <li>Passwords for specific submissions</li> <li>Limits on the service's actions</li> <li>Extraction sensitivity</li> </ul> <p>Tip</p> <p>When defined, these parameters appear in the submission options available to the user during submission.</p>"},{"location":"administration/service_management/#service-variables","title":"Service Variables","text":"<p>Service variables are configuration parameters specific to your deployment, aiding the service in running effectively. They often include:</p> <ul> <li>URLs for external services</li> <li>Credentials for external connections</li> <li>Default values used by the service</li> <li>Scanning capabilities configuration</li> </ul>"},{"location":"administration/service_management/#ocr-configuration","title":"OCR Configuration","text":"<p>Certain services conduct OCR (Optical Character Recognition) analysis on images either found in submissions or generated during the analysis. You can override or customize the default OCR terms defined in the service base by using the <code>ocr</code> key within the <code>config</code> block of the service manifest.</p>"},{"location":"administration/service_management/#simple-term-override-legacy","title":"Simple Term Override (Legacy)","text":"<p>To use a custom set of terms for <code>ransomware</code> detection, you can configure the following in your service manifest:</p> <pre><code>config:\n    ocr:\n        ransomware: ['bad1', 'bad2', ...]\n</code></pre> <p>This configuration will instruct the service to only use the specified terms for <code>ransomware</code> detection, adhering to the default hit threshold as defined in the service base.</p>"},{"location":"administration/service_management/#advanced-term-override","title":"Advanced Term Override","text":"<p>If you need to use custom terms for <code>ransomware</code> detection and set a specific hit threshold, you can configure the following:</p> <pre><code>config:\n    ocr:\n        ransomware:\n            terms: ['bad1', 'bad2', ...]\n            threshold: 1\n</code></pre> <p>This setup not only specifies custom terms but also defines a threshold value. The service will use only these terms for <code>ransomware</code> detection, with the detection threshold you have set.</p>"},{"location":"administration/service_management/#term-inclusionexclusion","title":"Term Inclusion/Exclusion","text":"<p>To modify the default set of terms by adding or excluding certain terms for <code>ransomware</code> detection, use the following configuration:</p> <pre><code>config:\n    ocr:\n        ransomware:\n            include: ['bad1', 'bad2', ...]\n            exclude: ['bank account']\n</code></pre> <p>This configuration allows the service to augment the default terms by including the terms specified in <code>include</code> and excluding those in <code>exclude</code>. The service will employ this adjusted set for OCR-based <code>ransomware</code> detection.</p>"},{"location":"administration/signature_management/","title":"Signature Management","text":""},{"location":"administration/signature_management/#signature-management","title":"Signature Management","text":"<p>Assemblyline's signature management interface lets you:</p> <ol> <li>List all signatures in the system</li> <li>Filter and search the current set of signatures</li> <li>View details about those signatures</li> <li>Set the status of a specific signature</li> <li>Remove signatures from the system</li> </ol> <p>You can find the signature management interface by clicking Manage then the Signatures menu from the navigation bar. (1)</p> <ol> <li></li> </ol> <p>Warning</p> <p>You cannot add new signatures to the system via this interface. Instead, Assemblyline has a source management interface which lets you add a variety of external sources to fetch signatures from. The updater of the different services takes care of loading the source URLs and the new signature(s) into the system. It will also sync existing signatures that have changed since the last import.</p>"},{"location":"administration/signature_management/#signature-list","title":"Signature list","text":"<p>The first page you will be taken to when loading the signature management interface will list all signatures that have been loaded into the system.</p> <p></p> <p>From this interface you can:</p> <ol> <li>Page through the different signatures from the list</li> <li>Filter the displayed signatures with the search bar<ul> <li>Assemblyline signatures can be searched using a Lucene query. As you start typing in the search box, the system will suggest fields that you can search into.</li> <li>You can also use the quick filter buttons for pre-defined searches. These pre-defined searches will help you get started with writing more complex signature searches.</li> </ul> </li> <li>Download the currently viewed signature set with the download arrow on the top right</li> <li>View the detail of a signature by clicking on it</li> </ol>"},{"location":"administration/signature_management/#signature-detail","title":"Signature detail","text":"<p>Once you click on a signature, the detail view for that signature will be shown.</p> <p></p> <p>This page will show you the following information:</p> <ol> <li>ID of the signature (under the signature detail header)</li> <li>The raw signature</li> <li>Statistics about the signature</li> <li>A histogram of the signature for the last 30 days</li> <li>A list of the last ten hits for that signature</li> </ol> <p>On the top right, it will also show actions on the signature:</p> <ol> <li>You can hit the search button to find all instances where that signature hits in the system</li> <li>Use the red delete button to delete the signature from the system<ul> <li>If the signature is still present in the source where it was retrieved, it will be re-added on the next update. In this case, you should disable the signature instead.</li> </ul> </li> <li>Change the state of a signature</li> </ol>"},{"location":"administration/signature_management/#changing-the-signature-state","title":"Changing the signature state","text":"<p>Signature states are synced with the source they are coming from but the state in your Assemblyline deployment will supersede the state that the rule updater is trying to set. This means that if you disable a rule in your Assemblyline instance, it will remain disabled even if the source where that rule is from changes.</p> <p>There are three different signature states: Deployed, Noisy, and Disabled</p> <ul> <li>Deployed:<ul> <li>Deployed will be used for detection and will generate a score depending on how the service handles these types of signatures</li> </ul> </li> <li>Noisy:<ul> <li>Noisy will be used for detection but rules with these states will not affect the score of the file</li> </ul> </li> <li>Disabled:<ul> <li>Disabled signatures are completely ignored in the system and the service will not even realize that these signatures exist</li> </ul> </li> </ul> <p>You can change the signatures by clicking the current signature state in the signature detail view. This will bring up the state-changing modal window which will let you pick a new state for the current rule. (1)</p> <ol> <li></li> </ol>"},{"location":"administration/source_management/","title":"Source Management","text":""},{"location":"administration/source_management/#source-management","title":"Source Management","text":"<p>You can access the source management interface by selecting \"Update Sources\" in the navigation menu. (1)</p> <p>The source management interface will list all services that support external sources and will show you the various sources currently configured in the system.</p> <p></p> <p>Here is where you can add sources for services that consume signatures or the safelist. In the above screenshot, you can see that the Sigma and Suricata services have configured sources where they ingest rules from.</p> <p>On this page, you can manually pull signatures from sources, add new sources, synchronize signatures on your system based on their source or view all signatures from a source.</p> <p>When signature synchronization is enabled with a remote source, this allows the system to auto-disable signatures no longer found in source (ie. a source may decide to remove a bad signature from their corpus). This feature is intended for services that will contribute to indices in Elasticsearch such as <code>badlist</code>, <code>safelist</code>, and <code>signature</code> but this flag could have meaning to updates that don't use Elasticsearch for persistence depending on its implementation.</p>"},{"location":"administration/source_management/#modifying-sources","title":"Modifying sources","text":"<p>With the source management interface, you can add, modify, or delete any sources of any service with the following actions:</p> <ol> <li>To add a new source to a given service, you can simply press the \"+\" button beside the service name for which you want to add the source.</li> <li>To modify or delete a source, simply click on the source you want to modify/delete.</li> </ol> <p>Both options will bring you to an interface that looks like this:</p> <p></p>"},{"location":"administration/source_management/#required-input","title":"Required input","text":"<p>The following sections are required to add/modify a signature source in Assemblyline:</p> <ul> <li>URI<ul> <li>This is the path to your sources. In this case, we will use a GitHub repository.</li> <li>The URI section also accepts HTTP URLs as input.</li> </ul> </li> <li>Source Name<ul> <li>This can be labeled at the user\u2019s discretion. For this example, we have used REVERSING_LABS_EXAMPLE.</li> <li>Please note that input for \"Source Name\" must not have any spaces.</li> </ul> </li> </ul>"},{"location":"administration/source_management/#optional-input","title":"Optional input","text":"<ul> <li>Pattern<ul> <li>The user may add a regex pattern to pull certain file types for a particular service. In this example, only <code>.yara</code> or <code>.yar</code> files will be added as signatures.</li> </ul> </li> <li>Username / Password<ul> <li>This is the username and password for the URL or git repository that you are targeting.</li> </ul> </li> <li>Private Key<ul> <li>If using SSH to connect to GitHub, you must generate a private SSH key and add it to this section.</li> </ul> </li> <li>Headers<ul> <li>Header name and Header value are for special HTTP headers that may be passed to the HTTP server, such as passing an API key.</li> </ul> </li> </ul>"},{"location":"administration/source_management/#alternate-methods-of-updating-sources","title":"Alternate methods of updating sources","text":"<p>There are alternate ways that the system administrator can use to modify the signatures in the system:</p> <ul> <li>Before loading the service into Assemblyline</li> <li>Inside the service management interface</li> </ul>"},{"location":"administration/source_management/#option-1-before-loading-the-service","title":"Option 1 - Before loading the service","text":"<p>The updater can be configured through the <code>service_manifest.yml</code>, which is in the root directory of each service. If you edit the files before pasting them into the system to add that service, the correct signature source(s) will be set once the service is first loaded.</p> <p>Suricata's updater</p> <p>You can find the Suricata updater configuration in its service_manifest.yml file.</p> <p>Its config block looks like this: </p><pre><code>...\nupdate_config:\n  generates_signatures: true\n  method: run\n  run_options:\n    allow_internet_access: true\n    command: [\"python\", \"-m\", \"suricata_.suricata_updater\"]\n    image: ${REGISTRY}cccs/assemblyline-service-suricata:$SERVICE_TAG\n  sources:\n    - name: emt\n      pattern: .*\\.rules\n      uri: https://rules.emergingthreats.net/open/suricata/emerging.rules.tar.gz\n  update_interval_seconds: 21600 # Quarter-day (every 6 hours)\n...\n</code></pre><p></p> <p>For more information about the update config block, you should check out the update config and update source sections of the service_manifest.yml documentation.</p>"},{"location":"administration/source_management/#option-2-inside-the-service-management-interface","title":"Option 2 - Inside the service management interface","text":"<ul> <li>You can find the service management interface by clicking the Administration topic then choose Services subtopic. (1)</li> <li>Click on the relevant service that you wish to update.</li> <li>Navigate to the \"Updates\" tab.</li> <li>You can change any value related to the updates in this section.</li> </ul> <ol> <li></li> </ol> <p>Tip</p> <p>The source update interface in this section is like the Source management page although there are a few added options for:</p> <ol> <li>Turning the \"generate signature flag\" on/off</li> <li>Waiting for a valid update or not</li> <li>Setting the updating interval</li> </ol>"},{"location":"administration/submission_actions/","title":"Submission (Post-Process) Actions","text":""},{"location":"administration/submission_actions/#submission-post-process-actions","title":"Submission (Post-Process) Actions","text":"<p>Assemblyline can be configured to take a number of actions with completed submissions. This can be because a submission has been processed, or because an ingested submission is completed as a cache hit. The following actions can be taken:</p> <ul> <li>creating an alert</li> <li>resubmitting the submission to more services</li> <li>calling a webhook</li> </ul> <p>The configuration of actions is under \"Post-process actions\" of the administration navigation menu of Assemblyline. The actions configuration is a yaml formatted dictionary with arbitrary keys and values formatted as PostprocessingAction objects.</p>"},{"location":"administration/submission_actions/#selecting-submissions-for-action","title":"Selecting submissions for action","text":"<p>Depending on the context where the submission is being tested for an action different terms are available for the Lucene query in the <code>filter</code> field. If only <code>run_on_completed</code> is set, the <code>filter</code> field may use any part of the Submission or Tagging objects where tag fields are prefixed with \"tags.\". When <code>run_on_cache</code> is set, irrespective of the value of <code>run_on_completed</code>, the search in <code>filter</code> may only use the following fields:</p> <ul> <li><code>sid</code></li> <li><code>max_score</code></li> <li><code>files.*</code></li> <li><code>metadata.*</code></li> <li><code>params.*</code></li> </ul>"},{"location":"administration/submission_actions/#action-configuration","title":"Action configuration","text":"<p>Actions are disabled by default and must be enabled using <code>enabled: true</code> setting</p>"},{"location":"administration/submission_actions/#alert","title":"Alert","text":"<p>There are no sub-configuration fields of the <code>raise_alert</code> field, it is either true or false.</p>"},{"location":"administration/submission_actions/#resubmit","title":"Resubmit","text":"<p>The <code>resubmit</code> action, when not null, must be a ResubmitOptions object. The <code>additional_services</code> field is a list of services to resubmit to in addition to the services given by a submission's <code>params.services.resubmit</code> parameter provided at submission or ingestion. The <code>random_below</code> parameter lets you further filter selected submissions by their max score, only randomly accepting submissions with score between 0 and the given value. The distribution is exponential with low scoring submissions being ignored more often.</p>"},{"location":"administration/submission_actions/#webhook","title":"Webhook","text":"<p>The <code>webhook</code> action will call a webhook url with a body holding a json object with the fields:</p> <ul> <li><code>is_cache</code>: True if this action is triggered by a submission cache hit in ingester.</li> <li><code>score</code>: The score of the submission.</li> <li><code>submission</code>: A Submission object for processed submissions, or a different Submission object for cache hit actions.</li> </ul> <p>The <code>webhook</code> field must be a Webhook object.</p> <p>Here is an example using Webhook Tester</p> <p>This webhook is only triggered on submission completion and only sends if the submission belongs to <code>user</code>. </p><pre><code>user_webhook:\n  enabled: true\n  run_on_completed: true\n  filter: 'params.submitter:user'\n  webhook:\n    uri: https://webhook-test.com/&lt;UUID&gt;\n    method: POST\n    username: xyz\n    password: xyz\n</code></pre><p></p>"},{"location":"administration/submission_actions/#default-actions","title":"Default actions","text":"<p>By default Assemblyline comes with two actions defined:</p> <pre><code>default_alerts:\n  enabled: true\n  filter: 'max_score: &gt;=500'\n  raise_alert: true\n  resubmit: null\n  run_on_cache: true\n  run_on_completed: true\n  webhook: null\ndefault_resubmit:\n  enabled: true\n  filter: 'max_score: &gt;=0'\n  raise_alert: false\n  resubmit:\n    additional_services: []\n    random_below: 500\n  run_on_cache: false\n  run_on_completed: true\n  webhook: null\n</code></pre> <p>The action named <code>default_alerts</code> applies to all submissions (both cache and non-cache) where the score is 500 or more; both <code>resubmit</code> and <code>webhook</code> are disabled on this action, and <code>raise_alert</code> is active.</p> <p>The action named <code>default_resubmit</code> applies on completion to submissions that are processed which scores zero or more. <code>webhook</code> and <code>raise_alert</code> are disabled, and <code>resubmit</code> is enabled with the following settings: <code>additional_services</code> is set to an empty list, so only the submission's own resubmit service list is used; <code>random_below</code> is set to 500, so submissions with a <code>max_score</code> between 0 and 500 will only be randomly resubmitted.</p>"},{"location":"administration/system_management/","title":"System Management","text":""},{"location":"administration/system_management/#system-management","title":"System Management","text":""},{"location":"administration/system_management/#assemblyline-major-upgrades","title":"Assemblyline Major Upgrades","text":""},{"location":"administration/system_management/#pause-processing","title":"Pause Processing","text":"<p>If upgrading the framework version (4.X \u2192 4.Y, where X &lt; Y) starting from Release 4.2.0.121+ or if performing another major change that affects Assemblyline, it's strongly recommended to pause the Ingester and Dispatcher and allow services to complete what's currently been tasked to them. This can be done by:</p> <ul> <li>Changing the state(s) using the <code>/api/v4/system/status/&lt;component&gt;/</code> API and setting the variable <code>active=false</code></li> <li>Using the toggle found in the Dashboard and switching Dispatcher/Ingester to the disabled state.</li> </ul> <p>This ensures that if there are any breaking changes between the core and services during a major upgrade (ie. framework), the services will complete their tasks with what they were compatible with.</p> <p>This also gives an extended period to upgrade services to the compatible framework version before resuming processing.</p> <p>Danger</p> <p>Systems upgrading before Release 4.2.0.121 are strongly advised to upgrade to that release at a minimum and pause the Ingester/Dispatcher before proceeding to upgrade to later releases. Alternatively, you can stop submitting files to your system and let existing submissions complete and then perform the upgrade.</p>"},{"location":"administration/system_management/#upgrade-deployment-core","title":"Upgrade Deployment (Core)","text":"<p>At this stage, you're ready to upgrade the core system and you shouldn't have any services actively processing.</p> <p>Depending on the type of deployment (Kubernetes/Docker), the method of upgrading differs:</p>"},{"location":"administration/system_management/#docker","title":"Docker","text":"<p>Update <code>SERVICE_VERSION</code> in .env file if necessary, then:</p> <pre><code>docker compose pull &amp;&amp; docker compose up -d\n</code></pre>"},{"location":"administration/system_management/#kubernetes","title":"Kubernetes","text":"<p>From version 4.7 each version of assemblyline is published as a new chart.</p> <ol> <li> <p>Make sure your system has the assemblyline helm repo registered and up to date.</p> <p></p><pre><code>helm repo add assemblyline https://cybercentrecanada.github.io/assemblyline-helm-chart/\n</code></pre> <pre><code>helm repo update\n</code></pre><p></p> </li> <li> <p>Remove any <code>release:</code> line from your <code>values.yaml</code> if you have it set.</p> </li> <li> <p>Run the <code>helm upgrade</code> command with the chart version corresponding to the assemblyline version you want to upgrade to.</p> <pre><code>helm upgrade &lt;deployment_name&gt; assemblyline/assemblyline -f /path/to/al_deployment/values.yaml -n &lt;deployment_namespace&gt; --version &lt;chart version&gt;\n</code></pre> </li> </ol>"},{"location":"administration/system_management/#optional-upgrade-services-to-compatible-framework","title":"(Optional) Upgrade Services to Compatible Framework","text":"<p>At this stage, your deployment has been successfully upgraded and your core components should be upgraded to a newer framework release.</p> <p>While the system is still paused, on the Services page you should find that all your services are in a disabled state as they're assumed to be incompatible with the rest of the system. If using public services, there should be advertisements to upgrade to the latest compatible version.</p> <p>If an advertisement doesn't appear for a service, it implies that there is no release tagged with a compatible version and requires a rebuild of the service using the more recent stable/dev tag.</p> <p>Once all services are upgraded, you can resume processing by toggling the Dispatcher/Ingester back to an active status or changing the state using the API and setting <code>active=true</code></p>"},{"location":"administration/system_management/#assemblyline-minor-upgrades-4x-4x","title":"Assemblyline Minor Upgrades (4.X \u2192 4.X)","text":"<p>This implies the changes should have little to no impact on systems within the same framework version.</p> <p>Setting the system to a paused state before upgrading isn't required.</p>"},{"location":"administration/system_management/#single-node-elasticsearch-maintenance","title":"Single Node Elasticsearch Maintenance","text":"<p>When running Elasticsearch in a single node deployment, it will sometimes drop to a <code>yellow</code> status without a real issue. This typically occurs when an index has (because of templating or defaults), its <code>number_of_replicas</code> set to more than zero. Since the single node deployment can't create a redundant replica, this forces the node to a yellow status. The details on the settings around this can be a little different depending on which version of Elasticsearch you are using, but the following should be true for versions 7 and 8.</p> <p>To check if this is the case for your Elasticsearch node, running the following command will reset the replicas to zero for all indices:</p> <pre><code>curl -XPUT -k -u \"${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\" https://localhost:9200/_all/_settings -H 'Content-Type: application/json' -d '{\"index\" : {\"number_of_replicas\" : 0}}'\n</code></pre> <p>You may need to replace <code>https</code> with <code>http</code> depending on your other settings. If your Elasticsearch doesn't return to a green status within a few minutes of running the command you may have other issues and should look for support.</p> <p>If the issue reoccurs often enough to be an issue for you, it should be preventable by ensuring no templates have a non-zero replica value set and creating an index template like the following to establish zero as the default value.</p> <pre><code>{\n   \"index_patterns\" : [\"*\"],\n   \"order\" : 0\n   \"settings\" : {\n       \"number_of_replicas\" : \"0\"\n   }\n}\n</code></pre>"},{"location":"administration/system_safelist/","title":"System safelist","text":""},{"location":"administration/system_safelist/#system-safelist","title":"System safelist","text":"<p>Assemblyline includes a safelisting system that will let you ignore certain tags generated by services. Although safelisting is available to all users throughout the interface, you can specify more complex rules via the administration interface.</p>"},{"location":"administration/system_safelist/#editing-the-safelist","title":"Editing the safelist","text":"<p>You can find the system safelist interface by clicking the Administration topic then choose the System Safelist subtopic.</p> <p></p> <p>Once the safelist management interface is open you will be greeted with a YAML file in an editor. From here you can edit the YAML directly in the interface and hit the \"Save changes\" button to apply the changes to the system.</p> <p></p> <p>The system safelist is composed of two sections and each of those sections are composed of tag types with lists of values:</p> <ol> <li><code>match</code> is where you list values for specific tag types that you want to safelist by using a direct string comparison (<code>==</code>)</li> <li><code>regex</code> is where you list regular expressions for specific tag types that you want to safelist by using regular expression matching (<code>.match()</code>)</li> </ol> <p>Example</p> <pre><code>  match:\n    &lt;tag-type&gt;:\n      - &lt;value&gt;\n\n  regex:\n    &lt;tag-type&gt;:\n      - &lt;regular expression&gt;\n</code></pre>"},{"location":"administration/system_safelist/#default-system-safelist","title":"Default system safelist","text":"<p>There is a safelist installed in the system by default which covers some basic cases. These are the tags that are safelisted by default:</p> Default system safelist <pre><code># Default tag_safelist.yml file\n#\n#    The following tags are safelisted:\n#     - Domains pointing to localhost\n#     - Domains commonly found in XML files, certificates, and during dynamic analysis runs\n#     - IPs in the private network IP space\n#     - URIs pointing to IPs in the private network IP space\n#     - URIs commonly found in XML files, certificates, and during dynamic analysis runs\n#\n#    Note: - You can override the default tag_safelist.yml by putting an\n#            updated version in /etc/assemblyline/tag_safelist.yml.\n#          - If you want to add values to one of the following tag types,\n#            you must copy the default values to the new file.\n#          - You can nullify values by putting an empty object or an empty list\n#            in your new file\n\n# Match section contains tag types where each tag type is\n#  a list of values that should be safelisted by using a direct\n#  string comparison.\nmatch:\n  # Direct match to dynamic domains\n  network.dynamic.domain:\n    - localhost\n    - android.googlesource.com\n    - play.google.com\n    - schemas.android.com\n    - xmlpull.org\n    - schemas.openxmlformats.org\n    - schemas.microsoft.com\n    - settings-win.data.microsoft.com\n    - vortex-win.data.microsoft.com\n    - wpad.reddog.microsoft.com\n    - verisign.com\n    - csc3-2010-crl.verisign.com\n    - csc3-2010-aia.verisign.com\n    - ocsp.verisign.com\n    - logo.verisign.com\n    - crl.verisign.com\n    - ctldl.windowsupdate.com\n    - ns.adobe.com\n    - www.w3.org\n    - purl.org\n  # Direct match to static domains\n  network.static.domain:\n    - localhost\n    - android.googlesource.com\n    - play.google.com\n    - schemas.android.com\n    - xmlpull.org\n    - schemas.openxmlformats.org\n    - schemas.microsoft.com\n    - settings-win.data.microsoft.com\n    - vortex-win.data.microsoft.com\n    - wpad.reddog.microsoft.com\n    - verisign.com\n    - csc3-2010-crl.verisign.com\n    - csc3-2010-aia.verisign.com\n    - ocsp.verisign.com\n    - logo.verisign.com\n    - crl.verisign.com\n    - ctldl.windowsupdate.com\n    - ns.adobe.com\n    - www.w3.org\n    - purl.org\n\n# Regex section contains tag types where each tag type is\n#  a list of regular expressions to be run to safelist\n#  the associated tags.\nregex:\n  # Regular expressions to safelist dynamic IPs (Private IPs)\n  # Note: Since IPs have already been validated, the regular expression is simpler\n  network.dynamic.ip:\n    - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).*\n  # Regular expression to safelist static IPs (Private IPs)\n  # Note: Since IPs have already been validated, the regular expression is simpler\n  network.static.ip:\n    - (?:127\\.|10\\.|192\\.168|172\\.1[6-9]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.).*\n  # Regular expression to safelist dynamic URIs\n  network.dynamic.uri:\n    - (?:ftp|http)s?://localhost(?:$|/.*)\n    - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*)\n    - https?://schemas\\.android\\.com/apk/res(-auto|/android)\n    - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*)\n    - https?://android\\.googlesource\\.com/toolchain/llvm-project\n    - https?://schemas\\.microsoft\\.com(?:$|/.*)\n    - https?://schemas\\.openxmlformats\\.org(?:$|/.*)\n    - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles)\n    - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing)\n    - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml\n    - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex\n    - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex\n    - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex\n    - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing)\n    - https?://schemas\\.openxmlformats\\.org/package/2006/relationships\n    - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006\n    - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math)\n    - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape\n    - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main\n    - https?://www\\.verisign\\.com/(rpa0|rpa|cps0)\n    - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat\n    - https?://ocsp\\.verisign\\.com\n    - https?://logo\\.verisign\\.com/vslogo\\.gif04\n    - https?://crl\\.verisign\\.com/pca3-g5\\.crl04\n    - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D\n    - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0\n    - https?://ctldl\\.windowsupdate\\.com/.*\n    - https?://ns\\.adobe\\.com/photoshop/1\\.0/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/mm/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent#\n    - https?://purl\\.org/dc/elements/1\\.1/\n    - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns#\n  # Regular expression to safelist static URIs\n  network.static.uri:\n    - (?:ftp|http)s?://localhost(?:$|/.*)\n    - (?:ftp|http)s?://(?:(?:(?:10|127)(?:\\.(?:[2](?:[0-5][0-5]|[01234][6-9])|[1][0-9][0-9]|[1-9][0-9]|[0-9])){3})|(?:172\\.(?:1[6-9]|2[0-9]|3[0-1])(?:\\.(?:2[0-4][0-9]|25[0-5]|[1][0-9][0-9]|[1-9][0-9]|[0-9])){2}|(?:192\\.168(?:\\.(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])){2})))(?:$|/.*)\n    - https?://schemas\\.android\\.com/apk/res(-auto|/android)\n    - https?://xmlpull\\.org/v1/doc/features\\.html(?:$|.*)\n    - https?://android\\.googlesource\\.com/toolchain/llvm-project\n    - https?://schemas\\.microsoft\\.com(?:$|/.*)\n    - https?://schemas\\.openxmlformats\\.org(?:$|/.*)\n    - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/relationships/(image|attachedTemplate|header|footnotes|fontTable|customXml|endnotes|theme|settings|webSettings|glossaryDocument|numbering|footer|styles)\n    - https?://schemas\\.microsoft\\.com/office/word/2010/(wordml|wordprocessingCanvas|wordprocessingInk|wordprocessingGroup|wordprocessingDrawing)\n    - https?://schemas\\.microsoft\\.com/office/word/(2012|2006)/wordml\n    - https?://schemas\\.microsoft\\.com/office/word/2015/wordml/symex\n    - https?://schemas\\.microsoft\\.com/office/drawing/2014/chartex\n    - https?://schemas\\.microsoft\\.com/office/drawing/2015/9/8/chartex\n    - https?://schemas\\.openxmlformats\\.org/drawingml/2006/(main|wordprocessingDrawing)\n    - https?://schemas\\.openxmlformats\\.org/package/2006/relationships\n    - https?://schemas\\.openxmlformats\\.org/markup-compatibility/2006\n    - https?://schemas\\.openxmlformats\\.org/officeDocument/2006/(relationships|math)\n    - https?://schemas\\.openxmlformats\\.org/word/2010/wordprocessingShape\n    - https?://schemas\\.openxmlformats\\.org/wordprocessingml/2006/main\n    - https?://www\\.verisign\\.com/(rpa0|rpa|cps0)\n    - https?://wpad\\.reddog\\.microsoft\\.com/wpad\\.dat\n    - https?://ocsp\\.verisign\\.com\n    - https?://logo\\.verisign\\.com/vslogo\\.gif04\n    - https?://crl\\.verisign\\.com/pca3-g5\\.crl04\n    - https?://csc3-2010-crl\\.verisign\\.com/CSC3-2010\\.crl0D\n    - https?://csc3-2010-aia\\.verisign\\.com/CSC3-2010\\.cer0\n    - https?://ctldl\\.windowsupdate\\.com/.*\n    - https?://ns\\.adobe\\.com/photoshop/1\\.0/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/mm/\n    - https?://ns\\.adobe\\.com/xap/1\\.0/sType/ResourceEvent#\n    - https?://purl\\.org/dc/elements/1\\.1/\n    - https?://www\\.w3\\.org/1999/02/22-rdf-syntax-ns#\n</code></pre> <p>Tip</p> <p>You can also find the default safelist in the code: tag_safelist.yml</p>"},{"location":"administration/user_management/","title":"User Management Guide","text":""},{"location":"administration/user_management/#user-management-guide","title":"User Management Guide","text":"<p>Managing users effectively is a key aspect of maintaining the security and efficiency of Assemblyline. This guide provides you with step-by-step instructions to utilize the user management interface within Assemblyline to manage user accounts with ease.</p>"},{"location":"administration/user_management/#accessing-the-user-management-interface","title":"Accessing the User Management Interface","text":"<p>To begin managing user accounts in Assemblyline, follow these steps to access the user management interface:</p> <ol> <li>Click on the Administration topic in your Assemblyline interface.</li> <li>Under the Administration section, select the Users subtopic.</li> <li>Once in the Users, you will be presented with the user management interface.</li> </ol> <p></p>"},{"location":"administration/user_management/#managing-user-accounts","title":"Managing User Accounts","text":""},{"location":"administration/user_management/#viewing-all-users","title":"Viewing All Users","text":"<p>The initial screen of the user management interface displays a list of all user accounts currently in the system.</p> <p></p> <p>Here are various actions you can perform from this interface:</p> <ul> <li>Navigate through the pages of the user list to explore the accounts.</li> <li>Apply filters using the query bar at the top. Begin typing, and the interface will provide suggestions for fields you can refine by. This feature helps you to quickly narrow down the list according to specific criteria.</li> <li>Click on the quick filter buttons for common predefined searches to assist in creating complex queries.</li> <li>Add new users to the system by clicking the Add User button located at the top right.</li> </ul>"},{"location":"administration/user_management/#user-account-details","title":"User Account Details","text":"<p>To view further details about a particular user, simply click on their username in the user list. This will open the user detail page, which provides comprehensive information about the user, including:</p> <p></p> <ul> <li>Profile: Including username, avatar, full name, groups, email address, and, if enabled by the administrator, security classification. The user's security classification dictates the level of information the user is permitted to access within the system.</li> <li>Roles: The permissions the user has within the system.</li> <li>Quotas: The usage limits for the user's API calls.</li> <li>Status: Whether the user account is active or disabled.</li> </ul> <p>On the user detail page, you can take various administrative actions:</p> <ul> <li>Remove the user account entirely.</li> <li>Enable or disable the user's access to Assemblyline.</li> <li>Adjust the user's role settings or classification as appropriate.</li> <li>Set or modify the user's API usage quotas.</li> <li>Change the user's password securely.</li> </ul>"},{"location":"administration/user_management/#step-by-step-actions-on-user-detail-page","title":"Step-by-Step Actions on User Detail Page","text":""},{"location":"administration/user_management/#enabling-or-disabling-a-user","title":"Enabling or Disabling a User","text":"<p>To change a user's access to the system:</p> <ol> <li>Click the Account Enabled or Account Disabled button, depending on the desired action, located below the avatar in the user's detail page.</li> </ol>"},{"location":"administration/user_management/#modifying-user-roles","title":"Modifying User Roles","text":"<p>To update a user's roles:</p> <ol> <li>Locate the Roles section within the detail page.</li> <li>Select or unselect roles as needed.</li> </ol>"},{"location":"administration/user_management/#adjusting-user-quotas","title":"Adjusting User Quotas","text":"<p>To set or change usage quotas:</p> <ol> <li>Go to the relevant quota setting in the Options section.</li> <li>Input the new quota values as per your requirements.</li> </ol>"},{"location":"administration/user_management/#updating-user-password","title":"Updating User Password","text":"<p>To change a user's password:</p> <ol> <li>In the Security section, click on the Change Password button.</li> <li>Follow the prompts to enter and confirm the new password.</li> </ol>"},{"location":"administration/user_management/#removing-a-user","title":"Removing a User","text":"<p>To permanently remove a user account from the system:</p> <ol> <li>Select the Remove action located above the avatar in the user's detail page</li> <li>Confirm the removal when prompted.</li> </ol> <p>To apply any changes to the user account, you'll need to select the Save changes button located at the bottom of the window.</p> <p>By following the detailed steps outlined in this guide, you can effectively manage user accounts within Assemblyline to ensure a secure and streamlined operation.</p>"},{"location":"blog/entries/","title":"Entries","text":""},{"location":"blog/entries/#blog-entries","title":"Blog Entries","text":"<p>That's right, we blog now! Check out our posts on Medium and articles written by our community!</p> <ul> <li> <p> One Library to Parse Them All</p> <p></p><p>Published: January 2026</p><p></p> <p></p> <p>Read how the Canadian Cyber Centre came up with a one-stop shop for malware configuration parsing with multi-framework support.</p> </li> <li> <p> Cracking Malware with Maco</p> <p></p><p>Published: January 2026</p><p></p> <p></p> <p>Learn how Assemblyline streamlines malware configuration parsing using Maco.</p> </li> <li> <p>  Advent of Configuration Extraction</p> <p></p><p>Published: December 2025</p><p></p> <p></p> <p>How does Sekoia use Assemblyline to automate extraction of malware configuration data?</p> <p>Page 1 | Page 2 | Page 3 | Page 4</p> </li> <li> <p> Crack any password-protected InnoSetup installer</p> <p></p><p>Published: August 2024</p><p></p> <p> Learn about InnoSetup and how we were able to automate cracking any related installer.</p> </li> <li> <p> What Can Assemblyline Learn From Other Malware Analysis Projects?</p> <p></p><p>Published: March 2024</p><p></p> <p> A deep dive on how Assemblyline compares to similar products and how could we improve the platform.</p> </li> <li> <p> The Great OneNote Scramble of 2023</p> <p></p><p>Published: January 2024</p><p></p> <p> See how fast we adapted Assemblyline in light of a malware campaign!</p> </li> <li> <p> What other security products WON\u2019T tell you about malicious archives</p> <p></p><p>Published: November 2023</p><p></p> <p> See what how one attack chain lead to substantial improvements to Assemblyline and other open source projects!</p> </li> <li> <p>  HackTheBox Business CTF 2023 Forensic Challenge</p> <p></p><p>Published: October 2023</p><p></p> <p> Can you use Assemblyline in a CTF?</p> <p>Let's find out!</p> <p>Part 1 | Part 2 | Part 3</p> </li> <li> <p> Static Analysis Showcase</p> <p></p><p>Published: September 2023</p><p></p> <p> See how well Assemblyline can perform without any dynamic analysis!</p> </li> <li> <p> A Little Bit Of History</p> <p></p><p>Published: August 2023</p><p></p> <p> Learn how Assemblyline came to be the powerful tool it is today.</p> </li> <li> <p> Supercharge Your Malware Analysis Workflow</p> <p></p><p>Published: August 2023</p><p></p> <p> Learn how you can use Assemblyline to automate your malware analysis!</p> </li> </ul>"},{"location":"blog/entries/#video-entries","title":"Video Entries","text":"<p>That's right, we have videos too! Check out videos made by the community that showcase Assemblyline!</p> <ul> <li> <p> Supercharging Open Source Malware Analysis (Bitwarden)</p> <p></p> </li> <li> <p> Supercharge your malware analysis workflow (VirusBulletin)</p> <p></p> </li> <li> <p> Beginner Malware Analysis - Emotet Infection Chain (Invoke RE)</p> <p></p> </li> <li> <p> Automating Malware Triage (Invoke RE)</p> <p></p> </li> <li> <p> Technical session CyberShock (CERT.LV)</p> <p></p> </li> <li> <p> Introducing CSE\u2019s open source AssemblyLine (Hackfest)</p> <p></p> </li> </ul>"},{"location":"developer_manual/core/infrastructure/","title":"Infrastructure","text":""},{"location":"developer_manual/core/infrastructure/#infrastructure","title":"Infrastructure","text":"<p>This section gives you a complete overview of all the different components of Assemblyline, so you have a better idea of what they are used for.</p>"},{"location":"developer_manual/core/infrastructure/#dependencies","title":"Dependencies","text":"<p>The components listed here are external dependencies used by Assemblyline:</p> Dependency Description Docker Docker is now at the heart of Assemblyline because all Assemblyline's components are now running as Docker containers. https://www.docker.com/ Kubernetes For multi-computer installations, Assemblyline uses Kubernetes to deploy the different Docker containers and keep them healthy. https://kubernetes.io/ Helm Helm is used to easily deploy and maintain our Kubernetes instance. https://helm.sh/ Elastic Stack Assemblyline uses the full Elastic stack to store results, logs, metrics, and APMs. It consists of the following components: https://www.elastic.co/elastic-stack Elasticsearch Elasticsearch is used for storing results, logs, and metrics of the system. It also provides search capability to Assemblyline. Kibana (optional) Provides dashboards to monitor your Assemblyline cluster. APM (optional) Gather Application Performance Metrics so that we can pinpoint potential performance issues with the system and fix them. Filebeat (optional) Gather all the logs for the different components into Elasticsearch to be displayed in Kibana. Metricbeat (optional) Gather metrics for the different hosts where the Docker containers are run. Redis We are using Redis for the queueing system, for messaging between the components, and as a remote data structure to keep multiple instances of a given component working in sync. https://redis.io/ Nginx Nginx is used by Assemblyline as a proxy to give the user access to the different user-facing components: UI, API, Socket Server, Kibana. https://www.nginx.com/ Minio For our default file storage, we use Minio which perfectly replicates the Amazon S3 API and is built to work with Kubernetes. https://min.io/"},{"location":"developer_manual/core/infrastructure/#core-components","title":"Core Components","text":"<p>The components listed here are Assemblyline-made processes that perform various tasks in the system:</p> Core components Description Link Alerter Create alerts for the different submissions in the system. Source code Dispatcher Route the files in the system while a submission is taking place. Make sure all files during a submission are completed by all required services. Source code Expiry Delete submissions and their results when their TTL (Time-to-live) expires. Source code Frontend Provides the user interface to interact with Assemblyline. Source code Ingester Move ingested files from the priority queues to the processing queues. Source code Metrics Aggregator Aggregate metrics of the different components in the system to save them into an ELK (Elasticsearch-Logstash-Kibana) stack. Source code Metrics Heartbeat Provide live metrics in the system for the dashboard. Source code Scaler Spin up and down services in the system depending on the load. Source code Statistics Aggregator Generate daily statistics about signatures and heuristics. Source code Updater Make sure the different services get their latest update files. Source code Workflow Run the different workflows in the system and apply their labels, priority, and status. Source code Service Server Provides an API for services to get tasks and post their results. Source code UI / Socket Server Provides the APIs and a socket.io interface to interact with Assemblyline. Source code"},{"location":"developer_manual/core/infrastructure/#service-interfaces","title":"Service interfaces","text":"<p>The interfaces listed here are used by Assemblyline's services to process files, generate results, and communicate back to the Service Server:</p> Service Interface Description Link Python 2 Compatibility Library A library that gives services Python 2 compatibility. Source code Result Class used by a service to generate results in the system. Source code Service Base Base Assemblyline service class. Source code Service Request Class that defines a request to scan a file for a given service. Source code Task Handler A Python wrapper that communicates with the service server to get a task, download files, and publish results. It communicates with the service via named pipes so that the service can execute the received tasks. Source code"},{"location":"developer_manual/core/infrastructure/#ui-plugins","title":"UI Plugins","text":"<p>The API may be extended via plugins. Plugins may be developed externally to Assemblyline to provide additional functionality. They must conform to the defined interface for their function. Currently, the only type of plugin defined is for information enrichment from systems external to Assemblyline.</p> Plugin Interface Description Link External system lookups These plugins enable the querying of Assmebyline data in an external system and brining back the results. Source code"},{"location":"developer_manual/docs/docs/","title":"Documentation","text":""},{"location":"developer_manual/docs/docs/#updating-documentation","title":"Updating documentation","text":"<p>This documentation is built using the awesome project: mkdocs-material</p>"},{"location":"developer_manual/docs/docs/#clone-the-documentation","title":"Clone the documentation","text":"<p>The documentation can be cloned from the following repository:</p> <pre><code>git clone git@github.com:CybercentreCanada/assemblyline4_docs.git\n</code></pre>"},{"location":"developer_manual/docs/docs/#install-dependencies","title":"Install dependencies","text":"<p>Create a virtual environment for your documentation in the <code>/venv/</code> directory right in the <code>assemblyline4_docs</code> source and install <code>mkdocs</code> dependencies:</p> <pre><code>cd assemblyline4_docs\npython -m venv venv\n\n# Activate the venv.\n# If on Windows, use \".\\venv\\Scripts\\activate.bat\" instead\nsource ./venv/bin/activate\n\npip install mkdocs-material\npip install mkdocs-static-i18n\npip install mkdocs-glightbox\n</code></pre>"},{"location":"developer_manual/docs/docs/#run-the-documentation-locally","title":"Run the documentation locally","text":"<p>You can manually run the documentation from a shell:</p> <pre><code>cd assemblyline4_docs\nmkdocs serve\n</code></pre> <p>Alternatively, if you are using VSCode, you can launch the pre-configured task: <code>Launch Assemblyline Documentation</code></p>"},{"location":"developer_manual/env/getting_started/","title":"Getting Started","text":""},{"location":"developer_manual/env/getting_started/#getting-started","title":"Getting Started","text":"<p>Before starting to develop for Assemblyline, you will need to set up your environment. We have a couple of options to assist you, from a free-to-use and easy-to-setup script to more complex setups.</p>"},{"location":"developer_manual/env/getting_started/#development-virtual-machine","title":"Development virtual machine","text":"<p>Whether you are developing a new service or working on core components, Assemblyline requires specific external packages and files installed in specific directories in the containers. For this reason, it is recommended that you do not develop directly from your desktop unless you use remote debugging features.</p>"},{"location":"developer_manual/env/getting_started/#the-minimum-specifications-for-development-virtual-machine","title":"The minimum specifications for development virtual machine","text":"<p>There are quite a few containers to run to spin-up the Assemblyline dependencies. For this reason, the development VM should have at least the following specifications:</p> <ul> <li>2 cores</li> <li>8 GB of RAM</li> <li>40 GB of disk space</li> </ul>"},{"location":"developer_manual/env/getting_started/#operating-system","title":"Operating system","text":"<p>We recommend that you use Ubuntu for your development VM, because all instructions that we provide in the document has been built with this OS in mind. There are two versions that you can pick from:</p> <ul> <li>[Recommended] Ubuntu 24.04 Desktop<ul> <li>For local development where your IDE runs in the same VM as the Assemblyline containers</li> </ul> </li> <li>Ubuntu 24.04 Server<ul> <li>For remote development where your IDE runs on your local computer and the Assemblyline containers run on the VM</li> </ul> </li> </ul>"},{"location":"developer_manual/env/getting_started/#choosing-your-ide","title":"Choosing your IDE","text":"<p>We have two IDEs for you to pick from, both of which support local and remote development:</p>"},{"location":"developer_manual/env/getting_started/#vscode","title":"VSCode","text":"<p>This is our recommended IDE since it is free and very easy to setup. Most of the Assemblyline team has moved to this IDE and we use a mix of local and remote development with it. Once you're done installing your VM, you can follow the instructions to get VSCode up and running using our simple setup script.</p> <p>For instructions on how to use VSCode, refer to the \"use VSCode\" documentation.</p>"},{"location":"developer_manual/env/getting_started/#pycharm","title":"PyCharm","text":"<p>This IDE is much more robust in terms Python development but the setup is more complex. There is both a paid and free version of this IDE. The free Community edition of PyCharm will allow you to do local develpment only and you will have to run the dependencies by hand using <code>docker compose</code>. The paid version, PyCharm Professional, will let you manage Docker dependencies inside the IDE and utilize remote development.</p> <p>Here are the installation instructions for the different setups:</p> <ul> <li>Local development (minimum requirement: PyCharm Community edition)</li> <li>Remote development (minimum requirement: PyCharm Professional edition)</li> </ul> <p>For instructions on how to use PyCharm, refer to the \"use PyCharm\" documentation.</p>"},{"location":"developer_manual/env/pycharm/local_development/","title":"Local development","text":""},{"location":"developer_manual/env/pycharm/local_development/#local-development","title":"Local development","text":"<p>This documentation will show you how to set up your development virtual machine for local development using the PyCharm Community Edition IDE (This would also work with PyCharm Professional). In this setup, you will run your IDE inside the virtual machine where the Assemblyline containers are running. This is by far the easiest setup to get PyCharm working and removes a lot of headaches.</p>"},{"location":"developer_manual/env/pycharm/local_development/#operating-system","title":"Operating system","text":"<p>For this documentation, we will assume that you are working on a fresh installation of Ubuntu 20.04 Desktop.</p>"},{"location":"developer_manual/env/pycharm/local_development/#update-vm","title":"Update VM","text":"<p>Make sure Ubuntu is running the latest software</p> <pre><code>sudo apt update\nsudo apt dist-upgrade\nsudo snap refresh\n</code></pre> <p>Reboot if needed</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#installing-pre-requisite-software","title":"Installing pre-requisite software","text":""},{"location":"developer_manual/env/pycharm/local_development/#install-assemblyline-apt-dependencies","title":"Install Assemblyline APT dependencies","text":"<pre><code>sudo apt update\nsudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#install-python-39","title":"Install Python 3.9","text":"<p>Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9.</p> <pre><code>sudo apt install -y software-properties-common\nsudo add-apt-repository -y ppa:deadsnakes/ppa\nsudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#installing-docker-and-docker-compose","title":"Installing Docker and Docker Compose","text":"<p>Installing <code>docker</code> and <code>docker compose</code> on your machine is necessary for Assemblyline local development.</p> <ol> <li> <p>Follow the install guide provided by the official Docker documentation:</p> <ul> <li>Install Guide for Ubuntu</li> <li>Install Guide for RHEL</li> <li>Install Guide for Other Platforms</li> </ul> </li> <li> <p>Ensure the installation was successful by invoking the commands:   </p><pre><code>docker version\ndocker compose version\n</code></pre><p></p> </li> </ol>"},{"location":"developer_manual/env/pycharm/local_development/#installing-pycharm","title":"Installing PyCharm","text":"<p>Let's install the desired PyCharm version. The Professional version is not needed but if you have a licence you can use it.</p> PyCharm CommunityPyCharm Professional <pre><code>sudo snap install --classic pycharm-community\n</code></pre> <pre><code>sudo snap install --classic pycharm-professional\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#adding-assemblyline-specific-configuration","title":"Adding Assemblyline specific configuration","text":""},{"location":"developer_manual/env/pycharm/local_development/#data-directories","title":"Data directories","text":"<p>Because Assemblyline uses its own set of folders inside the core, service-server, and UI containers, we must create the same folder structure here so we can run the components in debug mode.</p> <pre><code>sudo mkdir -p /etc/assemblyline\nsudo mkdir -p /var/cache/assemblyline\nsudo mkdir -p /var/lib/assemblyline\nsudo mkdir -p /var/log/assemblyline\n\nsudo chown $USER /etc/assemblyline\nsudo chown $USER /var/cache/assemblyline\nsudo chown $USER /var/lib/assemblyline\nsudo chown $USER /var/log/assemblyline\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#dev-default-configuration-files","title":"Dev default configuration files","text":"<p>Here we will create configuration files that match the default dev Docker Compose configuration files so that we can swap any of the components to the one that is being debugged.</p> <pre><code>echo \"enforce: true\" &gt; /etc/assemblyline/classification.yml\necho \"\nauth:\n  internal:\n    enabled: true\n\ncore:\n  alerter:\n    delay: 0\n  metrics:\n    apm_server:\n      server_url: http://localhost:8200/\n    elasticsearch:\n      hosts: [http://elastic:devpass@localhost]\n\ndatastore:\n  ilm:\n    indexes:\n      alert:\n        unit: m\n      error:\n        unit: m\n      file:\n        unit: m\n      result:\n        unit: m\n      submission:\n        unit: m\n\nfilestore:\n  cache:\n    - file:///var/cache/assemblyline/\n\nlogging:\n  log_level: INFO\n  log_as_json: false\n\nui:\n  audit: false\n  debug: false\n  enforce_quota: false\n  fqdn: 127.0.0.1.nip.io\n\" &gt; /etc/assemblyline/config.yml\n</code></pre> <p>Tip</p> <p>As you can see in the last command we are setting the FQDN to 127.0.0.1.nip.io. NIP.IO is a service that will resolve the first part of the domain 127.0.0.1.nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs.</p>"},{"location":"developer_manual/env/pycharm/local_development/#setup-assemblyline-source","title":"Setup Assemblyline source","text":""},{"location":"developer_manual/env/pycharm/local_development/#install-git","title":"Install git","text":"<p>Since your VM is running Ubuntu 20.04 you can just install it with APT:</p> <pre><code>sudo apt install -y git\n</code></pre> <p>Tip</p> <p>You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help</p>"},{"location":"developer_manual/env/pycharm/local_development/#clone-core-repositories","title":"Clone core repositories","text":"<p>Create the core working directory</p> <pre><code>mkdir -p ~/git/alv4\ncd ~/git/alv4\n</code></pre> <p>Clone Assemblyline's repositories</p> Git via SSHGit via HTTPS <p>Use SSH if you have your SSH id_rsa file configured to your GitHub account </p><pre><code>git clone git@github.com:CybercentreCanada/assemblyline-base.git\ngit clone git@github.com:CybercentreCanada/assemblyline-core.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-client.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-server.git\ngit clone git@github.com:CybercentreCanada/assemblyline-ui.git\ngit clone git@github.com:CybercentreCanada/assemblyline-v4-service.git\n</code></pre><p></p> <p>Use HTTPS if you don't have your GitHub account configured with an SSH key </p><pre><code>git clone https://github.com/CybercentreCanada/assemblyline-base.git\ngit clone https://github.com/CybercentreCanada/assemblyline-core.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-client.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-server.git\ngit clone https://github.com/CybercentreCanada/assemblyline-ui.git\ngit clone https://github.com/CybercentreCanada/assemblyline-v4-service.git\n</code></pre><p></p>"},{"location":"developer_manual/env/pycharm/local_development/#virtual-environment","title":"Virtual Environment","text":"<pre><code># Directly in the alv4 source directory\ncd ~/git/alv4\n\n# Create the virtualenv\npython3.9 -m venv venv\n\n# Install Assemblyline packages with their test dependencies\n~/git/alv4/venv/bin/pip install assemblyline[test] assemblyline-core[test] assemblyline-service-server[test] assemblyline-ui[test]\n\n# Remove Assemblyline packages because we will use the live code\n~/git/alv4/venv/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#setting-up-services-optional","title":"Setting up Services (Optional)","text":"<p>If you plan on doing service development in PyCharm you will need a dedicated directory for services with its own virtual environment.</p>"},{"location":"developer_manual/env/pycharm/local_development/#clone-service-repositories","title":"Clone service repositories","text":"<p>Create the service working directory</p> <pre><code>mkdir -p ~/git/services\ncd ~/git/services\n</code></pre> <p>Clone Assemblyline's services repositories</p> Git via SSHGit via HTTPS <p>Use SSH if you have your SSH id_rsa file configured to your GitHub account </p><pre><code>git clone git@github.com:CybercentreCanada/assemblyline-service-apivector.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-batchdeobfuscator.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-capa.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-cape.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-elf.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-elfparser.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-extract.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-floss.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pe.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-virustotal.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-yara.git\n</code></pre><p></p> <p>Use HTTPS if you don't have your GitHub account configured with an SSH key </p><pre><code>git clone https://github.com/CybercentreCanada/assemblyline-service-apivector.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-batchdeobfuscator.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-capa.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-cape.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-elf.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-elfparser.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-extract.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-floss.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pe.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-virustotal.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-yara.git\n</code></pre><p></p>"},{"location":"developer_manual/env/pycharm/local_development/#virtual-environment_1","title":"Virtual Environment","text":"<pre><code># Directly in the services source directory\ncd ~/git/services\n\n# Create the virtualenv\npython3.9 -m venv venv\n\n# Install Assemblyline packages from source in the services virtualenv\n~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-base\n~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-core\n~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-service-client\n~/git/services/venv/bin/pip install -e ~/git/alv4/assemblyline-v4-service\n</code></pre>"},{"location":"developer_manual/env/pycharm/local_development/#setup-pycharm-for-core","title":"Setup PyCharm for core","text":"<ol> <li>Load PyCharm<ul> <li>Choose whatever configuration option you want until the <code>Welcome screen</code></li> </ul> </li> <li>Click the <code>Open</code> button</li> <li>Choose the <code>~/git/alv4</code> directory</li> </ol> <p>Info</p> <p>Your Python interpreter shows as <code>No Interpreter</code> in the bottom right corner of the window, do the following:</p> <ol> <li>Click on it</li> <li>Click add interpreter</li> <li>Choose \"Existing Environment\"</li> <li>Click \"OK\"</li> </ol>"},{"location":"developer_manual/env/pycharm/local_development/#setup-pycharm-for-service-optional","title":"Setup PyCharm for service (optional)","text":"<ol> <li>From your core PyCharm window open the <code>File menu</code> then click <code>Open</code></li> <li>Choose the <code>~/git/services</code> directory</li> <li>Select <code>New Window</code></li> </ol> <p>Info</p> <p>Your Python interpreter shows as <code>No Interpreter</code> in the bottom right corner of the window, do the following:</p> <ol> <li>Click on it</li> <li>Click add interpreter</li> <li>Choose \"Existing Environment\"</li> <li>Click \"OK\"</li> </ol>"},{"location":"developer_manual/env/pycharm/local_development/#use-pycharm","title":"Use PyCharm","text":"<p>Now that your Local development VM is set up you should read the use PyCharm documentation to get you started.</p>"},{"location":"developer_manual/env/pycharm/remote_development/","title":"Remote development","text":""},{"location":"developer_manual/env/pycharm/remote_development/#remote-development","title":"Remote development","text":"<p>Warning</p> <p>To use this setup, we assume that you have a paid version of PyCharm (Pycharm professional) because we will be using features that are exclusive to the Professional version. If you don't, use the Local development setup instead.</p> <p>This document will show you how to set up your target virtual machine for remote development which means that you will run your IDE on your desktop and run the Assemblyline containers on the remote target VM.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#on-the-target-vm","title":"On the target VM","text":""},{"location":"developer_manual/env/pycharm/remote_development/#operating-system","title":"Operating system","text":"<p>For this document, we will assume that you are working on a fresh installation of Ubuntu 20.04 Server.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#update-target-vm","title":"Update target VM","text":"<p>Make sure Ubuntu is running the latest software</p> <pre><code>sudo apt update\nsudo apt dist-upgrade\n</code></pre> <p>Reboot if needed</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#installing-pre-requisite-software","title":"Installing pre-requisite software","text":""},{"location":"developer_manual/env/pycharm/remote_development/#install-ssh-daemon","title":"Install SSH Daemon","text":"<p>We need to make sure the remote target has an SSH daemon installed for remote debugging</p> <pre><code>sudo apt update\nsudo apt install -y ssh\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#install-assemblyline-apt-dependencies","title":"Install Assemblyline APT dependencies","text":"<pre><code>sudo apt update\nsudo apt-get install -yy libfuzzy2 libmagic1 libldap-2.4-2 libsasl2-2 build-essential libffi-dev libfuzzy-dev libldap2-dev libsasl2-dev libssl-dev\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#install-python-39","title":"Install Python 3.9","text":"<p>Assemblyline 4 containers are now all built on Python 3.9 therefore we will install Python 3.9.</p> <pre><code>sudo apt install -y software-properties-common\nsudo add-apt-repository -y ppa:deadsnakes/ppa\nsudo apt-get install -yy python3-venv python3.9 python3.9-dev python3.9-venv libffi7\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#installing-docker-and-docker-compose","title":"Installing Docker and Docker Compose","text":"<p>Installing <code>docker</code> and <code>docker compose</code> on your machine is necessary for Assemblyline remote development.</p> <ol> <li> <p>Follow the install guide provided by the official Docker documentation:</p> <ul> <li>Install Guide for Ubuntu</li> <li>Install Guide for RHEL</li> <li>Install Guide for Other Platforms</li> </ul> </li> <li> <p>Ensure the installation was successful by invoking the commands:   </p><pre><code>docker version\ndocker compose version\n</code></pre><p></p> </li> </ol>"},{"location":"developer_manual/env/pycharm/remote_development/#securing-docker-for-remote-access","title":"Securing Docker for remote access","text":"<p>We are going to make your Docker server accessible from the internet. To make it secure, we need to enable TLS authentication in the Docker daemon. Anywhere that you see assemblyline.local, you can change that value to your own DNS name. If you're planning on using an IP, you'll have to set a <code>static IP</code> to the remote VM because your certificate (cert) will only allow connections to that IP.</p> <pre><code># Create a cert directory\nmkdir ~/certs\ncd ~/certs\n\n# Create a CA (Remember the password you've set)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Create a certificate-signing request (ignore the .rng error)\nopenssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\"\n\n# Creating the server public/private key\nopenssl genrsa -out server-key.pem 4096\nopenssl req -subj \"/CN=assemblyline.local\" -sha256 -new -key server-key.pem -out server.csr\necho subjectAltName = DNS:assemblyline.local,IP:`ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`,IP:127.0.0.1 &gt;&gt; extfile.cnf\necho extendedKeyUsage = serverAuth &gt;&gt; extfile.cnf\nopenssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnf\n\n# Creating the client public/private key\nopenssl genrsa -out key.pem 4096\nopenssl req -subj '/CN=client' -new -key key.pem -out client.csr\necho extendedKeyUsage = clientAuth &gt; extfile-client.cnf\nopenssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnf\n\n# Remove unnecessary files\nrm -v client.csr server.csr extfile.cnf extfile-client.cnf\n\n# Change private and public key permissions\nchmod -v 0400 ca-key.pem key.pem server-key.pem\nchmod -v 0444 ca.pem server-cert.pem cert.pem\n\n# Moving server certs to their permanent location\nsudo mkdir -p /etc/docker/certs\nsudo mv server*.pem /etc/docker/certs\nsudo cp ca.pem /etc/docker/certs\n\n# Add system.d override configuration for Docker to start the TCP with TLS port\nsudo mkdir -p /etc/systemd/system/docker.service.d/\nsudo su -c 'echo \"# /etc/systemd/system/docker.service.d/override.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd -H fd:// --tlsverify --tlscacert=/etc/docker/certs/ca.pem --tlscert=/etc/docker/certs/server-cert.pem --tlskey=/etc/docker/certs/server-key.pem -H 0.0.0.0:2376\" &gt;&gt; /etc/systemd/system/docker.service.d/override.conf'\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n\n# Test the TLS connection with curl\ncurl https://127.0.0.1:2376/images/json --cert ~/certs/cert.pem --key ~/certs/key.pem --cacert ~/certs/ca.pem\n\n# Create an archive with the client certs\ntar czvf certs.tgz ca.pem cert.pem key.pem\n</code></pre> <p>The archive file <code>~/certs/certs.tgz</code> will have to be transferred to your desktop. We will use its content to log into the Docker daemon from your desktop.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#adding-assemblyline-specific-configuration","title":"Adding Assemblyline specific configuration","text":""},{"location":"developer_manual/env/pycharm/remote_development/#assemblyline-folders","title":"Assemblyline folders","text":"<p>Because Assemblyline uses its own set of folders inside the core, service-server, and UI container, we have to create the same folder structure here so that we can run the components in debug mode.</p> <pre><code>sudo mkdir -p ~/git\n\nsudo mkdir -p /etc/assemblyline\nsudo mkdir -p /var/cache/assemblyline\nsudo mkdir -p /var/lib/assemblyline\nsudo mkdir -p /var/log/assemblyline\n\nsudo chown $USER /etc/assemblyline\nsudo chown $USER /var/cache/assemblyline\nsudo chown $USER /var/lib/assemblyline\nsudo chown $USER /var/log/assemblyline\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#assemblyline-dev-configuration-files","title":"Assemblyline dev configuration files","text":"<p>Here we will create configuration files that match the default dev Docker Compose configuration files so that we can swap any of the components to the one that is being debugged.</p> <pre><code>echo \"enforce: true\" &gt; /etc/assemblyline/classification.yml\necho \"\nauth:\n  internal:\n    enabled: true\n\ncore:\n  alerter:\n    delay: 0\n  metrics:\n    apm_server:\n      server_url: http://localhost:8200/\n    elasticsearch:\n      hosts: [http://elastic:devpass@localhost]\n\ndatastore:\n  ilm:\n    indexes:\n      alert:\n        unit: m\n      error:\n        unit: m\n      file:\n        unit: m\n      result:\n        unit: m\n      submission:\n        unit: m\n\nfilestore:\n  cache:\n    - file:///var/cache/assemblyline/\n\nlogging:\n  log_level: INFO\n  log_as_json: false\n\nui:\n  audit: false\n  debug: false\n  enforce_quota: false\n  fqdn: `ip route get 8.8.8.8 | grep 8.8.8.8 | awk '{ print $7 }'`.nip.io\n\" &gt; /etc/assemblyline/config.yml\n</code></pre> <p>Tip</p> <p>As you can see in the last command we are setting the FQDN to YOUR_IP.nip.io. NIP.IO is a service that will resolve the first part of the domain <code>YOUR_IP</code>.nip.io to its IP value. We use this to fake DNS when there are none. This is especially useful for oAuth because some providers are forbidding redirect URLs to IPs. You can also replace the FQDN with your own DNS name if you have one.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#setup-python-virtual-environments","title":"Setup Python Virtual Environments","text":"<p>We will make two Python virtual environments:</p> <ul> <li>One for the core components</li> <li>One for services</li> </ul> <p>That should be enough to cover most cases. If a service has conflicting dependencies with another, I suggest you create a separate virtualenv for it when you try to debug it. The core components should all be fine in the same environment.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#setting-up-core-virtualenv","title":"Setting up Core Virtualenv","text":"<pre><code># Make sure the venv directory exists and we are in it\nmkdir -p ~/venv\ncd ~/venv\n\n# Create the virtualenv\npython3.9 -m venv core\n\n# Install Assemblyline packages with their test dependencies\n~/venv/core/bin/pip install assemblyline[test] assemblyline-core[test] assemblyline-service-server[test] assemblyline-ui[test]\n\n# Remove Assemblyline packages because we will use the live code\n~/venv/core/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-server assemblyline-ui\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#setting-up-service-virtualenv-optional","title":"Setting up Service Virtualenv (optional)","text":"<pre><code># Make sure the venv directory exists and we are in it\nmkdir -p ~/venv\ncd ~/venv\n\n# Create the virtualenv\npython3.9 -m venv services\n\n# Install Assemblyline Python client\n~/venv/services/bin/pip install assemblyline-client\n\n# Install Assemblyline service packages\n~/venv/services/bin/pip install assemblyline-service-client assemblyline-v4-service\n\n# Remove Assemblyline packages because we will use the live code\n~/venv/services/bin/pip uninstall -y assemblyline assemblyline-core assemblyline-service-client assemblyline-v4-service\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#on-your-desktop","title":"On your desktop","text":"<p>We are now done setting up the target VM. For the rest of the instructions, we will mainly setup your PyCharm IDE to interface with the target VM.</p>"},{"location":"developer_manual/env/pycharm/remote_development/#get-your-docker-certs-and-install-them","title":"Get your Docker certs and install them","text":"<pre><code>mkdir -p ~/docker_certs\ncd ~/docker_certs\nscp USER_OF_TARGET_VM@IP_OF_TARGET_VM:certs/certs.tgz ~/docker_certs/\ntar zxvf certs.tgz\nrm certs.tgz\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#install-pycharm","title":"Install PyCharm","text":"<p>You can download PyCharm Professional directly from JetBrains's website but if your desktop is running Ubuntu 20.04, you can just install it with <code>snap</code>:</p> <pre><code>sudo snap install --classic pycharm-professional\n</code></pre>"},{"location":"developer_manual/env/pycharm/remote_development/#install-git","title":"Install Git","text":"<p>You can get Git directly from GIT's website but if your desktop is running Ubuntu 20.04 you can just install it with <code>APT</code>:</p> <pre><code>sudo apt install -y git\n</code></pre> <p>Tip</p> <p>You should add your desktop SSH keys to your GitHub account to use Git via SSH. Follow these instructions to do so: GitHub Help</p>"},{"location":"developer_manual/env/pycharm/remote_development/#clone-repositories","title":"Clone repositories","text":""},{"location":"developer_manual/env/pycharm/remote_development/#core-components","title":"Core components","text":"<p>Create the core working directory</p> <pre><code>mkdir -p ~/git/alv4\ncd ~/git/alv4\n</code></pre> <p>Clone Assemblyline's repositories</p> Git via SSHGit via HTTPS <p>Use SSH if you have your SSH id_rsa file configured to your GitHub account </p><pre><code>git clone git@github.com:CybercentreCanada/assemblyline-base.git\ngit clone git@github.com:CybercentreCanada/assemblyline-core.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-client.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-server.git\ngit clone git@github.com:CybercentreCanada/assemblyline-ui.git\ngit clone git@github.com:CybercentreCanada/assemblyline-v4-service.git\n</code></pre><p></p> <p>Use HTTPS if you don't have your GitHub account configured with an SSH key </p><pre><code>git clone https://github.com/CybercentreCanada/assemblyline-base.git\ngit clone https://github.com/CybercentreCanada/assemblyline-core.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-client.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-server.git\ngit clone https://github.com/CybercentreCanada/assemblyline-ui.git\ngit clone https://github.com/CybercentreCanada/assemblyline-v4-service.git\n</code></pre><p></p>"},{"location":"developer_manual/env/pycharm/remote_development/#services-optional","title":"Services (optional)","text":"<p>Create the service working directory</p> <pre><code>mkdir -p ~/git/services\ncd ~/git/services\n</code></pre> <p>Clone Assemblyline's services repositories</p> Git via SSHGit via HTTPS <p>Use SSH if you have your SSH id_rsa file configured to your GitHub account </p><pre><code>git clone git@github.com:CybercentreCanada/assemblyline-service-apivector.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-antivirus.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-apkaye.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-avclass.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-batchdeobfuscator.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-capa.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-cape.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-characterize.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-configextractor.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-deobfuscripter.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-elf.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-elfparser.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-emlparser.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-espresso.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-extract.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-floss.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-frankenstrings.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-iparse.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-metapeek.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-oletools.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pdfid.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-peepdf.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pe.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-pixaxe.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-safelist.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-sigma.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-suricata.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-swiffer.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-torrentslicer.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-unpacker.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-unpacme.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-vipermonkey.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-virustotal.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git\ngit clone git@github.com:CybercentreCanada/assemblyline-service-yara.git\n</code></pre><p></p> <p>Use HTTPS if you don't have your GitHub account configured with an SSH key </p><pre><code>git clone https://github.com/CybercentreCanada/assemblyline-service-apivector.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-antivirus.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-apkaye.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-avclass.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-batchdeobfuscator.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-capa.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-cape.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-characterize.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-configextractor.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-deobfuscripter.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-elf.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-elfparser.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-emlparser.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-espresso.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-extract.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-floss.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-frankenstrings.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-iparse.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-metapeek.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-oletools.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pdfid.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-peepdf.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pe.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-pixaxe.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-safelist.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-sigma.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-suricata.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-swiffer.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-torrentslicer.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-unpacker.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-unpacme.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-vipermonkey.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-virustotal.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-XLMMacroDeobfuscator.git\ngit clone https://github.com/CybercentreCanada/assemblyline-service-yara.git\n</code></pre><p></p>"},{"location":"developer_manual/env/pycharm/remote_development/#setup-pycharm-for-core","title":"Setup PyCharm for core","text":"<p>Start with loading the core directory in Pycharm:</p> <p>Load core folder</p> <ol> <li>Load Pycharm Professional<ul> <li>Choose whatever configuration option you want until the <code>Welcome screen</code></li> </ul> </li> <li>Click the <code>Open</code> button</li> <li>Choose the <code>~/git/alv4</code> directory</li> </ol> <p>The setup of the remote deployment interpreter:</p> <p>Setup core remote interpreter</p> <ol> <li>Click <code>Files</code> -&gt; <code>Settings</code></li> <li>Select <code>Project: alv4</code> -&gt; <code>Python Interpreter</code></li> <li>Click the <code>cog wheel</code> on the top right -&gt; <code>Add</code></li> <li>Select <code>SSH Interpreter</code> -&gt; <code>New Configuration</code><ul> <li>Host: IP or DNS name of your target VM</li> <li>Username: username of the user on the target VM</li> <li>Port: 22 unless you changed it...</li> </ul> </li> <li>Click <code>Next</code></li> <li>Put your target VM password in the box, check <code>Save password</code>, and click <code>Next</code></li> <li>In the next window, do the following:<ul> <li>For the <code>interpreter box</code>, click the little <code>folder</code> and select your core venv (<code>/home/YOUR_TARGET_USER/venv/core/bin/python3.9</code>)</li> <li>For the <code>Sync folders</code> box, click the little <code>folder</code> and for the remote path set the path to: <code>/home/YOUR_TARGET_USER/git/alv4</code> then click <code>OK</code> (ensure target directory has write permissions for all users)</li> <li>Make sure <code>Automatically upload files to the server</code> is checked</li> <li>Make sure  <code>Execute code using this interpreter with root privileges via sudo</code> is checked</li> <li>Hit <code>Finish</code></li> </ul> </li> <li>Click <code>Ok</code></li> <li>Let it load the interpreter and do the transfers</li> </ol> <p>Finally link Docker for remote management:</p> <p>Setup Docker remote management</p> <ol> <li>Click <code>Files</code> -&gt; <code>Settings</code></li> <li>Select <code>build, Execution, Deployment</code> -&gt; <code>Docker</code></li> <li>Click the little <code>+</code> on top left</li> <li>Select <code>TCP Socket</code><ul> <li>In engine API URL put: <code>https://TARGET_VM_IP:2376</code></li> <li>In Certificates folder, click the little folder and browse to <code>~/docker_certs</code> directory</li> </ul> </li> <li>Click <code>OK</code></li> </ol>"},{"location":"developer_manual/env/pycharm/remote_development/#setup-pycharm-for-service-optional","title":"Setup PyCharm for service (optional)","text":"<p>Start with loading the core directory in PyCharm:</p> <p>Load services folder</p> <ol> <li>From your core PyCharm window open the <code>File menu</code> then click <code>Open</code></li> <li>Choose the <code>~/git/services</code> directory</li> <li>Select <code>New Window</code></li> </ol> <p>The setup of the remote deployment interpreter:</p> <p>Setup services remote interpreter</p> <ol> <li>Click <code>Files</code> -&gt; <code>Settings</code></li> <li>Select <code>Project: services</code> -&gt; <code>Python Interpreter</code></li> <li>Click the <code>cog wheel</code> on the top right -&gt; <code>Add</code></li> <li>Select <code>SSH Interpreter</code> -&gt; <code>New Configuration</code><ul> <li>Host: IP or DNS name of your target VM</li> <li>Username: username of the user on the target VM</li> <li>Port: 22 unless you changed it...</li> </ul> </li> <li>Click <code>Next</code></li> <li>Put your target VM password in the box, check <code>Save password</code>, and click <code>Next</code></li> <li>In the next window, do the following:<ul> <li>For the <code>interpreter box</code>, click the little <code>folder</code> and select your core venv (<code>/home/YOUR_TARGET_USER/venv/services/bin/python3.9</code>)</li> <li>For the <code>Sync folders</code> box, click the little <code>folder</code> and for the remote path set the path to: <code>/home/YOUR_TARGET_USER/git/services</code> then click <code>OK</code> (ensure target directory has write permissions for all users)</li> <li>Make sure <code>Automatically upload files to the server</code> is checked</li> <li>Make sure  <code>Execute code using this interpreter with root privileges via sudo</code> is checked</li> <li>Hit <code>Finish</code></li> </ul> </li> <li>Click <code>Ok</code></li> <li>Let it load the interpreter and do the transfers</li> </ol>"},{"location":"developer_manual/env/pycharm/remote_development/#use-pycharm","title":"Use Pycharm","text":"<p>Now that your remote development VM is set up you should read the use PyCharm documentation to get yourself started.</p>"},{"location":"developer_manual/env/pycharm/use_pycharm/","title":"Use PyCharm","text":""},{"location":"developer_manual/env/pycharm/use_pycharm/#use-pycharm","title":"Use PyCharm","text":"<p>Here are some pointers on how to run most of the core components live in PyCharm when either your local or remote development VM is ready.</p>"},{"location":"developer_manual/env/pycharm/use_pycharm/#load-docker-compose-files","title":"Load docker-compose files","text":""},{"location":"developer_manual/env/pycharm/use_pycharm/#minimal-dependencies","title":"Minimal Dependencies","text":"<p>When dependencies are loaded, you can launch any core components and their dependencies should be satisfied.</p> PyCharm ProfessionalDocker Compose (PyCharm Community) <ol> <li>In your project file viewer</li> <li>Browse to <code>assemblyline-base/dev/depends</code></li> <li>Right-click on <code>docker-compose-minimal.yml</code></li> <li>Select <code>Create 'depends/docker-compose...'</code></li> <li>Click <code>OK</code></li> <li>You'll notice on the top right that a new Compose deployment has been created for dependencies. Hit the play button next to it to launch the containers.</li> </ol> <p>Tip</p> <p>From now on you can just select that Compose deployment for dependencies from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name.</p> <p>In a new terminal on the VM, run the following commands:</p> <pre><code>cd ~/git/alv4/assemblyline-base/dev/depends/\nsudo docker compose -f docker-compose-minimal.yml up\n</code></pre> <p>Tip</p> <p>This will take over the terminal and the logs will be displayed there. Hit <code>ctrl-c</code> when you want to shut the containers down.</p> <p>If you close the terminal and the containers keep running, run the following commands to shut down the containers:</p> <pre><code>cd ~/git/alv4/assemblyline-base/dev/depends/\nsudo docker compose -f docker-compose-minimal.yml down\n</code></pre>"},{"location":"developer_manual/env/pycharm/use_pycharm/#core-services","title":"Core services","text":"<p>Core services depend on the dependencies in the <code>docker-compose</code> file. When the core services are loaded you should be able to point a browser at <code>https://IP_OF_VM</code> and you'll be greeted with a working version of Assemblyline with no services configured.</p> <p>Note</p> <p>Default admin user credentials are:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul> PyCharm ProfessionalDocker Compose (PyCharm Community) <ol> <li>In your project file viewer</li> <li>Browse to <code>assemblyline-base/dev/core</code></li> <li>Right-click on <code>docker-compose.yml</code></li> <li>Select <code>Create 'core: Compose Deploy...'</code></li> <li>Click <code>OK</code></li> <li>You'll notice on the top right that a new Compose deployment has been created for core services, hit the play button next to it to launch the containers.</li> </ol> <p>Tip</p> <p>From now on you can just select that Compose deployment for core components from the dropdown up top and hit the run button to launch it. It is good practice to Edit the configuration and give it a proper name.</p> <p>In a new terminal on the VM, run the following commands: </p><pre><code>cd ~/git/alv4/assemblyline-base/dev/core/\nsudo docker compose up\n</code></pre><p></p> <p>Tip</p> <p>This will take over the terminal and the logs will be displayed there. Hit <code>ctrl-c</code> when you want to shut the containers down.</p> <p>If you close the terminal and the containers keep running, run the following commands to shut down the containers: </p><pre><code>cd ~/git/alv4/assemblyline-base/dev/core/\nsudo docker compose down\n</code></pre><p></p>"},{"location":"developer_manual/env/pycharm/use_pycharm/#load-a-single-container-live","title":"Load a single container live","text":"<p>Loading a single container is very useful to test a newly create production service container or to launch the frontend while the backend is being debugged.</p> <p>You can easily load any container live in your Assemblyline Dev environment by following these instructions:</p> <p>Note</p> <p>For this demo we will assume that you want to run the service container from the developing an Assemblyline service documentation.</p> PyCharm ProfessionalDocker (PyCharm Community) <ol> <li>Click the <code>Run</code> menu then select <code>Edit Configurations...</code></li> <li>Click the <code>+</code> button at the top left`</li> <li>Select \"Docker Image\"</li> <li>In the <code>Name</code> field at the top, set the name to: <code>Sample Service - Container</code></li> <li>Set the <code>Image ID</code> to: <code>testing/assemblyline-service-sample</code></li> <li>Set the <code>Container Name</code> to SampleService</li> <li>Click the modify options and select <code>Environment variables</code></li> <li>Add the following environment variable: <code>SERVICE_API_HOST=http://172.17.0.1:5003</code></li> <li>Click the modify options and select <code>Run options</code></li> <li>Add the following <code>Run option</code>: <code>-network host</code></li> <li>Click <code>OK</code></li> </ol> <p>Tip</p> <p>From now on you can just select the <code>Sample Service - Container</code> run configuration from the dropdown up top and hit the run button to launch it.</p> <p>Pycharm Community does not have support for managing Docker containers, therefore you will have to run your containers using a <code>shell</code>.</p> <p>In a new terminal on the VM, run the following commands: </p><pre><code>docker run --env SERVICE_API_HOST=http://172.17.0.1:5003 --network=host --name SampleService testing/assemblyline-service-sample\n</code></pre><p></p> <p>Tip</p> <p>This will take over the terminal and the logs will be displayed there. Hit <code>ctrl-c</code> when you want to shut the containers down.</p>"},{"location":"developer_manual/env/pycharm/use_pycharm/#run-components-live-from-pycharm","title":"Run components live from PyCharm","text":""},{"location":"developer_manual/env/pycharm/use_pycharm/#core-components","title":"Core Components","text":"<p>Most of the core components are going to be as easy to run as finding the component main file then hit <code>Run</code> or <code>Debug</code>... All core components require you to run the <code>Minimal Dependencies</code> <code>docker-compose</code> file before launching them.</p> <p>Launching the API Server</p> <ol> <li>Find the UI main file in the project files browser<ul> <li><code>assemblyline-ui/assemblyline_ui/app.py</code></li> </ul> </li> <li>Right-click on it</li> <li>Select either <code>Run 'app'</code> or <code>Debug 'app'</code></li> </ol>"},{"location":"developer_manual/env/pycharm/use_pycharm/#live-services","title":"Live Services","text":"<p>The main exception to very easy-to-run components is debugging services live in the system. Services require you to run two separate components to process files. The two components talk via named pipes in the /tmp folder. Inside the Docker container, this is very easy but debugging this live requires some sacrifices.</p> <p>Limitations</p> <ol> <li>You can only run one live debugging service at the time.</li> <li>Depending on where you stop them and how you stop them, they don't fully clean up after themselves.</li> <li>They require extra configuration, it's not just \"click-and-go\".</li> </ol> <p>We will show you how to run the Sample service live in the system created in the developing and Assemblyline service documentation. This requires you to run the full infrastructure using the <code>docker-compose</code> files before executing the steps. (Minimal dependencies and Core services)</p> <p>Important</p> <p>Run the following example inside the PyCharm window pointing to the services (<code>~/git/services</code>)</p> <p>Example</p> <p>Setup <code>Task Handler</code> run configuration if it does not exist:</p> <ol> <li>Click the \"Run\" menu then select \"Edit Configurations...\"</li> <li>Click the <code>+</code> button at the top left</li> <li>Click <code>Python</code></li> <li>The first option in the configuration tab is a drop-down that says <code>Script path:</code> click it and choose <code>Module Name:</code></li> <li>In the box beside it, write: <code>assemblyline_service_client.task_handler</code></li> <li>In the name box at the top, write <code>Task Handler</code></li> <li>Click the <code>OK</code> Button</li> </ol> <p>Now if you click the green play button beside the newly created <code>Task Handler</code> run configuration, Task Handler will be waiting for the service to start.</p> <p>Setup the new service configuration:</p> <ol> <li>Click the \"Run\" menu then select \"Edit Configurations...\"</li> <li>Click the <code>+</code> button at the top left</li> <li>Click <code>Python</code></li> <li>The first option in the configuration tab is a drop-down that says <code>Script path:</code> click it and choose <code>Module Name:</code></li> <li>In the box beside it, write: <code>assemblyline_v4_service.run_service</code></li> <li>Add <code>;SERVICE_PATH=sample.Sample</code> to the <code>Environment variables</code></li> <li>Set the working directory to: <code>assemblyline-service-sample</code> by pressing the little <code>folder</code> on the right and browsing to that directory</li> <li>In the name box at the top, write <code>Sample Service - LIVE</code></li> <li>Click <code>OK</code></li> <li>Now that dropdown near the top says <code>Sample Service - LIVE</code>, click the <code>play</code> or the <code>bug</code> button to either <code>Run</code> or <code>Debug</code> the service<ul> <li>In the Run or Debug window, the service will be stuck at Waiting for receive task named pipe to be ready.... This is because task_handler shut down after registering the service for the first time.</li> </ul> </li> <li>Select <code>task_handler</code> from the dropdown near the top and click the <code>play</code> or <code>bug</code> button beside it again.<ul> <li>Now both <code>Sample Service - LIVE</code> and <code>Task Handler</code> will stay up and poll for tasks from the service server.</li> </ul> </li> </ol>"},{"location":"developer_manual/env/vscode/setup_script/","title":"Setup script","text":""},{"location":"developer_manual/env/vscode/setup_script/#setup-script","title":"Setup script","text":"<p>Assemblyline's VSCode installation is entirely scripted. It will set up the following things for you:</p> <ul> <li>Install VSCode via <code>snap</code> (Optional)</li> <li>Install AL4 development dependencies</li> <li>Clone all core component sources from GitHub</li> <li>Clone all service sources from GitHub (Optional)</li> <li>Create a virtual Python environment for core component development</li> <li>Create a virtual Python environment for service development (optional)</li> <li>Create Run targets inside VSCode for all core components and other important scripts</li> <li>Create Tasks inside VSCode for development using Docker Compose</li> <li>Setup our code formatting standards</li> <li>Deploy a local Docker registry on port 32000</li> </ul> <p>Note</p> <p>We recommend installing the VSCode extensions needed to use this environment once VSCode is launched in the workspace.</p>"},{"location":"developer_manual/env/vscode/setup_script/#pre-requisites","title":"Pre-requisites","text":"<p>The setup script assumes the following:</p> <ul> <li>You are running this on an Ubuntu machine / VM (20.04 and up).</li> <li>VSCode does not have to be running on the same host where you run this script so run the setup script on the target VM of a remote development setup.</li> <li>You have read the setup_vscode.sh script. This script will install and configure packages for ease of use.</li> </ul> <p>Important</p> <p>If you are uncomfortable with some of the changes that the script makes, you should comment them out before running the script.</p>"},{"location":"developer_manual/env/vscode/setup_script/#installation-instruction","title":"Installation instruction","text":"<p>Create your repository directory</p> <pre><code>mkdir -p ~/git\ncd ~/git\n</code></pre> <p>Clone repository</p> <pre><code>git clone https://github.com/CybercentreCanada/assemblyline-development-setup alv4\n</code></pre> <p>Run setup script. Choose the type of development you want to do and on what type of system.</p> Core only (Local)Core and Services (Local)Core only (Remote)Core and Services (Remote) <pre><code>cd alv4\n./setup_vscode.sh -c\n</code></pre> <pre><code>cd alv4\n./setup_vscode.sh -c -s\n</code></pre> <pre><code>cd alv4\n./setup_vscode.sh\n</code></pre> <pre><code>cd alv4\n./setup_vscode.sh -s\n</code></pre> <p>Important</p> <p>When running the setup script for the <code>Core and Services</code> installation you will get two dev folders: <code>~/git/alv4</code> and <code>~/git/services</code>.</p> <p>The reason for this is that we want to make sure that service Python dependencies don't interfere with core component dependencies. Therefore, two separate <code>venv</code> are created with different sets of dependencies. The service <code>venv</code> will point to the core components' live code to install <code>assemblyline-base</code>, <code>assemblyline-core</code>, <code>assemblyline-v4-service</code>, and <code>assemblyline-client</code>. That way, any modification you do to the core package code will be reflected in your service instantly.</p>"},{"location":"developer_manual/env/vscode/setup_script/#post-installation-instructions","title":"Post-installation instructions","text":"<p>When the installation is complete, you will be asked to reboot the VM. This is required for sudo-less Docker to work.</p> <p>After the VM has finished rebooted, you can use a shell to open VSCode:</p> <pre><code>code ~/git/alv4\n</code></pre> <p>Note</p> <p>If you've installed the services, you should open another VSCode window pointing to the services folder. </p><pre><code>code ~/git/services\n</code></pre><p></p>"},{"location":"developer_manual/env/vscode/setup_script/#install-recommended-extensions","title":"Install recommended extensions","text":"<p>To take full advantage of this setup, we strongly advise installing the recommended extensions when prompted or typing <code>@recommended</code> in the <code>Extensions tab</code>.</p> <p></p>"},{"location":"developer_manual/env/vscode/setup_script/#start-using-vscode","title":"Start using VSCode","text":"<p>You can now refer to the \"use VSCode\" instructions to get you started using the VSCode environment with Assemblyline.</p>"},{"location":"developer_manual/env/vscode/use_vscode/","title":"Use VSCode","text":""},{"location":"developer_manual/env/vscode/use_vscode/#use-vscode","title":"Use VSCode","text":"<p>Once you are done running the setup script, your installation of VSCode will be ready to run and debug any components of Assemblyline and most launch targets are already pre-configured. This page will point you in the right direction to perform some of the more common tasks that you'll have to do when developing an aspect of Assemblyline.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#running-tasks","title":"Running Tasks","text":"<p>After all recommended extensions are finished installing in VSCode, the task button on the sidebar should be revealed and will give you quick access to the most important tasks in the system.</p> <p></p> <p>These tasks are split into 3 categories:</p> <ol> <li>Container - Run a single container for a single task</li> <li>Docker-compose - Execute a set of containers for a specific task dependency</li> <li>Pytest dependencies - Run necessary dependencies to run tests</li> </ol> <p>Tip</p> <p>You can edit the task list by modifying the <code>.vscode/tasks.json</code>. The default <code>task.json</code> file can be found in the assemblyline-development-setup repository.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#container-tasks","title":"Container tasks","text":"<p>A container task executes one specific container in the system. Two of these tasks are predefined:</p>"},{"location":"developer_manual/env/vscode/use_vscode/#frontend","title":"Frontend","text":"<p>The Frontend task is used to run the User Interface of Assemblyline. It is only useful for when you launch the Assemblyline API Server in debug mode and are NOT using the <code>core components</code> task. Assemblyline's frontend is now built using ReactJS and is served via <code>NPM serve</code> which is why it is not part of this setup. Refer to the frontend development page for more information on how to do development on the Assemblyline frontend.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#resultsample","title":"ResultSample","text":"<p>The ResultSample task was created to show the developers how to run newly created service containers in the system.</p> <p>Deep dive in the ResultSample Task</p> <p>This is what the JSON block for executing the ResultSample service in VSCode looks like: </p><pre><code>...\n{\n    \"label\": \"Container - ResultSample\",\n    \"type\": \"shell\",\n    \"options\": {\n        \"env\": {\n            \"LOCAL_IP\": \"172.17.0.1\"\n        }\n    },\n    \"command\": \"docker run --env SERVICE_API_HOST=http://${LOCAL_IP}:5003 --network=host cccs/assemblyline-service-resultsample\",\n    \"runOptions\": {\n        \"instanceLimit\": 1\n    }\n},\n...\n</code></pre><p></p> <p>Essentially, this runs the <code>docker run</code> command and specifies where the service server API is located. You can change the <code>LOCAL_IP</code> environment variable if your Docker subnet is different.</p> <p>If you want to make sure Docker's local IP is indeed the default, <code>172.17.0.1</code>, just run this command: </p><pre><code>ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d\"/\"\n</code></pre><p></p>"},{"location":"developer_manual/env/vscode/use_vscode/#docker-compose-tasks","title":"Docker-compose tasks","text":"<p>The <code>docker-compose</code> tasks are used to run sets of predefined dependencies in the system.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#dependencies","title":"Dependencies","text":"<p>Before trying to run anything, at the bare minimum you will need one of the two <code>Dependencies</code> tasks running.</p> <ul> <li><code>Dependencies (Basic)</code> will run the bare minimum set of containers to start components in the system: Elasticsearch, Redis, Minio, and Nginx.</li> <li><code>Dependencies (Basic + Kibana)</code> will run the same containers as the Basic task but will add Kibana, Filebeat, and APM so that you can have access to the Kibana dashboard and debug your system more efficiently.</li> </ul>"},{"location":"developer_manual/env/vscode/use_vscode/#core-components","title":"Core components","text":"<p>The <code>core components</code> <code>docker-compose</code> task runs all Assemblyline core components:</p> <ul> <li>Service server</li> <li>Frontend</li> <li>API Server</li> <li>Socket IO Server</li> <li>Alerter</li> <li>Expiry</li> <li>Metrics</li> <li>Heartbeats</li> <li>Statistics</li> <li>Workflow</li> <li>Plumber</li> <li>Dispatcher</li> <li>Ingester</li> </ul> <p>Tip</p> <p>If you are wondering what each component does, you should read the Infrastructure documentation which gives a brief description of each component.</p> <p>The <code>core components</code> task will also add two test users to the system:</p> uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin"},{"location":"developer_manual/env/vscode/use_vscode/#scaler-and-updater","title":"Scaler and Updater","text":"<p>This task is an extra core component task that will launch Updater and Scaler. This task requires you to run one of the <code>dependency</code> tasks as well as the <code>core components</code> task to run properly. Scaler and Updater have been separated from the <code>core components</code> task because they interfere with running services live in the system. You are unlikely to ever run this task unless you are working on an issue loading containers from Scaler or Updater.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#services","title":"Services","text":"<p>This task will register all services to the system and allow them to be instantiated by Scaler and Updater. You are unlikely to run this task unless you want a working dev system that can scan files just like an appliance can.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#pytest-dependencies","title":"Pytest dependencies","text":"<p>The Pytest dependencies tasks are used to set up the environment properly to be able to run the tests like if you were building the packages. There are 4 possible dependencies for the tests:</p> <ul> <li>Base - To run the tests for the <code>assemblyline-base</code> repository</li> <li>Core - To run the tests for the <code>assemblyline-core</code> repository</li> <li>Service Server - To run the tests for the <code>assemblyline-service-server</code> repository</li> <li>UI or All - To run the tests for the <code>assemblyline-ui</code> repository or any other repository for that matter</li> </ul> <p>Tip</p> <p>In most cases, if you are running tests you can use <code>UI or All</code> tasks because all the tests will work with it, but you might want to use the other tasks to save on resources and on dependency loading time.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#launching-components-in-debug-mode","title":"Launching components in debug mode","text":"<p>Using debug mode on all of Assemblyline's components will probably be the single most useful thing that is pre-configured by the setup script. All Assemblyline's components get a launch target out of the box to simplify your debugging needs. Simply click the <code>Run/Debug</code> quick access side menu, select the component that you want to launch, and click the play button.</p> <p></p> <p>The configured launch targets have been pre-fixed with category to help you identify what they do:</p> <ul> <li>Core - Core components unrelated to services</li> <li>Data - Script that generates random data in the system</li> <li>Service Server - Service API server</li> <li>Service - Service Live debugging</li> <li>UI - Assemblyline API Servers</li> </ul> <p>Warning</p> <p>To be able to successfully run these components, you will have to run at minimum the Dependencies (Basic) tasks</p>"},{"location":"developer_manual/env/vscode/use_vscode/#core-service-server-and-ui","title":"Core, Service Server, and UI","text":"<p>The core, service server, and UI categories of launch targets are mostly self-explanatory. They will launch in debug mode any of the core components from the system.</p> <p>Tip</p> <p>If you are wondering what each component does, you should read the Infrastructure which gives a brief description of every single one of them.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#cli-command-line-interface","title":"CLI - Command Line Interface","text":"<p>This launch target from the core category launches an interactive console that will let perform a specific task in the system. You can use the <code>help</code> command to find out what possible commands are available.</p> <p></p> <p>Warning</p> <p>The commands that are found in this interactive console can be very dangerous and most of them should not be run on a production system.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#data-launch-target","title":"Data Launch target","text":"<p>The launch targets in the data category are used to create random data in the system.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#create-default-users","title":"Create default users","text":"<p>This launch target will generate the same default user as the Core components tasks does:</p> uid password is_admin apikey user user no devkey:user admin admin yes devkey:admin"},{"location":"developer_manual/env/vscode/use_vscode/#generate-random-data","title":"Generate random data","text":"<p>In addition to creating the default users, this task will also use the random data generator to fill the different indexes of the system with data that has been randomly generated. This is especially useful when testing APIs or Frontend which require you to have data in the different indices.</p>"},{"location":"developer_manual/env/vscode/use_vscode/#running-live-services","title":"Running live services","text":"<p>Running services in a live Assemblyline dev environment is the most important thing to know while building a service. It will allow you to send files via the user interface and put breakpoints in your service to catch files as they come through for processing.</p> <p>To run a service live in the system you will need to launch two components:</p> <ul> <li>Task Handler</li> <li>Your service</li> </ul> <p>Warning</p> <p>Here are a few things to consider before running a service live in debug mode:</p> <ol> <li>You need to start both the Dependencies (Basic) and the Core components tasks before starting your service to ensure that it will receive files.</li> <li>You can have one Task handler instance running at the time therefore can only debug one service at the time. Task handler communicates with the service via fixed named pipes in the <code>/tmp</code> directory which is the reason for that limitation.</li> <li>The first time you start a service, you'll have to start the task handler and your service twice because the service stops itself after registering in the DB.</li> <li>You should launch your service from the VSCode window pointing to your services git folder (<code>~/git/services</code>) and for it to exist, you should have run the <code>setup_script.sh</code> with the <code>-s</code> option.</li> </ol>"},{"location":"developer_manual/env/vscode/use_vscode/#create-a-launch-target-for-your-service","title":"Create a launch target for your service","text":"<p>To add a launch target for your service, you will have to modify the <code>.vscode/launch.json</code> file in your <code>~/git/services</code> directory. You can mimic the Safelist launch target as a baseline.</p> <p>Demo Safelist launch target</p> <pre><code>...\n    {\n        \"name\": \"[Service] Safelist - LIVE\",\n        \"type\": \"python\",\n        \"request\": \"launch\",\n        \"module\": \"assemblyline_v4_service.run_service\",\n        \"env\": {\n            \"SERVICE_PATH\": \"safelist.Safelist\"\n        },\n        \"console\": \"internalConsole\",\n        \"cwd\": \"${workspaceFolder}/assemblyline-service-safelist\"\n    },\n...\n</code></pre> <p>The only three things you will have to change for launching your service are the following</p> <ol> <li>The name of the launch target so you can pick it from the list later</li> <li>The environment variable <code>SERVICE_PATH</code>. This should be the Python path to the service class relative to your current working directory.</li> <li>The current working directory. This should be the directory where your service code is, a new directory you've created for your service inside the <code>~/git/services</code> folder.</li> </ol>"},{"location":"developer_manual/env/vscode/use_vscode/#launch-the-service","title":"Launch the service","text":"<p>Launching the service will take place again in the VSCode window dedicated to services. From the <code>Run/Debug</code> quick access sidebar, you will launch the <code>Task handler</code> first then the launch target for your service that you've just created.</p> <p></p> <p>Your service should now be running in live mode!</p>"},{"location":"developer_manual/env/vscode/use_vscode/#running-tests","title":"Running Tests","text":"<p>After running the setup script for VSCode, the testing interface from the VSCode Python extension will be shown in your quick access side menu and will present you with all available unit tests. You can then simply click the test or group of tests you want to run and hit the <code>play</code> button to run the tests.</p> <p>Warning</p> <p>Do not forget to first launch the appropriate Pytest Dependency from the tasks otherwise all the tests will fail.</p> <p></p> <p>No tests available?</p> <p>If the testing interface is not showing any tests, load up the 'UI and All' Pytest Dependency task and hit the \"Reload tests\" button.</p>"},{"location":"developer_manual/frontend/frontend/","title":"Frontend","text":""},{"location":"developer_manual/frontend/frontend/#assemblyline-frontend-development","title":"Assemblyline frontend development","text":"<p>This documentation will show you how to set up your environment for Assemblyline frontend development.</p>"},{"location":"developer_manual/frontend/frontend/#libraries","title":"Libraries","text":"<p>First off, Assemblyline's frontend uses the following libraries. Here are some key notes when using them while developing a feature on the frontend.</p>"},{"location":"developer_manual/frontend/frontend/#react","title":"React","text":"<p>The Assemblyline frontend was developed using the React library. It was originally chosen for its popularity and has stood the test of time. Where Angular is a framework because some decisions were made for the developer, React is an unopinionated library allowing the developer to make their own decision for their app. The main advantage of using React is it allows developers to create modular and reusable components throughout the app.</p> <p>ReactDOM: React comes with the ReactDOM library that implements the concept of Virtual DOM (VDOM). It works by storing a virtual representation of the DOM tree in memory and whenever an event causes changes of state such as a user clicking on a button or entering text in an input field, the VDOM will reconcile the differences and re-render the components and associated child components where the state change occurred .</p> <p>React Developer Tools: You can install this tool as a browser extension to be able to view and track the component's state and to optimize the rendering performance of the app.</p> <ul> <li>functions over classes: In Assemblyline, we prefer to use functional components instead of class components as they seem to have been deprecated or are in legacy support.</li> <li>Hooks: Similarly, we use hooks in most use cases ever since they became popular.</li> <li>Components in a single file: Components that are only used a limited amount of time should be stored in the same file that they are used in to avoid having to search all over the place for them.</li> <li>Memoize everything: Performance optimization is a key factor when it comes to developing Assemblyline's frontend where frequent DOM changes occur. As explained before, a component will have to re-render all its content if one of its props, state or state resulting from hooks has changed. As a good rule of thumb, functional components should be wrapped with <code>React.memo()</code>, methods within a component should be implemented with <code>useCallback()</code> and use <code>useMemo()</code> for state-derived variables to avoid unnecessary re-rendering. Note that React 19 will finally remove the need to memoize everything.</li> </ul>"},{"location":"developer_manual/frontend/frontend/#react-router","title":"React Router","text":"<p>This library handles all of the \"client-side routing\" and provides all the hooks to track the state of the location, its parameters and the need to navigate to another location.</p> <ul> <li>Code-Splitting and Lazy loading: In order to reduce the initial bundling size, every page of Assemblyline's frontend is lazy loaded meaning that page's code and all underlying components will only be sent to the user's browser when they first navigate to it.</li> <li>Vite Chunking: When building for production, Vite is configured to create chunks, or to split the resulting JavaScript code into multiple files in order to reduce their size. In effect, this reduces the initial loading time by only sending the chunk of code needed to display the requested page.</li> </ul>"},{"location":"developer_manual/frontend/frontend/#typescript","title":"Typescript","text":"<p>This library adds additional syntax for types that builds on JavaScript. Well integrated into VSCode, it provides the type inference needed to catch errors during development.</p> <ul> <li>Type everything: To make full use of this library, it is encouraged to add typing to all parameters in order to be able to catch potential issues during development.</li> </ul>"},{"location":"developer_manual/frontend/frontend/#material-ui","title":"Material UI","text":"<p>Most of the React components that are used in Assemblyline come from the Material UI (MUI) library. It provides all the standard UI element one might expect with minimalistic animation making easy and fast to develop on the frontend.</p> <p>Material Icon: Most of Assemblyline's icons and symbols are from Material Icon as they are well integrated with its components.</p> <ul> <li>Performance over style: Note that there's a performance trade-off with using the MUI components where you get the functionalities and the styling but lose a lot of performance. In the case of rendering thousands of elements like buttons in a table, it is much faster to use regular HTML and pure CSS since the UI doesn't have to process all the component's parameters. As an example, we rarely use the <code>&lt;Box /&gt;</code> component from MUI as it doesn't offer anything more than a regular <code>&lt;div /&gt;</code> doesn't and the rendering time differences is considerable at scale.</li> </ul>"},{"location":"developer_manual/frontend/frontend/#vite","title":"Vite","text":"<p>Vite.js is a build tool which aims to streamline the web development process, making it faster and more efficient for developers. The term comes from the French word for \"fast\" or \"quick\". The Assemblyline frontend previously used Create React App, but moved from it to get faster startup time, hot module replacement (HMR) and build time.</p>"},{"location":"developer_manual/frontend/frontend/#vitest","title":"Vitest","text":"<p>Because the frontend now uses Vite, it uses Vitest as its testing framework. For the time being, testing components on the frontend is done selectively and mostly on pure functions. Otherwise, having both Typescript and ESLint helps to find issues that might occur during development.</p>"},{"location":"developer_manual/frontend/frontend/#eslint","title":"ESLint","text":"<p>ESLint is a static code analysis tool for identifying problematic patterns or code that don't adhere to certain guidelines or standards in order to maintain code quality and consistency within a codebase. For the most part, the Assemblyline's frontend uses standard rulesets written by the React community.</p> <ul> <li>Vite integration: ESLint was not integrated with Vite runtime to preserve its fast response time during development. To view all the warnings and errors on the frontend's code, execute the <code>pnpm eslint .</code> command.</li> </ul>"},{"location":"developer_manual/frontend/frontend/#install-the-development-environment-prerequisites","title":"Install the development environment prerequisites","text":""},{"location":"developer_manual/frontend/frontend/#clone-the-ui-frontend-code","title":"Clone the UI frontend code","text":"<pre><code>cd ~/git\ngit clone https://github.com/CybercentreCanada/assemblyline-ui-frontend.git\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#install-nodejs-ubuntu","title":"Install Node.js (Ubuntu)","text":"<p>Node.js is the JavaScript runtime that will execute the frontend's code. Using the Node Version Manager (NVM) makes it easy to install and manage your Node.js environment. Use these following commands install the Node.js version 20.</p> <pre><code># installs nvm (Node Version Manager)\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh |\n\n# download and install Node.js (you may need to restart the terminal)\nnvm install 22\n\n# verifies the right Node.js version is in the environment\nnode -v # should print `v22.14.0`\n\n# verifies the right NPM version is in the environment\nnpm -v # should print `10.9.2`\n</code></pre> <p>Otherwise, you can follow these simple commands to install Node.js</p> <pre><code>curl -sL https://deb.nodesource.com/setup_20.x | sudo -E bash -\nsudo apt-get install -y nodejs\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#install-pnpm","title":"Install PNPM","text":"<p>PNPM (short for \"performant npm\") is a fast, disk space-efficient package manager for JavaScript and TypeScript projects. It serves as an alternative to package managers like npm and Yarn, with a unique approach to managing dependencies that significantly improves speed and reduces disk usage.</p> <pre><code># install pnpm\nnpm install --global pnpm\n\n# check that pnpm is installed by running:\npnpm --version\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#install-npm-dependencies","title":"Install NPM dependencies","text":"<p>Still in your <code>assemblyline-ui-frontend</code> directory, run the following command to install all the frontend's dependencies.</p> ~/git/assemblyline-ui-frontend<pre><code># install all dependencies\npnpm install\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#installing-docker-and-docker-compose","title":"Installing Docker and Docker Compose","text":"<p>Installing <code>docker</code> and <code>docker compose</code> on your machine is necessary for development.</p> <ol> <li> <p>Follow the install guide provided by the official Docker documentation:</p> <ul> <li>Install Guide for Ubuntu</li> <li>Install Guide for RHEL</li> <li>Install Guide for Other Platforms</li> </ul> </li> <li> <p>Ensure the installation was successful by invoking the commands:   </p><pre><code>docker version\ndocker compose version\n</code></pre><p></p> </li> </ol>"},{"location":"developer_manual/frontend/frontend/#configure-the-development-environment","title":"Configure the development environment","text":""},{"location":"developer_manual/frontend/frontend/#setup-webpack-for-debugging-behind-a-proxy","title":"Setup Webpack for debugging behind a proxy","text":"<p>Create a file named <code>.env.local</code> at the root of the <code>assemblyline-ui-frontend</code> directory with the following content:</p> ~/git/assemblyline-ui-frontend/.env.local<pre><code>HOST=0.0.0.0\nWDS_SOCKET_PORT=443\nHTTPS=true\nBROWSER=none\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#setup-docker-compose-environment","title":"Setup Docker Compose environment","text":""},{"location":"developer_manual/frontend/frontend/#setup-ip-routing","title":"Setup IP routing","text":"<p>Create a file in the <code>docker</code> directory named <code>.env</code>.</p> <p>This file should only contain the following where <code>&lt;YOUR_IP&gt;</code> is replaced by your development computer IP.</p> ~/git/assemblyline-ui-frontend/docker/.env<pre><code>EXTERNAL_IP=&lt;YOUR_IP&gt;\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#setup-assemblyline-configuration-file","title":"Setup Assemblyline configuration file","text":"<p>From the <code>docker</code> directory, copy the file <code>config.yml.template</code> to <code>config.yml</code> in the same directory.</p> <p>Change the <code>&lt;YOUR_IP&gt;</code> in the newly created <code>config.yml</code>file to the IP of your development machine.</p>"},{"location":"developer_manual/frontend/frontend/#setup-assemblyline-classification-engine-file","title":"Setup Assemblyline classification engine file","text":"<p>From the <code>docker</code> directory, copy the file <code>classification.yml.template</code> to <code>classification.yml</code> in the same directory.</p> <p>Change the <code>enforce</code> value to <code>true</code> in the <code>classification.yml</code> file to turn on the classification engine.</p>"},{"location":"developer_manual/frontend/frontend/#frontend-development-only","title":"Frontend development only","text":"<p>The default <code>docker-compose</code> configuration is setup for users who only want to work on Assemblyline's frontend. It will deploy the required docker containers of <code>nginx</code>, <code>minio</code>, <code>redis</code> and <code>elasticsearch</code> used for any development environment, but will also deploy the <code>ui</code> and <code>socketio</code> instances as docker containers and will create some test data.</p>"},{"location":"developer_manual/frontend/frontend/#dependencies","title":"Dependencies","text":"<p>Go to the <code>docker</code> directory and run the following command to launch the Assemblyline database and user interface.</p> ~/git/assemblyline-ui-frontend/docker<pre><code>docker compose up -d\n</code></pre>"},{"location":"developer_manual/frontend/frontend/#frontend","title":"Frontend","text":"<p>Use the <code>pnpm run start</code> command to launch the frontend.</p> ~/git/assemblyline-ui-frontend<pre><code># start the development instance\npnpm run start\n</code></pre> <p>Access the dev frontend at the following link: <code>https://&lt;YOUR_IP&gt;.nip.io</code></p>"},{"location":"developer_manual/frontend/frontend/#full-stack-web-development","title":"Full-Stack Web Development","text":"<p>This development environment is for users who want to make changes to every aspect of Assemblyline. For example, you may want to create new API routes for a new frontend's page.</p>"},{"location":"developer_manual/frontend/frontend/#docker-configuration","title":"Docker configuration","text":"<p>Open the <code>docker-compose</code> config file located in the <code>assemblyline-ui-frontend/docker</code> directory. Comment out the <code>create_test_data</code>, <code>al_ui</code> and <code>al_socketio</code> services as we will make them run as processes instead of docker containers.</p> <p>You can also uncomment the <code>kibana</code> service if you want a dashboard to view the raw data stored in <code>elasticsearch</code>.</p>"},{"location":"developer_manual/frontend/frontend/#setup-the-core-services","title":"Setup the core services","text":"<p>Follow the instruction highlighted on the setup script page to create the <code>alv4</code> directory with all the core services. With VSCode, you only need to start a <code>[UI] API Server</code> process and maybe <code>[UI] Socket Server</code> if you are working on a component that requires a socket connection like the <code>/dashboard</code>. Keep in mind that any changes made to those processes will require you to reset its instances to see its effect.</p> <p></p>"},{"location":"developer_manual/frontend/frontend/#setup-the-frontend","title":"Setup the Frontend","text":"<p>Just like before, run the following command to start the frontend's instance.</p> ~/git/assemblyline-ui-frontend<pre><code># start the development instance\npnpm run start\n</code></pre> <p>Access the dev frontend at the following link: <code>https://&lt;YOUR_IP&gt;.nip.io</code></p>"},{"location":"developer_manual/plugins/plugins/","title":"Plugins","text":""},{"location":"developer_manual/plugins/plugins/#plugins","title":"Plugins","text":"<p>The API may be extended via plugins. Plugins may be developed externally to Assemblyline to provide additional functionality. They must conform to the defined interface for their function.</p> <p>This section gives you an overview of why plugins exist and what they are used for.</p>"},{"location":"developer_manual/plugins/plugins/#why","title":"Why","text":"<p>Plugins are designed to enable interoperability with other systems. This method allows for the de-coupling of system specific code that is not core to Assemblyline and may not want to be enabled by all users.</p> <p>These plugins enable a single Assemblyline API to be defined that is able to cleanly query all desired systems, no matter how obscure.</p>"},{"location":"developer_manual/plugins/plugins/#architecture","title":"Architecture","text":"<p>Plugins are essentially standalone microservices that act as a proxy layer between Assemblyline and the external system. They exist to translate Assmemblyline specific ontology into queries that the external service will understand. For example, the builtin external lookup plugin for VirusTotal will take assembyline tags and translate and search for them in VirusTotal. In this case the Assemblyline tags <code>network.dynamic.ip</code> and <code>network.static.ip</code> will be mapped to a VirusTotal search for <code>ip_address</code>.</p> <p>Plugins are run as separate containers and only require incoming ingress connections from the Assemblyline UI API. They then only require external egress to whatever resources are needed to perform their function.</p>"},{"location":"developer_manual/plugins/plugins/#core-plugin-types","title":"Core plugin types","text":""},{"location":"developer_manual/plugins/plugins/#external-lookups","title":"External Lookups","text":"<p>These plugins enable the querying of Assemblyline data in an external system to then bring back and enrich the Assembyline Web UI. Use cases of this style of plugin include:</p> <ul> <li>Looking up IOCs in a private or shared threat intelligence systems (eg. MISP);</li> <li>Checking for existance or similar links in public malware analysis systems (eg. VirusTotal and Malware Bazaar);</li> <li>Querying internal mirrors and databases</li> </ul> <p>All of this can be achived from within the Assemblyline UI, without having to copy/paste the data and context switch.</p> <p>The defined interface that these plugins must implement is located in the assemblyine-ui plugins/external_lookup/template directory.</p>"},{"location":"developer_manual/plugins/plugins/#developing-a-plugin","title":"Developing a plugin","text":"<p>An example template should be defined for each type of plugin in the assemblyine-ui plugins directory.</p> <p>The quickest way to develop your own plugin is to simply take a complete copy of this template directory for the type of plugin you wish to write, and use it as the base of a new repository. In this new repository, modify the <code>app.py</code> to write your functionality. Any Python requirements can be added to the <code>requirements.txt</code> file. Tests should then be written in a <code>test_app.py</code> module and any additional test requirements added to a <code>requirements_tests.txt</code> file.</p> <p>The templates also contain a standard Dockerfile for building your container image which will run your app with <code>gunicorn</code>. This Dockerfile can also be modifed as required, as can the basic <code>gunicorn</code> config file.</p> <p>Note: You are not required to use the exact directory structure as the templates provided. These are simply minimal single module examples. For more complex requirements, you may wish to write a full Python package. The only requirement is that the query interface is matched.</p> <p>Once the plugin is written, the Docker image should be built and deployed to your Assemblyline environment. The plugin can then be added to your Assemblyline configuration. This can be done through Helm by modifying your <code>values.yaml</code> or through Docker by updating your <code>config.yml</code>.</p>"},{"location":"developer_manual/plugins/plugins/#enabling-a-plugin","title":"Enabling a plugin","text":"<p>The plugin container should be deployed using either Docker or Kubernetes depending on your operating environment. Individual plugin configuration can be applied through environment variables, depending on your plugin. If deployed in Kubernetes with network policies, remember to ensure correct policies are in place.</p> <p>Once deployed and running, plugins can be enabled by adding their details to the Assemblyline configuration file under the section: <code>ui.&lt;plugin_type&gt;</code>. The required config for each plugin type is defined in here.</p> <p>Once their configuration has been added, the plugin will be enabled on the next UI reload.</p> <p>For example, External Lookup plugins can be enabled with the following:</p> <pre><code>ui:\n  external_sources:\n    - name: &lt;display name for the source&gt;\n      url: &lt;full url to the plugin microservice api&gt;\n      classification: &lt;optional minimum classification require to access the upstream service&gt;\n      max_classification: &lt;optional maximum classification that can be sumbitted to the upstream service&gt;\n</code></pre> <p>Individual plugin configuration is set in the plugins deployment (eg. through ENV variables in Docker). For example, the required API key for accessing the VirusTotal API can be set in the plugin setting the <code>VT_API_KEY</code> environment variable in the container.</p> <p>If deploying to Kubernetes, plugins can be configured and deployed through the Assemblyline Helm chart. All plugin related fields are stored in the values file under the <code>uiPlugins</code> key. Additional custom developed lookup plugins should be added to the list under: <code>uiPlugins.lookup.plugins</code>.</p>"},{"location":"developer_manual/plugins/plugins/#included-plugins","title":"Included Plugins","text":""},{"location":"developer_manual/plugins/plugins/#assemblyline","title":"Assemblyline","text":"<p>This plugin can be enabled and configured to query other Assemblyline instances. This may be useful if you wish to query data from a partner or have multiple instances yourself. All hashes and tags can be queried by default.</p>"},{"location":"developer_manual/plugins/plugins/#container-configuration","title":"Container configuration","text":"<p>The following environment variables may be configured:</p> <ul> <li>API_KEY: Your API key for the service to query with [Default: \"\"]</li> <li>VERIFY: Use secure HTTPS connections [Default: True]</li> <li>MAX_LIMIT: Maximum amount of results to return [Default: 100]</li> <li>MAX_TIMEOUT: Maximum amount of time to wait for results [Default: 3]</li> <li>CLASSIFICATION: The classification of upstream service [Default: TLP:CLEAR]</li> <li>URL_BASE: The base URL of the upstream service [Default: https://assemblyline-ui]</li> </ul>"},{"location":"developer_manual/plugins/plugins/#virustotal","title":"VirusTotal","text":"<p>The following Assembyline data can be queried by default:</p> <ul> <li>Hashes (MD5, SHA1, SHA256)</li> <li><code>network.dynamic.domain</code> and <code>network.static.domain</code></li> <li><code>network.dynamic.ip</code> and <code>network.static.ip</code></li> <li><code>network.dynamic.uri</code> and <code>network.static.uri</code></li> </ul>"},{"location":"developer_manual/plugins/plugins/#container-configuration_1","title":"Container configuration","text":"<p>The following environment variables may be configured:</p> <ul> <li>VT_API_KEY: Your VirusTotal API key for the service to query with [Default: \"\"]</li> <li>VT_VERIFY: Use secure HTTPS connections [Default: True]</li> <li>MAX_TIMEOUT: Maximum amount of time to wait for results [Default: 3]</li> <li>CLASSIFICATION: The classification of the data being returned from this service [Default: TLP:CLEAR]</li> <li>API_URL: The base URL for the VirusTotal API [Default: https://www.virustotal.com/api/v3]</li> <li>FRONTEND_URL: The base URL of the VirusTotal web UI [Default: https://www.virustotal.com/gui/search]</li> </ul>"},{"location":"developer_manual/plugins/plugins/#malware-bazaar","title":"Malware Bazaar","text":"<p>The following Assembyline data can be queried by default:</p> <ul> <li>Hashes (MD5, SHA1, SHA256)</li> <li><code>file.pe.imports.imphash</code></li> </ul>"},{"location":"developer_manual/plugins/plugins/#container-configuration_2","title":"Container configuration","text":"<p>The following environment variables may be configured:</p> <ul> <li>MB_VERIFY: Use secure HTTPS connections [Default: True]</li> <li>MB_MAX_LIMIT: Maximum amount of results to return [Default: 100]</li> <li>MAX_TIMEOUT: Maximum amount of time to wait for results [Default: 3]</li> <li>CLASSIFICATION: The classification of the data being returned from this service [Default: TLP:CLEAR]</li> <li>API_URL: The base URL for the Malware Bazaar API [Default: https://mb-api.abuse.ch/api/v1]</li> <li>FRONTEND_URL: The base URL of the Malware Bazaar web UI [Default: https://bazaar.abuse.ch/browse.php]</li> </ul>"},{"location":"developer_manual/services/adding_a_service_updater/","title":"Adding a Service Updater","text":""},{"location":"developer_manual/services/adding_a_service_updater/#adding-a-service-updater","title":"Adding a Service Updater","text":"<p>This documentation builds on Developing an Assemblyline service in the event where you have a service that's dependent on signatures (ie. YARA/Suricata/Sigma rules) or on a collection of files (ie. CSV table, custom parsers) for analysis.</p>"},{"location":"developer_manual/services/adding_a_service_updater/#build-your-updater","title":"Build your updater","text":"<p>You will need to subclass the <code>ServiceUpdater</code> and implement/override functions as deemed necessary.</p> <p>Refer to ServiceUpdater class for more details.</p> Datastore PersistedStorage Disk Persisted <p>Let's say the following code is written in <code>update_server.py</code> in the root of your service directory:</p> <pre><code>from assemblyline.odm.models.signature import Signature\nfrom assemblyline_v4_service.updater.updater import ServiceUpdater\n\nclass SampleUpdateServer(ServiceUpdater):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def import_update(self, files_sha256, client, source, default_classification): -&gt; None\n        # Purpose:  Used to import a set of signatures from a source into Assemblyline for signature management\n        # Inputs:\n        #   files_sha256:           A list of tuples containing file paths and their respective sha256\n        #   client:                 An Assemblyline client used to interact with the API on behalf of a service account\n        #   source:                 The name of the source\n        #   default_classification: The default classification given to a signature if none is provided\n\n        signatures = []\n        for file, _ in files_sha256:\n            # Iterate over all the files retrieved by source and upload them as signatures in Assemblyline\n            with open(file, 'r') as fh:\n                signatures.append(Signature(dict(\n                    classification = default_classification,\n                    data = fh.read(),\n                    name = f'sample_signature{len(signatures)}',\n                    source = source_name,\n                    status = 'DEPLOYED',\n                    type = 'sample'\n                )))\n        client.signatures.add_update_many(signatures)\n        self.log.info(f'Successfully imported {len(signatures)} signatures')\n        return\n\n    def is_valid(self, file_path) -&gt; bool:\n        # Purpose:  Used to determine if the file associated is 'valid' to be processed as a signature\n        # Inputs:\n        #   file_path:  Path to a signature file from an external source\n        return super().is_valid(file_path) #Returns true always\n\nif __name__ == '__main__':\n    with SampleUpdateServer(default_pattern=\"*.json\") as server:\n        server.serve_forever()\n</code></pre> <p>Let's say the following code is written in <code>update_server.py</code> in the root of your service directory:</p> <pre><code>from assemblyline.odm.models.signature import Signature\nfrom assemblyline_v4_service.updater.updater import ServiceUpdater\n\nclass SampleUpdateServer(ServiceUpdater):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def import_update(self, files_sha256, client, source, default_classification): -&gt; None\n        # Purpose:  Used to import a set of signatures from a source into a reserved directory\n        # Inputs:\n        #   files_sha256:           A list of tuples containing file paths and their respective sha256\n        #   client:                 An Assemblyline client used to interact with the API on behalf of a service account\n        #   source:                 The name of the source\n        #   default_classification: The default classification given to a signature if none is provided\n\n        # You'll want to write your files to self.latest_updates_dir which should hold all your downloaded files.\n        # The contents in this directory will then be used by prepare_output_directory().\n\n        # Organize files by source\n        dest_dir = os.path.join(self.latest_updates_dir, source)\n        os.makedirs(dest_dir, dirs_exist_ok=True)\n\n        # For every file in this source, move to latest updates directory in a subdirectory labelled by source.\n        for file, _ in files_sha256:\n          dest_file = os.path.join(dest_dir, os.path.basename(file))\n          shutil.move(file, dest_file)\n        self.log.info(f\"Finished moving {len(files_sha256)} files for {source} to {self.latest_updates_dir}\")\n        return\n\n    def is_valid(self, file_path) -&gt; bool:\n        # Purpose:  Used to determine if the file associated is 'valid' to be processed as a signature\n        # Inputs:\n        #   file_path:  Path to a signature file from an external source\n        return super().is_valid(file_path) #Returns true always\n\n    def prepare_output_directory(self) -&gt; str:\n        # Purpose: Prepare your downloaded sources before it's made available to your service.\n        # This could be a function where if you need to compile a bunch of files together or to re-organize\n        # in a certain directory format, you would call this and keep the original dataset intact.\n        # Return:\n        #    output_directory: the path of the directory to serve\n        output_directory = tempfile.mkdtemp()\n        shutil.copytree(self.latest_updates_dir, output_directory, dirs_exist_ok=True)\n        return output_directory\n\nif __name__ == '__main__':\n    with SampleUpdateServer(default_pattern=\"*.json\") as server:\n        server.serve_forever()\n</code></pre>"},{"location":"developer_manual/services/adding_a_service_updater/#add-it-to-the-manifest","title":"Add it to the manifest","text":"<p>In addition to your service manifest, you would append the following:</p> <p>Warning</p> <p>The updater dependency needs to be named <code>updates</code> for the service to recognize it as an updater rather than a normal dependency container.</p> <p>Critical</p> <p>Updaters need to <code>run_as_core</code> which allows them to run at the same level as other core containers.</p> Datastore PersistedStorage Disk Persisted <p>This configuration is intended if your update files are signatures to be imported into Assemblying using the Signatures API.</p> <pre><code># Adding your dependency called 'updates' and specify the command the container should run\ndependencies:\n  updates:\n    container:\n      allow_internet_access: true\n      command: [\"python\", \"-m\", \"update_server\"]\n      image: ${REGISTRY}testing/assemblyline-service-sample:latest\n      ports: [\"5003\"]\n    run_as_core: True\n\n# Update configuration block\nupdate_config:\n  # list of source object from where to fetch files for update and what will be the name of those files on disk\n  sources:\n    - uri: https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json\n      name: sample_1kb_file\n  # interval in seconds at which the updater dependency runs\n  update_interval_seconds: 300\n  # Should the downloaded files be used to create signatures in the system\n  generates_signatures: true\n</code></pre> <p>You'll have to indicate that this updater doesn't generate signatures (<code>generate_signatures: false</code>) for Assemblyline and therefore needs to be persisted to disk rather than using a client to interact with the Signatures API</p> Temporary DiskPersistent Volume <p>This configuration sets up downloaded files to only persist on temporary storage for as long as the container runs.</p> <pre><code># Adding your dependency called 'updates' and specify the command the container should run\ndependencies:\n  updates:\n    container:\n      allow_internet_access: true\n      command: [\"python\", \"-m\", \"update_server\"]\n      image: ${REGISTRY}testing/assemblyline-service-sample:latest\n      ports: [\"5003\"]\n    run_as_core: True\n\n# Update configuration block\nupdate_config:\n  # list of source object from where to fetch files for update and what will be the name of those files on disk\n  sources:\n    - uri: https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json\n      name: sample_1kb_file\n  # interval in seconds at which the updater dependency runs\n  update_interval_seconds: 300\n  # Should the downloaded files be used to create signatures in the system\n  generates_signatures: false\n</code></pre> <p>This configuration sets up downloaded files to only persist on a persistent volume.</p> <p>Note</p> <p>This will involve using the <code>UPDATER_DIR</code> env to tell the updater that your updates will be stored in a specific location.</p> <pre><code># Adding your dependency called 'updates' and specify the command the container should run\ndependencies:\n  updates:\n    container:\n      allow_internet_access: true\n      command: [\"python\", \"-m\", \"update_server\"]\n      image: ${REGISTRY}testing/assemblyline-service-sample:latest\n      ports: [\"5003\"]\n      # Overwrite the UPDATER_DIR variables to point to your persisted mountpoint\n      environment:\n        - name: UPDATER_DIR\n          value: /mnt/persistent_updates/\n    # Allocate a storage volume and mount it to your updater's filesystem\n    volumes:\n      updates:\n        mount_path: /mnt/persistent_updates/\n        capacity: 5242880\n        storage_class: mystorageclass\n    run_as_core: True\n\n# Update configuration block\nupdate_config:\n  # list of source object from where to fetch files for update and what will be the name of those files on disk\n  sources:\n    - uri: https://file-examples-com.github.io/uploads/2017/02/file_example_JSON_1kb.json\n      name: sample_1kb_file\n  # interval in seconds at which the updater dependency runs\n  update_interval_seconds: 300\n  # Should the downloaded files be used to create signatures in the system\n  generates_signatures: false\n</code></pre>"},{"location":"developer_manual/services/developing_an_assemblyline_service/","title":"Developing a service","text":""},{"location":"developer_manual/services/developing_an_assemblyline_service/#developing-an-assemblyline-service","title":"Developing an Assemblyline service","text":"<p>This guide has been created for developers who are looking to develop services for Assemblyline. It is aimed at individuals with general software development knowledge and basic Python skills. In-depth knowledge of the Assemblyline framework is not required to develop a service. You should understand the concepts found here though.</p>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#pre-requisites","title":"Pre-requisites","text":"<p>Before getting started, ensure you have read through the setup environment documentation and created the appropriate development environment to perform service development.</p>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#build-your-first-service","title":"Build your first service","text":"<p>This section will guide you through the bare minimum steps required to create a running, but functionally useless service. Each sub-section below outlines the steps required for each of the different files required to create an Assemblyline service. All files created in the following sub-sections must be placed in a common directory.</p> <p>Important</p> <p>For this documentation, we will assume that your new service directory is located at <code>~/git/services/assemblyline-service-sample</code></p> <p>To build a service, we need a minimum of three files.</p> <ol> <li>A Python script that has a class implementing <code>ServiceBase</code> and the <code>execute</code> function</li> <li>A service manifest</li> <li>A Dockerfile.</li> </ol> <p>The Dockerfile is not needed for development but will be required to deploy to your service to an Assemblyline instance.</p>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#service-python-code","title":"Service Python code","text":"<p>In your service directory, you will first start by creating your service's python file. Let's use <code>result_sample.py</code>.</p> <p>Put the following code in your service's file:</p> ~/git/services/assemblyline-service-sample/result_sample.py <pre><code>from assemblyline_v4_service.common.base import ServiceBase\nfrom assemblyline_v4_service.common.request import ServiceRequest\nfrom assemblyline_v4_service.common.result import Result, ResultSection\n\nclass Sample(ServiceBase):\n    def __init__(self, config=None):\n        super(Sample, self).__init__(config)\n\n    def start(self):\n        # ==================================================================\n        # Startup actions:\n        #   Your service might have to do some warming up on startup to make things faster\n        # ==================================================================\n\n        self.log.info(f\"start() from {self.service_attributes.name} service called\")\n\n    def execute(self, request: ServiceRequest) -&gt; None:\n        # ==================================================================\n        # Execute a request:\n        #   Every time your service receives a new file to scan, the execute function is called.\n        #   This is where you should execute your processing code.\n        #   For this example, we will only generate results ...\n        # ==================================================================\n\n        # 1. Create a result object where all the result sections will be saved to\n        result = Result()\n\n        # 2. Create a section to be displayed for this result\n        text_section = ResultSection('Example of a default section')\n\n        # 2.1. Add lines to your section\n        text_section.add_line(\"This is a line displayed in the body of the section\")\n\n        # 2.2. Your section can generate a score. To do this it needs to fire a heuristic.\n        #     We will fire heuristic #1\n        text_section.set_heuristic(1)\n\n        # 2.3. Your section can add tags, we will add a fake one\n        text_section.add_tag(\"network.static.domain\", \"cyber.gc.ca\")\n\n        # 3. Make sure you add your section to the result\n        result.add_section(text_section)\n\n        # 4. Wrap-up: Save your result object back into the request\n        request.result = result\n</code></pre>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#service-manifest-yaml","title":"Service manifest YAML","text":"<p>Now that you have a better understanding of the python portion of a service. We will create the associated manifest file that holds the different configurations for the service.</p> <p>In your service directory, you will add the YAML configuration file <code>service_manifest.yml</code> with the following content:</p> ~/git/services/assembyline-service-sample/service_manifest.yml <pre><code># Name of the service\nname: Sample\n# Version of the service\nversion: 4.5.0.dev0\n# Description of the service\ndescription: ALv4 Sample service from the documentation\n\n# Regex defining the types of files the service accepts and rejects\naccepts: .*\nrejects: empty\n\n# At which stage the service should run (one of FILTER, EXTRACT, CORE, SECONDARY, POST, REVIEW)\n# NOTE: Stages are executed in the order defined in the list\nstage: CORE\n# Which category the service is part of (one of Antivirus, Dynamic Analysis, External, Extraction, Filtering, Internet Connected, Networking, Static Analysis)\ncategory: Static Analysis\n\n# Does the service require access to the file to perform its task\n# If set to false, the service will only have access to the file metadata (e.g. Hashes, size, type, ...)\nfile_required: true\n# Maximum execution time the service has before it's considered to be timed out\ntimeout: 60\n\n# is the service enabled by default\nenabled: true\n\n# Service heuristic blocks: List of heuristic objects that define the different heuristics used in the service\nheuristics:\n  - description: This is a demo heuristic\n    filetype: \"*\"\n    heur_id: 1\n    name: Demo\n    score: 100\n\n# Docker configuration block which defines:\n#  - the name of the docker container that will be created\n#  - CPU and ram allocation by the container\ndocker_config:\n  image: ${REGISTRY}testing/assemblyline-service-sample:latest\n  cpu_cores: 1.0\n  ram_mb: 1024\n</code></pre> <p>Important</p> <p>The <code>service_manifest.yml</code> has a lot more configurable parameters that you might be required to change depending on the service you are building. You should get familiar with the complete list by reading the service manifest advanced documentation.</p>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#dockerfile","title":"Dockerfile","text":"<p>Finally, the last file needed to complete your assemblyline service is the <code>Dockerfile</code> that is used to create its Docker container.</p> <p>In your service directory, create a file named <code>Dockerfile</code> with the following content:</p> ~/git/services/assemblyline-service-sample/Dockerfile <pre><code>FROM cccs/assemblyline-v4-service-base:stable\n\n# Python path to the service class from your service directory\n#  The following example refers to the class \"Sample\" from the \"result_sample.py\" file\nENV SERVICE_PATH result_sample.Sample\n\n# Install any service dependencies here\n# For example: RUN apt-get update &amp;&amp; apt-get install -y libyaml-dev\n#\n#              COPY requirements.txt requirements.txt\n#              RUN pip install --no-cache-dir --user --requirement requirements.txt &amp;&amp; rm -rf ~/.cache/pip\n\n# Switch to assemblyline user\nUSER assemblyline\n\n# Copy Sample service code\nWORKDIR /opt/al_service\nCOPY . .\n</code></pre> <p>The Dockerfile is required to build a Docker container. When developing a Docker container for an Assemblyline service, the following must be ensured:</p> <ul> <li>The parent image must be <code>cccs/assemblyline-v4-service-base:stable</code> so you are using a stable build of service base.</li> <li>An environment variable named <code>SERVICE_PATH</code> must be set whose value defines the Python module path to the main service class which inherits from the <code>ServiceBase</code> class.</li> <li>Any dependency installation must be completed as the <code>root</code> user, which is set by default in the parent image. Once all dependency installations have been completed, you must change the user to <code>assemblyline</code>.</li> <li>The service code and any dependency files must be copied to the <code>/opt/al_service</code> directory.</li> </ul>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#conclusion","title":"Conclusion","text":"<p>After you've completed creating your first service, your <code>~/git/services/assemblyline-service-sample</code> directory should have the following files at a minimum:</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 result_sample.py\n\u2514\u2500\u2500 service_manifest.yml\n</code></pre>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#good-service-examples","title":"Good service examples","text":""},{"location":"developer_manual/services/developing_an_assemblyline_service/#elfparser","title":"ElfParser","text":"<ul> <li>Package a compiled executable from https://github.com/jacob-baines/elfparser</li> <li>Parse the output of the executable to fill <code>ResultSections</code> for the user</li> </ul>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#apivector","title":"ApiVector","text":"<ul> <li>Use a public library (<code>apiscout</code>, <code>lief</code>)</li> <li>Load an external file</li> <li>Use an updater</li> </ul>"},{"location":"developer_manual/services/developing_an_assemblyline_service/#urldownloader","title":"UrlDownloader","text":"<p>Unique key-value usage in the service manifest relative to the average service:</p> <ul> <li><code>stage: POST</code></li> <li><code>file_required: false</code></li> <li><code>is_external, allow_internet_access: true</code></li> <li><code>uses_tag_scores, uses_metadata, uses_temp_submission_data: true</code></li> </ul>"},{"location":"developer_manual/services/run_your_service/","title":"Run your service","text":""},{"location":"developer_manual/services/run_your_service/#run-your-service","title":"Run your service","text":"<p>This section of the service documentation will show you how to run your service in 3 different ways:</p> <ol> <li>Standalone mode directly on a file</li> <li>Live code on an Assemblyline system</li> <li>Production container on an Assemblyline system</li> </ol> <p>Important</p> <p>This documentation assumes the following:</p> <ol> <li>You have read through the setup environment documentation and created the appropriate development environment to perform service development.</li> <li>You have completed the developing an Assemblyline service documentation and your service is in the <code>~/git/services/assemblyline-service-sample</code> directory on the machine where your IDE runs.</li> <li>Whether you are using VSCode or PyCharm as your IDE, you have a virtual environment dedicated to running services either located at <code>~/git/services/venv</code> or <code>~/venv/services</code> on the VM where the code runs</li> </ol>"},{"location":"developer_manual/services/run_your_service/#standalone-mode","title":"Standalone mode","text":"<p>To test an Assemblyline service in standalone mode, the <code>run_service_once</code> module from the assemblyline-v4-service package can be used to run a single task through the service for testing.</p>"},{"location":"developer_manual/services/run_your_service/#running-the-sample-service","title":"Running the Sample service","text":"<p>Load the virtual environment</p> <pre><code>source ~/git/services/venv/bin/activate\n</code></pre> <p>Ensure the current working directory is the root of the service directory of the service to be run.</p> <pre><code>cd ~/git/services/assemblyline-service-sample\n</code></pre> <p>From a terminal, run the <code>run_service_once</code> module, specifying the service path for the service to run and the path to the file to scan. For this example, we will have the service scan itself.</p> <pre><code>python -m assemblyline_v4_service.dev.run_service_once result_sample.Sample result_sample.py\n</code></pre> <p>The <code>run_service_once</code> module creates a directory at the same spot where the file is found with the service name that scanned the file appended to it. In the previous example, the output of the service should be located at <code>~/git/services/assemblyline-service-sample/sample.py_sample</code>. The directory will contain a <code>result.json</code> file containing the result from the service.</p> <p>You can view the <code>result.json</code> file using the following command:</p> <pre><code>cat ~/git/services/assemblyline-service-sample/result_sample.py_sample/result.json | json_pp\n</code></pre> <p>It will look something like this:</p> <p>~/git/services/assemblyline-service-sample/sample.py_sample/result.json pretty printed</p> <pre><code>{\n    \"classification\" : \"TLP:W\",\n    \"drop_file\" : false,\n    \"response\" : {\n        \"extracted\" : [],\n        \"milestones\" : {\n            \"service_completed\" : \"2021-09-15T15:52:47.024909Z\",\n            \"service_started\" : \"2021-09-15T15:52:47.024761Z\"\n        },\n        \"service_context\" : null,\n        \"service_debug_info\" : null,\n        \"service_name\" : \"sample\",\n        \"service_tool_version\" : null,\n        \"service_version\" : \"1\",\n        \"supplementary\" : []\n    },\n    \"result\" : {\n        \"score\" : 100,\n        \"sections\" : [\n            {\n                \"auto_collapse\" : false,\n                \"body\" : \"This is a line displayed in the body of the section\",\n                \"body_format\" : \"TEXT\",\n                \"classification\" : \"TLP:W\",\n                \"depth\" : 0,\n                \"heuristic\" : {\n                \"attack_ids\" : [],\n                \"frequency\" : 1,\n                \"heur_id\" : 1,\n                \"score\" : 100,\n                \"score_map\" : {},\n                \"signatures\" : {}\n                },\n                \"tags\" : {\n                \"network\" : {\n                    \"static\" : {\n                        \"domain\" : [\n                            \"cyber.gc.ca\"\n                        ]\n                    }\n                }\n                },\n                \"title_text\" : \"Example of a default section\",\n                \"zeroize_on_tag_safe\" : false\n            }\n        ]\n    },\n    \"sha256\" : \"dab595d88c22eba68e831e072471b02206d12cb29173c708c606416ecf50b942\",\n    \"temp_submission_data\" : {}\n}\n</code></pre> <p>Using the standalone mode via <code>run_service_once</code> is going to help quickly catch any bug, but won't allow you to see the UI elements.</p>"},{"location":"developer_manual/services/run_your_service/#live-mode","title":"Live mode","text":"<p>The following technique is how to hook in a service to an Assemblyline development instance so you can perform live debugging and you can send files to your service using the Assemblyline UI.</p> <p>The way to run a service in debug mode will differ depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup:</p> <ul> <li>Run a service LIVE in VSCode</li> <li>Run a service LIVE in PyCharm</li> </ul> <p>Important</p> <p>You will need to adjust the documentation according to:</p> <ol> <li>The correct name for your service (<code>Sample</code>)</li> <li>The correct service python module for your service (<code>sample.Sample</code>)</li> <li>The correct working directory for your service (<code>~/git/services/assemblyline-service-sample</code>)</li> </ol>"},{"location":"developer_manual/services/run_your_service/#run-from-a-shell","title":"Run from a shell","text":"<p>If you don't plan on doing any debugging and you just want to run the service live in your development environment, you can just spin up two shells and run <code>Task Handler</code> in one and your <code>Sample service</code> in the other.</p>"},{"location":"developer_manual/services/run_your_service/#task-handler","title":"Task Handler","text":"<pre><code># Load your service virtual environment\nsource ~/git/services/venv/bin/activate\n\n# Run task handler\npython -m assemblyline_service_client.task_handler\n</code></pre>"},{"location":"developer_manual/services/run_your_service/#sample-service","title":"Sample service","text":"<pre><code># Load your service virtual environment\nsource ~/git/services/venv/bin/activate\n\n# Go to your service directory\ncd ~/git/services/assemblyline-service-sample\n\n# Run your service\nSERVICE_PATH=result_sample.Sample python -m assemblyline_v4_service.run_service\n</code></pre>"},{"location":"developer_manual/services/run_your_service/#production-container-mode","title":"Production container mode","text":"<p>When you are confident your service is stable enough, it is time to test it in its final form: A Docker container.</p>"},{"location":"developer_manual/services/run_your_service/#build-the-container","title":"Build the container","text":"<p>Change working directory to root of the service:</p> <pre><code>cd ~/git/services/assemblyline-service-sample\n</code></pre> <p>Run the <code>docker build</code> command and tag the container with the same name that the container name has in your service manifest</p> <pre><code>docker build -t testing/assemblyline-service-sample .\n</code></pre>"},{"location":"developer_manual/services/run_your_service/#run-the-container-live","title":"Run the container LIVE","text":"<p>The way to run a container LIVE in your development environment differs depending on if you've been using VSCode or PyCharm as your IDE. Follow the appropriate documentation for your current setup:</p> <ul> <li>Run a single container in VSCode</li> <li>Run a single container in PyCharm</li> </ul> <p>Important</p> <p>You will need to adjust the documentation according to:</p> <ol> <li>The correct name for your service (<code>Sample</code>)</li> <li>The correct container name (<code>testing/assemblyline-service-sample</code>)</li> </ol>"},{"location":"developer_manual/services/run_your_service/#run-container-from-a-shell","title":"Run container from a shell","text":"<p>If you don't want to use the IDE to test your production container, you can always run it straight from a shell.</p> <p>Use the following command to run it:</p> <pre><code>docker run --env SERVICE_API_HOST=http://`ip addr show docker0 | grep \"inet \" | awk '{print $2}' | cut -f1 -d\"/\"`:5003 --network=host --name SampleService testing/assemblyline-service-sample\n</code></pre>"},{"location":"developer_manual/services/run_your_service/#add-the-container-to-your-deployment","title":"Add the container to your deployment","text":"<p>Note</p> <p>For the scaler and updater to be able to use your service container, they must be able to get it from a docker registry. You can either use DockerHub, a work central registry, or a local registry.</p>"},{"location":"developer_manual/services/run_your_service/#push-the-container-to-your-local-registry","title":"Push the container to your local registry","text":"<p>Tip</p> <p>A local docker registry should have already been installed during the development environment setup. If not, you can start one by simply running this command:</p> <pre><code>sudo docker run -dp 32000:5000 --restart=always --name registry registry\n</code></pre> <p>Use the following to push your sample service to the local registry:</p> <pre><code>docker tag testing/assemblyline-service-sample localhost:32000/testing/assemblyline-service-sample\ndocker push --all-tags localhost:32000/testing/assemblyline-service-sample\n</code></pre>"},{"location":"developer_manual/services/run_your_service/#add-the-service-in-the-management-interface","title":"Add the service in the management interface","text":"<ol> <li>Using your web browser, go to the service management page: https://localhost/admin/services (Replace localhost by your VM's IP)</li> <li>Click the <code>Add service</code> button</li> <li>Paste the entire content of the <code>service_manifest.yml</code> file from your service directory in the text box<ul> <li>If you are using the local registry from this documentation, change the <code>${REGISTRY}</code> from the content of <code>service_manifest.yml</code> to <code>host.docker.internal:32000/</code></li> </ul> </li> <li>Click the <code>Add</code> button</li> </ol> <p>Your service information has been added to the system. The scaler component should automatically start a container of your newly created service.</p>"},{"location":"developer_manual/services/advanced/exceptions/","title":"Exceptions","text":""},{"location":"developer_manual/services/advanced/exceptions/#servicebase-class","title":"ServiceBase class","text":"<p>There are few exceptions that a service can raise that have unique handling in Assemblyline.</p> Always raise an exception if something goes wrong in your service that submits files to external services <p>If you write a service that submits a file to an external service and it fails silently if the external service fails, this result will be cached and then will be returned the next time that the same file is seen in the system.</p>"},{"location":"developer_manual/services/advanced/exceptions/#nonrecoverableerror","title":"NonRecoverableError","text":"<p>This exception is raised to indicate that the analysis of the file is never going to succeed, so it is not worth resubmitting the file to try again.</p> <p>Situations when you should use this exception in your service:</p> <ul> <li>A file is received by a service that handles submitting that file to an external server like CAPE, Intezer, VirusTotal, etc. The external server fails to analyze the file in a way that provides high confidence that the external server will fail consistently for this file, or any file during a given time frame such as when the REST API is down.</li> </ul>"},{"location":"developer_manual/services/advanced/exceptions/#recoverableerror","title":"RecoverableError","text":"<p>This exception is raised to indicate that the analysis of the file is could succeed, so it is worth resubmitting the file to try again.</p> <p>Situations when you should use this exception in your service:</p> <ul> <li>A file is received by a service that handles submitting that file to an external server like CAPE, Intezer, VirusTotal, Suricata, etc. The external server fails to analyze the file but you have high confidence that the external server won't fail again.</li> </ul>"},{"location":"developer_manual/services/advanced/exceptions/#when-to-fail-silently","title":"When to fail silently","text":"<p>Sometimes the above exceptions are not what you are looking for.</p> <p>Situations when you should catch an error and return no result:</p> <ul> <li>A file is received by a service which uses a library for analysis. The library crashes when it attempts to analyze this file, and it will always crash for this file.</li> </ul>"},{"location":"developer_manual/services/advanced/partial_results/","title":"Partial Results","text":""},{"location":"developer_manual/services/advanced/partial_results/#partial-results","title":"Partial Results","text":"<p>Producing a partial result is an optional feature that services can use to delay processing while waiting for other services to provide additional information.</p> <p>This additional information can come from other files within the same submission, even after the file being processed has already been otherwise completed. If no additional information is provided that allows the service to create a complete (non-partial) result then the last partial result created will be kept in the submission results.</p> <p>Partial results don't participate in caching. If a service is only able to produce partial results then it will always be retried when a new submission processes that file.</p>"},{"location":"developer_manual/services/advanced/partial_results/#service-development","title":"Service Development","text":"<p>When a service produces a result it may optionally flag it as 'partial'. This signals to Assemblyline that the service may be able to do further processing. A hypothetical service that expects a password to be provided might have sections that looks like this:</p> <pre><code>class IncompleteService(ServiceBase):\n\n    def execute(self, request: ServiceRequest):\n        # prepare a result\n        result = Result()\n        request.result = result\n\n        try:\n            passwords = self.get_passwords(request)\n            self.process_password_protected_data(request, passwords)\n        except PasswordNotFoundError:\n            request.partial()\n            section = ResultTextSection(\n                \"Failed to extract password protected file.\",\n                heuristic=Heuristic(1),\n                parent=request.result\n            )\n            section.add_tag(\"file.behavior\", \"Archive Unknown Password\")\n            return\n\n\n    def get_passwords(self, request: ServiceRequest):\n        # Start with default passwords from the service config\n        passwords = list(self.config.get(\"default_password_list\", []))\n\n        # Get passwords users may have added to this submission\n        user_supplied = request.get_param(\"password\")\n        if user_supplied:\n            passwords.append(user_supplied)\n\n        # Get passwords from temp data, other services might be providing some\n        if \"passwords\" in request.temp_submission_data:\n            passwords.extend(request.temp_submission_data[\"passwords\"])\n\n        return passwords\n</code></pre> <p>Further processing will be triggered by services producing changes in monitored keys in the temporary submission data. What keys can be monitored is configured per system, which keys a service monitors is configured per service, both must be set for this feature to function. Our example service that wants to wait for other services to update the list of possible passwords related to the submission would add this to its manifest:</p> <pre><code>uses_temp_submission_data: true\nmonitored_keys:\n  - passwords\n</code></pre>"},{"location":"developer_manual/services/advanced/partial_results/#system-configuration","title":"System Configuration","text":"<p>The set of keys that can be used by services for monitoring must be configured at the system level. New fields can be added by setting them under the <code>submission.temporary_keys</code> configuration field.</p> <p>What aggregation will be used to combine temporary data from different services must be set for each key. Available options are:</p> <ul> <li><code>union</code> - Keep this key as a submission wide list merging equal items</li> <li><code>overwrite</code> - Keep this key submission wide on a \"last write wins\" basis</li> </ul> <p>To add the temporary submission data keys 'sample_key_a' and 'sample_key_b' with the union and overwrite aggregations respectively the following could be used:</p> <pre><code>submission:\n    temporary_keys:\n        sample_key_a: union\n        sample_key_b: overwrite\n</code></pre>"},{"location":"developer_manual/services/advanced/partial_results/#defaults","title":"Defaults","text":"<p>By default all systems are configured with some keys active that will contain at minimum:</p> <ul> <li><code>passwords</code>: union, a list of possible passwords related to this submission extracted from different parts of the submitted file.</li> <li><code>email_body</code>: union, a list of words found in email bodies embedded into this submission.</li> </ul> <p>Adding more values via <code>submission.temporary_keys</code> won't overwrite these defaults or prevent future defaults from being added.</p> <p>If you wish to remove these defaults, or block future changes to them, you can overwrite the <code>submission.default_temporary_keys</code> field.</p> <pre><code>submission:\n    default_temporary_keys: {}\n</code></pre>"},{"location":"developer_manual/services/advanced/request/","title":"Request Class","text":""},{"location":"developer_manual/services/advanced/request/#request-class","title":"Request class","text":"<p>The <code>Request</code> object is the parameter received by the service execute function. It holds information about the task to be processed by the service.</p> <p>You can view the source for the class here: Request class source</p>"},{"location":"developer_manual/services/advanced/request/#class-variables","title":"Class variables","text":"<p>The following table describes all of the <code>Request</code> object variables which the service can use.</p> Variable name Description deep_scan Returns whether the file should be deep-scanned or not. Deep-scanning usually takes more time and is better suited for files that are sent manually. file_contents Returns the raw byte contents of the file to be scanned. file_name Returns the name of the file (as submitted by the user) to be scanned. file_path Returns the path to the file to be scanned. The service can use this path directly to access the file. file_type Returns the Assemblyline-style file type of the file to be scanned. max_extracted Returns the maximum number of files that are allowed to be extracted by a service. By default this is set to 500. md5 Returns the MD5 hash of the file to be scanned. result Used to get and set the current result. sha1 Returns the SHA1 hash of the file to be scanned. sha256 Returns the SHA256 hash of the file to be scanned. sid ID of the submission being scanned. task The original task object used to create this request. You can find more information there about the request (metadata submitted, files already extracted by other services, tags already generated by other services and more...) temp_submission_data Can be used to get and set temporary submission data which is passed onto subsequent tasks resulting from adding extracted files."},{"location":"developer_manual/services/advanced/request/#class-functions","title":"Class functions","text":"<p>The following table describes the <code>Request</code> object functions which the service can use.</p>"},{"location":"developer_manual/services/advanced/request/#add_extracted","title":"add_extracted()","text":"<p>This function adds a file extracted by the service to the result. The extracted file will also be scanned through a set of services, as if it had been originally submitted. For example with a ZIP file, Extract is going to send back as an extracted file anything that came out of the ZIP file.</p> <p>This function can take the following parameters:</p> <ul> <li><code>path</code>: Complete path to the file</li> <li><code>name</code>: Display name of the file</li> <li><code>description</code>: Descriptive text about the file</li> <li><code>classification</code>: Optional classification of the file</li> </ul> Example <p>Excerpt from Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Re-submitting files to the system\n#     Adding extracted files will have them resubmitted to the system for analysis\n...\nfd, temp_path = tempfile.mkstemp(dir=self.working_directory)\nwith os.fdopen(fd, \"wb\") as myfile:\n    myfile.write(b\"CLASSIFIED!!!__\"+data.encode())\nrequest.add_extracted(temp_path, \"classified.doc\", \"Classified file ... don't look\",\n                      classification=cl_engine.RESTRICTED)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/request/#add_supplementary","title":"add_supplementary()","text":"<p>This function adds a supplementary file generated by the service to the result. The supplementary file is uploaded for the user's informational use only and is not scanned. For example, Extract may add a supplementary file for the list of passwords it tried on a password-protected file if it failed to extract it. Supplementary files make more sense if there is a bigger and more complex file, like a JSON file.</p> <p>This function can take the following parameters:</p> <ul> <li><code>path</code>: Complete path to the file</li> <li><code>name</code>: Display name of the file</li> <li><code>description</code>: Descriptive text about the file</li> <li><code>classification</code>: Optional classification of the file</li> </ul> Example <p>Excerpt from Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Supplementary files\n#     Adding supplementary files will save them on the datastore for future\n#      reference but won't reprocess those files.\nfd, temp_path = tempfile.mkstemp(dir=self.working_directory)\nwith os.fdopen(fd, \"w\") as myfile:\n    myfile.write(json.dumps(urls))\nrequest.add_supplementary(temp_path, \"urls.json\", \"These are urls as a JSON file\")\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/request/#drop","title":"drop()","text":"<p>When called, the task will be dropped and will not be processed further by other remaining service(s).</p> Example <p>Excerpt from Assemblyline Safelist service: safelist.py</p> <pre><code>...\n# Stop processing, the file is safe\nrequest.drop()\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/request/#get_param","title":"get_param()","text":"<p>Retrieve a submission parameter for the task.</p> <p>This function can take the following parameter:</p> <p><code>name</code>: name of the submission parameter to retrieve</p> Example <p>Excerpt from Assemblyline Extract service: extract.py</p> <pre><code>...\ndef execute(self, request: ServiceRequest):\n    ...\n    continue_after_extract = request.get_param('continue_after_extract')\n    ...\n</code></pre> <p>These submission parameters are also defined with default values in the service manifest. A system administrator can change the default values at <code>/admin/services</code> and any user can overwrite these values when they create a submission.</p> <p></p> <p>Users can also set their preferred default values for submission parameters in their profile at <code>/settings</code>. For example, in the Extract service, I always want to try the password \"mycustompassword\" whenever I upload a password-protected ZIP file because that is my personal favourite password to use when password-protecting ZIP files.</p> <p></p>"},{"location":"developer_manual/services/advanced/request/#set_service_context","title":"set_service_context()","text":"<p>Set the context of the service which ran the file. For example, if the service ran an AntiVirus engine on the file, then the AntiVirus definition version would be the service context.</p> <p>This function can take the following parameters:</p> <p><code>context</code>: Service context as string</p> Example <p>Excerpt from Assemblyline Antivirus service: antivirus.py</p> <pre><code>...\ndef execute(self, request: ServiceRequest):\n    ...\n    request.set_service_context(\n        f\"Engine Update Range: {epoch_to_local(lower_range)} - {epoch_to_local(upper_range)}\"\n    )\n    ...\n</code></pre>"},{"location":"developer_manual/services/advanced/result/","title":"Result Class","text":""},{"location":"developer_manual/services/advanced/result/#result-class","title":"Result class","text":"<p>All services in Assemblyline must create a <code>Result</code> object containing the different ResultSections which encapsulate their findings. This <code>Result</code> object must then be set as the Request's <code>result</code> parameter which will then be saved in the database to show to the user.</p> <p>You can view the source for the class here: Result class source</p>"},{"location":"developer_manual/services/advanced/result/#class-variables","title":"Class variables","text":"<p>Even though the <code>Result</code> class is critical for the service, it does not have many variables that should be used by the service.</p> Variable Name Description sections List of all the <code>ResultSection</code> objects that have been added to the <code>Result</code> object so far."},{"location":"developer_manual/services/advanced/result/#class-functions","title":"Class functions","text":"<p>There is only one function that the service writer should ever use in a <code>Result</code> object.</p>"},{"location":"developer_manual/services/advanced/result/#add_section","title":"add_section()","text":"<p>This function allows the service to add a ResultSection object to the current <code>Result</code> object.</p> <p>It can take the following parameters:</p> <ul> <li><code>section</code>: The ResultSection object to add to the list</li> <li><code>on_top</code>: (Optional) Boolean value that indicates if the section should be on top of the other sections or not</li> </ul> Example <p>Excerpt from Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\nkv_body = {\n    \"a_str\": \"Some string\",\n    \"a_bool\": False,\n    \"an_int\": 102,\n}\nkv_section = ResultSection('Example of a KEY_VALUE section', body_format=BODY_FORMAT.KEY_VALUE,\n                            body=json.dumps(kv_body))\nresult.add_section(kv_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result/#tips-on-writing-good-results","title":"Tips on writing good results","text":"<p>Here's a few tips on writing services:</p> <p>DO's</p> <ol> <li>Make sure your results are easily readable for the user</li> <li>Include only necessary information</li> <li>Collapse sections that are less important by default</li> <li>If your service has information that can be tagged, don't forget to add those tags</li> <li>Choose a ResultSection type that will represent your data well</li> </ol> <p>DON'T's</p> <ol> <li>If your service has nothing to say, do not create ResultSection objects that contain that the service found nothing. There are a lot of shortcuts that the system takes with empty results and this will circumvent them.</li> <li>Don't overwhelm the users with information or they will stop reading it and just skim through</li> <li>Don't resubmit too many embedded files when you are not sure if those files are worth analyzing, otherwise the results will be hard to read and will take a very long time to execute.</li> </ol>"},{"location":"developer_manual/services/advanced/result_section/","title":"ResultSection Class","text":""},{"location":"developer_manual/services/advanced/result_section/#resultsection-class","title":"ResultSection class","text":"<p>A <code>ResultSection</code> is bascally part of a service result that encapsulates a certain type of information that your service needs to convey to the user. For example, if you have a service that extracts networking indicators as well as process lists, you should put network indicators in their own section and then the process list in another.</p> <p>Result sections have the following properties:</p> <ul> <li>They have different types that will display information in different manners</li> <li> <p>They can attach a single heuristic which will add a maliciousness score to the result section</p> <ul> <li>Each signature can have an associated score, that will raise the score of the heuristic by overriding the default heuristic score.</li> </ul> </li> <li> <p>They can tag important pieces of information about a file, which can be used to easily search in the system</p> </li> <li>They can contain subsections which are just sections inside of another section</li> <li> <p>They can have a classification which allows the API to redact partial results from a service depending on the user</p> <ul> <li>Different classifications would mostly be used if you have external data sources, like signatures. Some may have a higher classification than others</li> </ul> </li> </ul> <p>The total score for the heuristic is going to determine the colour and categorization of everything in the <code>ResultSection</code>. The <code>ResultSection</code> is going to show up yellow or red if the score is high enough, as are the tags for that <code>ResultSection</code>.</p> <p>For example, if you extract a domain that you know is malicious, you can tag it in the <code>ResultSection</code> that has a heuristic with a high score, so that the tag will easily be seen by the users.</p> <p>You can view the source for the class here: ResultSection class source</p>"},{"location":"developer_manual/services/advanced/result_section/#class-variables","title":"Class variables","text":"<p>The <code>ResultSection</code> class includes many instance variables which can be used to shape the way the section will be shown to the user.</p> <p>The following table describes all of the variables of the <code>ResultSection</code> class.</p> Variable Name Description parent Parent <code>ResultSection</code> object (Only if the section is a child of another) subsections List of children <code>ResultSection</code> objects body Body of the section. Can take multiple forms depending on the section type: List of strings, string, JSON blob... classification The classification level of the current section body_format The types of body of the current section (Default: TEXT) tags Dictionary containing the different tags that have been added to the section heuristic Current heuristic assigned to the section zeroize_on_tag_safe Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted? (Default: False) auto_collapse Should the section be displayed in collapsed mode when first rendered in the UI? (Default: False) zeroize_on_sig_safe Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted? (Default: True)"},{"location":"developer_manual/services/advanced/result_section/#class-functions","title":"Class functions","text":""},{"location":"developer_manual/services/advanced/result_section/#__init__","title":"__init__()","text":"<p>The constructor of the <code>ResultSection</code> object allows you to set all variables from the start.</p> <p>Parameters:</p> <ul> <li><code>title_text</code>: (Required) Title of the section</li> <li><code>body</code>: Body of the section</li> <li><code>classification</code>: Classification of the section</li> <li><code>body_format</code>: Type of body</li> <li><code>heuristic</code>: Heuristic assigned to the section</li> <li><code>tags</code>: Dictionary of tags assigned to the section</li> <li><code>parent</code>: Parent of the section (either another section or the Result object)</li> <li><code>zeroize_on_tag_safe</code>: Should the section be forced to a score of 0 if all tags found in it are marked as Safelisted?</li> <li><code>auto_collapse</code>: Should the section be displayed in collapsed mode when first rendered in the UI?</li> <li><code>zeroize_on_sig_safe</code>: Should the section be forced to a score of 0 if all heuristic signatures found in it are marked as Safelisted?</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# The classification of a section can be set to any valid classification for your system\nsection_color_map = ResultSection(\"Example of colormap result section\", body_format=BODY_FORMAT.GRAPH_DATA,\n                                    body=json.dumps(color_map_data), classification=cl_engine.RESTRICTED)\nresult.add_section(section_color_map)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#add_line","title":"add_line()","text":"<p>This function allows the service to add a line to the body of a ResultSection object.</p> <p>Parameters:</p> <ul> <li><code>text</code>: A string containing the line to add to the body</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\ntext_section = ResultSection('Example of a default section')\n# You can add lines to your section one at a time\n#   Here we will generate a random line\ntext_section.add_line(get_random_phrase())\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#add_lines","title":"add_lines()","text":"<p>This function allows the service to add multiple lines to the body of a ResultSection object.</p> <p>Parameters:</p> <ul> <li><code>lines</code>: List of string to add as multiple lines in the body</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\ntext_section = ResultSection('Example of a default section')\n...\n# Or your can add them from a list\n#   Here we will generate random amount of random lines\ntext_section.add_lines([get_random_phrase() for _ in range(random.randint(1, 5))])\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#add_subsection","title":"add_subsection()","text":"<p>This function allows the service to add a subsection to the current <code>ResultSection</code> object.</p> <p>Parameters:</p> <ul> <li><code>subsection</code>: The <code>ResultSection</code> object to add as a subsection</li> <li><code>on_top</code>: (Optional) Boolean value that indicates if the section should be on top of the other sections or not</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\nurl_sub_section = ResultSection('Example of a url sub-section with multiple links',\n                                body=json.dumps(urls), body_format=BODY_FORMAT.URL,\n                                heuristic=url_heuristic, classification=cl_engine.RESTRICTED)\n...\nurl_sub_sub_section = ResultSection('example of a two level deep sub-section',\n                                    body=json.dumps(ips), body_format=BODY_FORMAT.URL)\n...\n# Since url_sub_sub_section is a sub-section of url_sub_section\n# we will add it as a sub-section of url_sub_section not to the main result itself\nurl_sub_section.add_subsection(url_sub_sub_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#add_tag","title":"add_tag()","text":"<p>This function allows the service writer to add a tag to the <code>ResultSection</code></p> <p>Parameters:</p> <ul> <li><code>type</code>: Type of tag</li> <li><code>value</code>: Value of the tag</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# You can tag data to a section. Tagging is used to quickly find defining information about a file\ntext_section.add_tag(\"attribution.implant\", \"ResultSample\")\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#set_body","title":"set_body()","text":"<p>Set the body and the body format of a section</p> <p>Parameters:</p> <ul> <li><code>body</code>: New body value</li> <li><code>body_format</code>: (Optional) Type of body - Default: TEXT</li> </ul>"},{"location":"developer_manual/services/advanced/result_section/#set_heuristic","title":"set_heuristic()","text":"<p>Set a heuristic for a current section/subsection. A heuristic is required to assign a score to a result section/subsection.</p> <p>Parameters:</p> <ul> <li><code>heur_id</code>: Heuristic ID as set in the service manifest</li> <li><code>attack_id</code>: (optional) Attack ID related to the heuristic</li> <li><code>signature</code>: (optional) Signature name that triggered the heuristic</li> </ul> Example <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# If the section needs to affect the score of the file you need to set a heuristic\n#   Here we will pick one at random\n#     In addition to adding a heuristic, we will associate a signature to the heuristic.\n#     We're doing this by adding the signature name to the heuristic. (Here we use a random name)\ntext_section.set_heuristic(3, signature=\"sig_one\")\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#section-types","title":"Section types","text":"<p>These are all result section types that Assemblyline supports. You can see a screenshot of each section as well as the code that was used to generate the actual section.</p>"},{"location":"developer_manual/services/advanced/result_section/#text","title":"TEXT","text":"Code used to generate the TEXT section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Standard text section: BODY_FORMAT.TEXT - DEFAULT\n#   Text sections basically just dump the text to the screen...\n#     The scores of all sections will be SUMed in the service result\n#     The Result classification will be the highest classification found in the sections\ntext_section = ResultSection('Example of a default section')\n# You can add lines to your section one at a time\n#   Here we will generate a random line\ntext_section.add_line(get_random_phrase())\n# Or your can add them from a list\n#   Here we will generate a random amount of random lines\ntext_section.add_lines([get_random_phrase() for _ in range(random.randint(1, 5))])\n# You can tag data to a section. Tagging is used to quickly find defining information about a file\ntext_section.add_tag(\"attribution.implant\", \"ResultSample\")\n# If the section needs to affect the score of the file you need to set a heuristics\n#   Here we will pick one at random\n#     In addition to adding a heuristic, we will associate a signature to the heuristic.\n#     We're doing this by adding the signature name to the heuristic. (Here we use a random name)\ntext_section.set_heuristic(3, signature=\"sig_one\")\n# You can attach ATT&amp;CK IDs to heuristics after they where defined\ntext_section.heuristic.add_attack_id(random.choice(list(software_map.keys())))\ntext_section.heuristic.add_attack_id(random.choice(list(attack_map.keys())))\ntext_section.heuristic.add_attack_id(random.choice(list(group_map.keys())))\ntext_section.heuristic.add_attack_id(random.choice(list(revoke_map.keys())))\n# Same thing for the signatures, they can be added to a heuristic after the fact and you can even say how\n#   many time the signature fired by setting its frequency. If you call add_signature_id twice with the\n#   same signature, this will also increase the frequency of the signature.\ntext_section.heuristic.add_signature_id(\"sig_two\", score=20, frequency=2)\ntext_section.heuristic.add_signature_id(\"sig_two\", score=20, frequency=3)\ntext_section.heuristic.add_signature_id(\"sig_three\")\ntext_section.heuristic.add_signature_id(\"sig_three\")\ntext_section.heuristic.add_signature_id(\"sig_four\", score=0)\n# The heuristic for text_section should have the following properties:\n#   1. 1 ATT&amp;CK ID: T1066\n#   2. 4 signatures: sig_one, sig_two, sig_three and sig_four\n#   3. Signature frequencies are cumulative, therefore they will be as follows:\n#      - sig_one = 1\n#      - sig_two = 5\n#      - sig_three = 2\n#      - sig_four = 1\n#   4. The score used by each heuristic is driven by the following rules: signature_score_map is the highest\n#      priority, then score value for the add_signature_id is in second place and finally the default\n#      heuristic score is used. The score used to calculate the total score for the text_section is\n#      as follows:\n#      - sig_one: 10    -&gt; heuristic default score\n#      - sig_two: 20    -&gt; score provided by the function add_signature_id\n#      - sig_three: 30  -&gt; score provided by the heuristic map\n#      - sig_four: 40   -&gt; score provided by the heuristic map because it's higher priority than the\n#                          function score\n#    5. Total section score is then: 1x10 + 5x20 + 2x30 + 1x40 = 210\n# Make sure you add your section to the result\nresult.add_section(text_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#memory_dump","title":"MEMORY_DUMP","text":"Code used to generate the MEMORY_DUMP section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\nfrom assemblyline.common.hexdump import hexdump\n...\n# ==================================================================\n# Memory dump section: BODY_FORMAT.MEMORY_DUMP\n#     Dump whatever string content you have into a &lt;pre/&gt; html tag so you can do your own formatting\ndata = hexdump(b\"This is some random text that we will format as an hexdump and you'll see \"\n               b\"that the hexdump formatting will be preserved by the memory dump section!\")\nmemdump_section = ResultSection('Example of a memory dump section', body_format=BODY_FORMAT.MEMORY_DUMP,\n                                body=data)\nmemdump_section.set_heuristic(random.randint(1, 4))\nresult.add_section(memdump_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#graph_data","title":"GRAPH_DATA","text":"Code used to generate the GRAPH_DATA section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Color map Section: BODY_FORMAT.GRAPH_DATA\n#     Creates a color map bar using a minimum and maximum domain\n#     e.g. We are using this section to display the entropy distribution in some services\ncmap_min = 0\ncmap_max = 20\ncolor_map_data = {\n    'type': 'colormap',\n    'data': {\n        'domain': [cmap_min, cmap_max],\n        'values': [random.random() * cmap_max for _ in range(50)]\n    }\n}\n# The classification of a section can be set to any valid classification for your system\nsection_color_map = ResultSection(\"Example of colormap result section\", body_format=BODY_FORMAT.GRAPH_DATA,\n                                    body=json.dumps(color_map_data), classification=cl_engine.RESTRICTED)\nresult.add_section(section_color_map)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#url","title":"URL","text":"Code used to generate the URL section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# URL section: BODY_FORMAT.URL\n#   Generate a list of clickable URLs using a JSON encoded format\n#     As you can see here, the body of the section can be set directly instead of line by line\nrandom_host = get_random_host()\nurl_section = ResultSection('Example of a simple url section', body_format=BODY_FORMAT.URL,\n                            body=json.dumps({\"name\": \"Random url!\", \"url\": f\"https://{random_host}/\"}))\n\n# Since URLs are very important features, we can tag those features in the system so that they are easy to find\n#   Tags are defined by a type and a value\nurl_section.add_tag(\"network.static.domain\", random_host)\n\n# You may also want to provide a list of URLs!\n#   Also, no need to provide a name, the URL link will be displayed\nhost1 = get_random_host()\nhost2 = get_random_host()\nurls = [\n    {\"url\": f\"https://{host1}/\"},\n    {\"url\": f\"https://{host2}/\"}]\n\n# A heuristic can fire more then once without being associated to a signature\nurl_heuristic = Heuristic(4, frequency=len(urls))\n\nurl_sub_section = ResultSection('Example of a URL sub-section with multiple links',\n                                body=json.dumps(urls), body_format=BODY_FORMAT.URL,\n                                heuristic=url_heuristic, classification=cl_engine.RESTRICTED)\nurl_sub_section.add_tag(\"network.static.domain\", host1)\nurl_sub_section.add_tag(\"network.dynamic.domain\", host2)\n\n# Since url_sub_section is a subsection of url_section\n# we will add it as a subsection of url_section, not to the main result itself\nurl_section.add_subsection(url_sub_section)\n\nresult.add_section(url_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#json","title":"JSON","text":"Code used to generate the JSON section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# JSON section:\n#     Re-use the JSON editor we use for administration (https://github.com/josdejong/jsoneditor)\n#     to display a tree view of JSON results.\n#     NB: Use this sparingly! As a service developer you should do your best to include important\n#     results as their own result sections.\n#     The body argument must be a json dump of a python dictionary\njson_body = {\n    \"a_str\": \"Some string\",\n    \"a_list\": [\"a\", \"b\", \"c\"],\n    \"a_bool\": False,\n    \"an_int\": 102,\n    \"a_dict\": {\n        \"list_of_dict\": [\n            {\"d1_key\": \"val\", \"d1_key2\": \"val2\"},\n            {\"d2_key\": \"val\", \"d2_key2\": \"val2\"}\n        ],\n        \"bool\": True\n    }\n}\njson_section = ResultSection('Example of a JSON section', body_format=BODY_FORMAT.JSON,\n                                body=json.dumps(json_body))\nresult.add_section(json_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#key_value","title":"KEY_VALUE","text":"Code used to generate the KEY_VALUE section <p>Excerpt from Assemblyline Result sample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# KEY_VALUE section:\n#     This section allows the service writer to list a bunch of key/value pairs to be displayed in the UI\n#     while also providing easy to parse data for auto mated tools.\n#     NB: You should definitely use this over a JSON body type since this one will be displayed correctly\n#         in the UI for the user\n#     The body argument must be a dictionary (only str, int, and booleans are allowed)\nkv_section = ResultKeyValueSection('Example of a KEY_VALUE section')\n# You can add items individually\nkv_section.set_item('key', \"value\")\n# Or simply add them in bulk\nkv_section.update_items({\n    \"a_str\": \"Some string\",\n    \"a_bool\": False,\n    \"an_int\": 102,\n})\nresult.add_section(kv_section)\n\n# ==================================================================\n# ORDERED_KEY_VALUE section:\n#     This section provides the same functionality as the KEY_VALUE section except the order of the fields\n#     are garanteed to be preserved in the order in which the fields are added to the section. Also with\n#     this section, you can repeat the same key name multiple times\noredered_kv_section = ResultOrderedKeyValueSection('Example of an ORDERED_KEY_VALUE section')\n# You can add items individually\nfor x in range(random.randint(3, 6)):\n    oredered_kv_section.add_item(f'key{x}', f\"value{x}\")\n\nresult.add_section(oredered_kv_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#process_tree","title":"PROCESS_TREE","text":"Code used to generate the section <p>Excerpt from Assemblyline Result sample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# PROCESS_TREE section:\n#     This section allows the service writer to list a bunch of dictionary objects that have nested lists\n#     of dictionaries to be displayed in the UI. Each dictionary object represents a process, and therefore\n#     each dictionary must have be of the following format:\n#     {\n#       \"process_pid\": int,\n#       \"process_name\": str,\n#       \"command_line\": str,\n#       \"children\": [] NB: This list either is empty or contains more dictionaries that have the same\n#                          structure\n#     }\nnc_body = [\n    {\n        \"process_pid\": 123,\n        \"process_name\": \"evil.exe\",\n        \"command_line\": \"C:\\\\evil.exe\",\n        \"signatures\": {},\n        \"children\": [\n            {\n                \"process_pid\": 321,\n                \"process_name\": \"takeovercomputer.exe\",\n                \"command_line\": \"C:\\\\Temp\\\\takeovercomputer.exe -f do_bad_stuff\",\n                \"signatures\": {\"one\": 250},\n                \"children\": [\n                    {\n                        \"process_pid\": 456,\n                        \"process_name\": \"evenworsethanbefore.exe\",\n                        \"command_line\": \"C:\\\\Temp\\\\evenworsethanbefore.exe -f change_reg_key_cuz_im_bad\",\n                        \"signatures\": {\"one\": 10, \"two\": 10, \"three\": 10},\n                        \"children\": []\n                    },\n                    {\n                        \"process_pid\": 234,\n                        \"process_name\": \"badfile.exe\",\n                        \"command_line\": \"C:\\\\badfile.exe -k nothing_to_see_here\",\n                        \"signatures\": {\"one\": 1000, \"two\": 10, \"three\": 10, \"four\": 10, \"five\": 10},\n                        \"children\": []\n                    }\n                ]\n            },\n            {\n                \"process_pid\": 345,\n                \"process_name\": \"benignexe.exe\",\n                \"command_line\": \"C:\\\\benignexe.exe -f \\\"just kidding, i'm evil\\\"\",\n                \"signatures\": {\"one\": 2000},\n                \"children\": []\n            }\n        ]\n    },\n    {\n        \"process_pid\": 987,\n        \"process_name\": \"runzeroday.exe\",\n        \"command_line\": \"C:\\\\runzeroday.exe -f insert_bad_spelling\",\n        \"signatures\": {},\n        \"children\": []\n    }\n]\nnc_section = ResultSection('Example of a PROCESS_TREE section',\n                            body_format=BODY_FORMAT.PROCESS_TREE,\n                            body=json.dumps(nc_body))\nresult.add_section(nc_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#table","title":"TABLE","text":"Code used to generate the TABLE section <p>Excerpt from Assemblyline Result sample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# TABLE section:\n#     This section allows the service writer to have their content displayed in a table format in the UI\n#     The body argument must be a list [] of dict {} objects. A dict object can have a key value pair\n#     where the value is a flat nested dictionary, and this nested dictionary will be displayed as a nested\n#     table within a cell.\ntable_body = [\n    {\n        \"a_str\": \"Some string1\",\n        \"extra_column_here\": \"confirmed\",\n        \"a_bool\": False,\n        \"an_int\": 101,\n    },\n    {\n        \"a_str\": \"Some string2\",\n        \"a_bool\": True,\n        \"an_int\": 102,\n    },\n    {\n        \"a_str\": \"Some string3\",\n        \"a_bool\": False,\n        \"an_int\": 103,\n    },\n    {\n        \"a_str\": \"Some string4\",\n        \"a_bool\": None,\n        \"an_int\": -1000000000000000000,\n        \"extra_column_there\": \"confirmed\",\n        \"nested_table\": {\n            \"a_str\": \"Some string3\",\n            \"a_bool\": False,\n            \"nested_table_thats_too_deep\": {\n                \"a_str\": \"Some string3\",\n                \"a_bool\": False,\n                \"an_int\": 103,\n            },\n        },\n    },\n]\ntable_section = ResultSection('Example of a TABLE section',\n                                body_format=BODY_FORMAT.TABLE,\n                                body=json.dumps(table_body))\nresult.add_section(table_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#image","title":"IMAGE","text":"Code used to generate the IMAGE section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Image Section\n#     This type of section allows the service writer to display images to the user\nimage_section = ResultImageSection(request, 'Example of Image section')\nfor x in range(6):\n    image_section.add_image(\n        os.path.join(os.path.dirname(__file__),\n                        'data', f'000{x+1}.jpg'),\n        f'000{x+1}.jpg', f'ResultSample screenshot 000{x+1}', ocr_heuristic_id=6)\nresult.add_section(image_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#timeline","title":"TIMELINE","text":"Code used to generate the TIMELINE section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Timeline Section\n#     This type of section allows the service writer to create a visual timeline\ntimeline_section = ResultTimelineSection(\"Timeline\")\nfor x in range(4):\n    timeline_section.add_node(title=f\"Node {x}\", content=f\"Description: {x}\",\n                                opposite_content=f\"Value: {x}\")\nresult.add_section(timeline_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#multi","title":"MULTI","text":"Code used to generate the MULTI section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Multi Section\n#     This type of section allows the service writer to display multiple section types\n#     in the same result section. Here's a concrete example of this:\nmulti_section = ResultMultiSection('Example of Multi-typed section')\nmulti_section.add_section_part(TextSectionBody(body=\"We have detected very high entropy multiple sections \"\n                                                    \"of your file, this section is most-likely packed or \"\n                                                    \"encrypted.\\n\\nHere are affected sections:\"))\nsection_count = random.randint(1, 4)\nfor x in range(section_count):\n    multi_section.add_section_part(\n        KVSectionBody(section_name=f\".UPX{x}\", offset=f'0x00{8+x}000', size='4196 bytes'))\n    graph_part = GraphSectionBody()\n    graph_part.set_colormap(0, 8, [7 + random.random() for _ in range(20)])\n    multi_section.add_section_part(graph_part)\n    if x != section_count - 1:\n        multi_section.add_section_part(DividerSectionBody())\n    multi_section.add_tag(\"file.pe.sections.name\", f\".UPX{x}\")\n\nmulti_section.set_heuristic(5)\nresult.add_section(multi_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/result_section/#sandbox","title":"SANDBOX","text":"Code used to generate the SANDBOX section <p>Excerpt from the Assemblyline ResultSample service: result_sample.py</p> <pre><code>...\n# ==================================================================\n# Sandbox Section\n#     This section allows a service writer to structure, correlate, and\n#     enrich sandbox analysis results in a single, unified result section.\n#\n#     The SANDBOX section is designed to represent dynamic analysis output\n#     such as:\n#        - Analysis and environment metadata\n#        - Process execution trees\n#        - Network activity (DNS, HTTP, SMTP, TCP, etc.)\n#        - Detection signatures and ATT&amp;CK mappings\n#\n#     Each data category is modeled using strongly typed items\n#     (SandboxProcessItem, SandboxNetflowItem, SandboxSignatureItem, etc.),\n#     allowing the UI to present the data in structured, tabbed views.\nsandbox_section = ResultSandboxSection(\"Example of a SANDBOX section\")\n\n# ------------------------------------------------------------------\n# Sandbox analysis information\n# ------------------------------------------------------------------\nsandbox_section.set_analysis_information(\n    sandbox_name=\"Cuckoo Sandbox\",\n    sandbox_version=\"2.0.7\",\n    analysis_metadata=SandboxAnalysisMetadata(\n        task_id=1,\n        start_time=\"2025-10-14T12:00:00Z\",\n        end_time=\"2025-10-14T12:10:30Z\",\n        routing=\"Internet\",\n        window_size=\"1024x768\",\n        machine_metadata=SandboxMachineMetadata(\n            ip=\"192.168.0.15\",\n            hypervisor=\"KVM\",\n            hostname=\"analysis-vm-01\",\n            platform=\"Windows\",\n            version=\"10.0.19045\",\n            architecture=\"x64\",\n        ),\n    ),\n)\n\n# ------------------------------------------------------------------\n# Processes\n# ------------------------------------------------------------------\nsandbox_section.add_process(SandboxProcessItem(\n    image=\"C:\\\\Windows\\\\System32\\\\svchost.exe\",\n    start_time=\"2025-10-14T12:00:02Z\",\n    end_time=\"2025-10-14T12:01:00Z\",\n    pid=50,\n    ppid=4,\n    command_line=\"svchost.exe -k netsvcs\",\n    integrity_level=\"system\",\n    image_hash=\"svchosthash001\",\n    original_file_name=\"svchost.exe\",\n    safelisted=True,\n    sources=[\"capemon\"],\n))\n\nsandbox_section.add_process(SandboxProcessItem(\n    image=\"powershell.exe\",\n    start_time=\"2025-10-14T12:00:10Z\",\n    end_time=\"2025-10-14T12:00:45Z\",\n    pid=120,\n    ppid=100,\n    command_line=\"powershell.exe -enc aQBmACgA\",\n    integrity_level=\"high\",\n    image_hash=\"evilhash789\",\n    safelisted=False,\n    sources=[\"capemon\"],\n))\n\n# ------------------------------------------------------------------\n# Network connections\n# ------------------------------------------------------------------\nsandbox_section.add_network_connection(SandboxNetflowItem(\n    destination_ip=\"45.83.23.19\",\n    destination_port=80,\n    source_ip=\"192.168.0.15\",\n    source_port=54321,\n    time_observed=\"2025-10-14T12:00:10Z\",\n    process=120,\n    direction=\"outbound\",\n    transport_layer_protocol=\"tcp\",\n    http_details=SandboxNetworkHTTP(\n        request_uri=\"http://malicious.example.com/payload.exe\",\n        request_method=\"GET\",\n        response_status_code=200,\n        response_content_mimetype=\"application/octet-stream\",\n        request_headers={\"User-Agent\": \"PowerShell\"},\n    ),\n    connection_type=\"http\",\n    sources=[\"capemon\"],\n))\n\n# ------------------------------------------------------------------\n# Signatures\n# ------------------------------------------------------------------\nsandbox_section.add_signature(SandboxSignatureItem(\n    name=\"Suspicious PowerShell Execution\",\n    type=\"CUCKOO\",\n    classification=cl_engine.RESTRICTED,\n    description=\"PowerShell launched with encoded commands\",\n    score=1000,\n    pid=[120],\n    attacks=[\n        SandboxAttackItem(\"T1059.001\", \"PowerShell execution\", [\"execution\"]),\n    ],\n    sources=[\"capemon\"],\n))\n\n# ------------------------------------------------------------------\n# Add the SANDBOX section to the result\n# ------------------------------------------------------------------\nresult.add_section(sandbox_section)\n...\n</code></pre>"},{"location":"developer_manual/services/advanced/service_base/","title":"ServiceBase Class","text":""},{"location":"developer_manual/services/advanced/service_base/#servicebase-class","title":"ServiceBase class","text":"<p>All service created for Assemblyline must inherit from the <code>ServiceBase</code> class which can be imported from <code>assemblyline_v4_service.common.base</code>. In this section we will go through the different methods and variables available to you in the <code>ServiceBase</code> class.</p> <p>You can view the source for the class here: ServiceBase class source</p>"},{"location":"developer_manual/services/advanced/service_base/#class-variables","title":"Class variables","text":"<p>The <code>ServiceBase</code> base class includes many instance variables which can be used to access service related information.</p> <p>The following tables describes all of the variables of the <code>ServiceBase</code> class.</p> Variable Name Description config Reference to the service parameters containing values updated by the user for service configuration. dependencies A dictionary containing connection details for service dependencies log Reference to the logger. ontologies A dictionary containing ontologies to get appended to a service result. rules_directory Returns the directory path which contains the current location of your rules rules_hash A hash of the files in the rules_list. Used to invalidate caching if rules change. rules_list Returns a list of directory paths which point to rule files derived from rules_directory service_attributes Service attributes from the service manifest. update_time An integer representing the epoch of when the last update occurred working_directory Returns the directory path which the service can use to temporarily store files during each task execution. <p>The <code>config</code> property is a dictionary that comes from the service manifest that will contain the default values. An Assemblyline system administrator can go into the service management section of their instance <code>/admin/services</code> and change the default configuration, so you'll be able to access that configuration through <code>config</code>. An example of this is the default password list of the Extract service, known as <code>default_pw_list</code>. This list can be configured by your administrator to always include \"password\", \"infected\" or the name of your organization.</p> <p></p> <p>The <code>working_directory</code> property is a temporary directory that you should use if you extract or create temporary files. That way, your different file submissions won't interfere with each other, and you won't leave files in your running container that could start bloating it.</p> <p>If you need to log anything for debugging, you could use <code>print</code> or <code>self.log</code>. The advantage of <code>self.log</code> is that with the right setup, it'll be forwarded to your logging stack where you can do more analysis.</p>"},{"location":"developer_manual/services/advanced/service_base/#class-functions","title":"Class functions","text":"<p>This is the list of all the functions that you can override in your service. They are explained in order of importance and the likelihood at which you will override them.</p>"},{"location":"developer_manual/services/advanced/service_base/#execute","title":"execute()","text":"<p>This is where your service processing code lives!</p> <p>The <code>execute</code> function is called every time the service receives a new file to scan. To this function, a single Request object is provided as an input which provides the service all the information available about the file which was requested for scanning.</p> <p>It is inside this function that you will create a Result objects to send your scan results back to the system for storage and display in the user interface. This has to be done before the end of the function execution. Make sure you go through the tips on writing good results before starting to built your service.</p> <p>If multiple files come in around the same time, the system is optimized to reuse deployed services. The <code>start</code> function won't be called between submissions, so you need to make sure not to keep data between runs in the class variables as it can lead to wrong results.</p>"},{"location":"developer_manual/services/advanced/service_base/#get_tool_version","title":"get_tool_version()","text":"<p>The purpose of the <code>get_tool_version</code> function is to the return a string indicating the version of the tools used by the service or a hash of the signatures it uses. The tool version should be updated to reflect changes in the service tools or signatures, so that Assemblyline can rescan files on the new service version if they are submitted again.</p>"},{"location":"developer_manual/services/advanced/service_base/#init-and-start","title":"init() and start()","text":"<p>The <code>__init__</code> and <code>start</code> functions are called when the Assemblyline service is initiated and should be used to prepare your service for task execution. The main difference between these methods is that you have access to the content provided by your updater in the <code>start</code> method.</p>"},{"location":"developer_manual/services/advanced/service_base/#get_api_interface","title":"get_api_interface()","text":"<p>The purpose of the <code>get_api_interface</code> function is to give the service direct access to the service server component to perform API request to find out if a file or a tag is meant to be safelisted.</p> <p>Warning</p> <p>By using this function, your service will not work using the <code>run_service_once</code> command and will be completely tied to the service server API.</p>"},{"location":"developer_manual/services/advanced/service_base/#stop","title":"stop()","text":"<p>The <code>stop</code> function is called when the Assemblyline service is stopped and should be used to cleanup your service.</p> <p>The following functions are used if and only if you're using a dependency that's a service updater named 'updates'. For this reason, we reserve the dependency name 'updates' to be used for service updaters.</p>"},{"location":"developer_manual/services/advanced/service_base/#_load_rules","title":"_load_rules()","text":"<p>The <code>_load_rules</code> function is called to process the rules_list in a specific way defined by the service.</p>"},{"location":"developer_manual/services/advanced/service_base/#_clear_rules","title":"_clear_rules()","text":"<p>The <code>_clear_rules</code> function is optionally called to remove the current ruleset from memory. Requires implementation by the service writer for use.</p>"},{"location":"developer_manual/services/advanced/service_base/#_download_rules","title":"_download_rules()","text":"<p>The <code>_download_rules</code> function is called after each <code>_cleanup</code> call to check if there is new updates to be processed. If so, it will attempt to download and use the new ruleset otherwise it will revert to the old ruleset.</p> <p>It will call on <code>_load_rules</code> and <code>_clear_rules</code> during this attempt process.</p>"},{"location":"developer_manual/services/advanced/service_base/#attach_ontological_result","title":"attach_ontological_result()","text":"<p>The <code>attach_ontological_result</code> function is called when you want to attach an ontological result to your service result.</p> <p>Refer to Result Ontologies for more information on the current set of ontological models.</p>"},{"location":"developer_manual/services/advanced/service_manifest/","title":"Service manifest","text":""},{"location":"developer_manual/services/advanced/service_manifest/#service-manifest","title":"Service manifest","text":"<p>Every service must have a <code>service_manifest.yml</code> file in its root directory. The manifest file presents essential information about the service to the Assemblyline core system, information the Assemblyline core system must have before it can run the service.</p> <p>The table below shows all the elements that the manifest file can contain, including a brief description of each.</p> Field name Value type Required? Description accepts Keyword No  Default: <code>.*</code> Regexes applied to Assemblyline style file type string. For example, <code>.*</code> will allow the service to accept all types of files. category Keyword No  Default: <code>Static Analysis</code> Which category is the service part of? Must be one of <code>Antivirus</code>, <code>Dynamic Analysis</code>, <code>External</code>, <code>Extraction</code>, <code>Filtering</code>, <code>Internet Connected</code>, <code>Networking</code>, or <code>Static Analysis</code>. config Mapping of Any No Dictionary of service configuration variables. The key names can be any Keyword and the value can be of Any type. default_result_classification Classification string No  Default: <code>UNRESTRICTED</code> The default classification for the results generated by the service. If no classification is provided for a result section, this default classification is used. dependencies Mapping of Dependency Config No Refer to the dependency config section. description Text No  Default: <code>NA</code> Detailed description of the service and its features. disable_cache Boolean No  Default: <code>false</code> Should the result cache be disabled for this service? Only disable caching for services that will always provide different results each run. docker_config Docker Config Yes Refer to the Docker Config section. enabled Boolean No  Default: <code>false</code> Should the service be enabled by default? file_required Boolean Does the service require access to the file to perform its task? If set to <code>false</code>, the service will only have access to the file metadata (e.g. hashes, size, type, etc.). heuristics List of Heuristic No List of heuristic(s) used in the service for scoring. Refer to the heuristic section. is_external Boolean No  Default: <code>false</code> Does the service make API calls to other products not part of the Assemblyline infrastructure (e.g. VirusTotal, ...)? licence_count Integer No  Default: <code>0</code> Number of concurrent services allowed to run at the same time. name Keyword Yes Name of the service. privileged Boolean No  Default: <code>false</code> Allow service to have direct access to core for processing.  Note: Should only be enabled on services that perform static analysis. rejects Keyword No  Default: <code>empty|metadata/.*</code> Regexes applied to Assemblyline style file type string. For example, <code>empty|metadata/.*</code> will reject all empty and metadata files. stage Keyword No  Default: <code>CORE</code> At which stage should the service run. Must be one of: (1) <code>FILTER</code>, (2) <code>EXTRACT</code>,  (3) <code>CORE</code>, (4) <code>SECONDARY</code>, (5) <code>POST</code>, (6) <code>REVIEW</code>. Note that stages are executed in the numbered order shown. submission_params List of Submission Params No List of submission param(s) that define parameters that the user can change about the service for each of its scans. Refer to the submission_params section. timeout Integer No  Default: <code>60</code> Maximum execution time the service has before the task is timed out. update_config Update Config No Refer to the update config section. version<sup>1</sup> Keyword Yes Version of the service. <p><sup>1</sup> the version in the manifest must be the same as the image tag in order to successfully pass registration on service update/load.</p>"},{"location":"developer_manual/services/advanced/service_manifest/#dependency-config","title":"Dependency config","text":"Field name Value type Required? Description container Docker Config Yes Refer to Docker Config section. volumes Mapping of Persistent Volume No Refer to the persistent volume section."},{"location":"developer_manual/services/advanced/service_manifest/#docker-config","title":"Docker config","text":"Field name Value type Required? Description allow_internet_access Boolean No  Default: <code>false</code> Should the container be allowed to access the internet? command List[Keyword] No Command that should be run when the container launches. cpu_cores Float No  Default: <code>1.0</code> Amount of CPU that should be allocated to the container. environment List of Environment Variable No Refer to the environment variable section. image Keyword Yes Image name always prepended by ${REGISTRY} or ${PRIVATE_REGISTRY} if image not on DockerHub. Append the rest of the image path. Do not put a / between the rest of the image path and registry var. Image name always ends in :$SERVICE_TAG ports List[Keyword] No List of ports to bind from the container. ram_mb Integer No  Default: <code>1024</code> Amount of RAM in MB that should be allocated to the container."},{"location":"developer_manual/services/advanced/service_manifest/#environment-variable","title":"Environment variable","text":"Field name Value type Required? Description name Keyword Yes Name of the variable. value Keyword Yes Value of the variable."},{"location":"developer_manual/services/advanced/service_manifest/#heuristic","title":"Heuristic","text":"Field name Value type Required? Description attack_id Enum No Mitre's Att&amp;ck matrix ID. classification Classification No  Default: <code>UNRESTRICTED</code> description Text Yes Detailed description of the heuristic which addresses the technique used to score. filetype Keyword Yes Regex of the filetype which applies to this heuristic. heur_id Keyword Yes Unique ID for identifying the heuristic. max_score Integer No The maximum score the heuristic can have. name Keyword Yes Short name for the heuristic. score Integer Yes Score that should be applied when this heuristic is set."},{"location":"developer_manual/services/advanced/service_manifest/#persistent-volume","title":"Persistent volume","text":"Field name Value type Required? Description mount_path Keyword Yes Path into the container to mount volume. capacity Keyword Yes Storage capacity required in bytes. storage_class Keyword Yes"},{"location":"developer_manual/services/advanced/service_manifest/#submission-params","title":"Submission params","text":"Field name Value type Required? Description default Any Yes Default value of the parameter. name Keyword Yes Variable name of the parameter. type Enum Yes Type of variable. Must be one of: <code>bool</code>, <code>int</code>, <code>list</code>, or <code>str</code>. value Any Yes Value of the variable as configured by the user or the default if not configured."},{"location":"developer_manual/services/advanced/service_manifest/#update-config","title":"Update config","text":"Field name Value type Required? Description generates_signatures Boolean No  Default: <code>false</code> Should the downloaded files be used to create signatures in the system? sources List of Update Source No List of source(s) from which updates can be downloaded. Refer to the update source section. update_interval_seconds Integer Yes Interval in seconds at which the updater runs. wait_for_update Boolean False Should the service wait for its updater dependency to be running? signature_delimiter Enum Must be of: <code>new_line</code>, <code>double_new_line</code>, <code>pipe</code>, <code>comma</code>, <code>space</code>, <code>none</code>, <code>file</code>, <code>custom</code> Type of delimiter used for signaure downloads. custom_delimiter Keyword Optional Custom signature delimiter to use when <code>signature_delimiter: custom</code>"},{"location":"developer_manual/services/advanced/service_manifest/#update-source","title":"Update source","text":"Field name Value type Required? Description headers List of Environment Variable No Refer to the environment variable section. name Keyword Yes Unique name of the source. password Keyword No The password required to access the file. pattern Keyword No Regex pattern to match against the file names of all downloaded files from this source. This is useful when you want to filter out some files from a repo which contains many files. private_key Keyword No Key for accessing file or Git repo. uri Keyword Yes URL of the update file. Some example URL formats are: <code>git@github.com:sample/sample-repo.git</code>, <code>https://file-examples.com/wp-content/uploads/2017/02/zip_2MB.zip</code>. username Keyword No The username required to access the file."},{"location":"developer_manual/services/advanced/service_updater_base/","title":"ServiceUpdater Class","text":""},{"location":"developer_manual/services/advanced/service_updater_base/#serviceupdater-class","title":"ServiceUpdater class","text":"<p>Some services created for Assemblyline will require signatures/rules as part of it's analysis process. <code>ServiceUpdater</code> class which can be imported from <code>assemblyline_v4_service.updater.updater</code>. In this section we will go through the different methods and variables available to you in the <code>ServiceUpdater</code> class.</p> <p>You can view the source for the class here: ServiceUpdater class source</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#class-variables","title":"Class variables","text":"<p>The <code>ServiceUpdater</code> base class includes many instance variables which can be used to access service related information.</p> <p>The following tables describes all of the variables of the <code>ServiceUpdater</code> class.</p> Variable Name Description updater_type The type of updater, typically the service's name lower-cased taken from the SERVICE_PATH environment variable default_pattern The default regex pattern used if a source doesn't provide one (Default: .*)"},{"location":"developer_manual/services/advanced/service_updater_base/#class-functions","title":"Class functions","text":"<p>This is the list of all the functions that you can override in your updater. They are explained in order of importance and the likelihood at which you will override them.</p> <p>Note: the updater are yours to define however you would like for your service, we have implemented the basics that work with our existing services but this does not mean you have to follow what's already defined. Override it! ie. [Safelist] (https://github.com/CybercentreCanada/assemblyline-service-safelist)</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#import_update-implementation-required","title":"import_update() [Implementation Required]","text":"<p>The <code>import_update</code> function is called to import a set of files into Assemblyline. This involves creating a list of <code>Signature</code> objects and importing them via the Signature API by using the Assemblyline client.</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#do_source_update-override-optional","title":"do_source_update() [Override Optional]","text":"<p>The <code>do_source_update</code> function is called on a separate thread that checks external signature sources for changes. This will then fetch the new ruleset on modification and update the signature set in Assemblyline to make it available to both users and the <code>do_local_update</code> thread.</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#is_valid-override-optional","title":"is_valid() [Override Optional]","text":"<p>The <code>is_valid</code> function is called to determine if a file from a source is a valid file. The definition of its validity can vary between services but it should be able to answer the question 'Can the service use this?'.</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#do_local_update-override-optional","title":"do_local_update() [Override Optional]","text":"<p>The <code>do_local_update</code> function is called on a separate thread that checks Assemblyline for local changes to signatures such as change in status or additions/removals to the ruleset. This will then fetch the new ruleset on modification and make it available to be served to service instances.</p>"},{"location":"developer_manual/services/advanced/service_updater_base/#prepare_output_directory-override-optional","title":"prepare_output_directory() [Override Optional]","text":"<p>The <code>prepare_output_directory</code> function is called to prepare/organize your collection of external files/directories in a format that's acceptable by your service. The path to the resulting output_directory should be returned so it can be served.</p> <p>Note: This only needs to be implemented if the your service manifest contains <code>update_config.generates_signatures: False</code>. The implicatation is that whatever is generated won't get added to the Signatures API and will be local to the updater only to manage.</p> <p>Warning</p> <p>Failure to implement import_update() in your service's subclass of <code>ServiceUpdater</code> will render the updater unusable.</p> <p>For an example, see Adding a Service Updater</p>"},{"location":"getting_help/contact_us/","title":"Contact Us","text":""},{"location":"getting_help/contact_us/#contact-us","title":"Contact Us","text":"<p>We will update this section with typical issues and solutions that arise when deploying Assemblyline.</p> <p>You can post your question to our Discord community or feel free to open an issue on our Github.</p>"},{"location":"getting_help/core/","title":"Core","text":""},{"location":"getting_help/core/#core","title":"Core","text":"<p>This section contains troubleshooting steps for core components of Assemblyline</p>"},{"location":"getting_help/core/#api-ui","title":"API / UI","text":"When deployed behind a proxy, I just get a login loop upon sign in <p>By default, Assemblyline will try to perform validation of the session IP and user-agent. Since operating behind a proxy could cause either or both to fail validation, you can disable these checks with the following configuration(s): </p><pre><code>ui:\n  validate_session_ip: false\n  validate_session_useragent: false\n</code></pre><p></p>"},{"location":"getting_help/core/#statistics","title":"Statistics","text":"Why is the hits counter for a certain signature not incrementing even though it hit 5000 times in the last hour? <p>Rules' hit count are not calculated live. There is an external process that calculates them daily for performance optimization.</p> <p>However, you can click on the rule itself and it does calculate the stats for that specific rule and updates it right away.</p>"},{"location":"getting_help/core/#updater","title":"Updater","text":"Failed to establish a new connection: [Errno 110] Connection timed out <p>Depending on the host mentioned in the error, ensure the deployment has access to the registry and its able to call the associated API.</p> <p>The following modifications will have to be made to your values.yaml:</p> External RegistryLocal Registry <p>Let's say the domain of the registry is 'registry.local' and is hosted on port 443 </p><pre><code>configuration:\nservices:\n    image_variables:\n        REGISTRY: \"registry.local/\"\n    allow_insecure_registry: true\n</code></pre><p></p> <p>Let's say the local registry is hosted on port 32000 </p><pre><code>configuration:\nservices:\n    image_variables:\n        REGISTRY: \"localhost:32000/\"\n    update_image_variables:\n        REGISTRY: \"&lt;HOST_IP&gt;:32000/\"\n    allow_insecure_registry: true\n</code></pre><p></p>"},{"location":"getting_help/docker/","title":"Docker","text":""},{"location":"getting_help/docker/#docker","title":"Docker","text":"<p>This section contains troubleshooting steps for deploying Assemblyline in a Docker environment.</p> Redis is logging: 'WARNING Memory overcommit must be enabled!', what should I do? <p>To fix this issue add <code>vm.overcommit_memory = 1</code> to /etc/sysctl.conf and then reboot or run the command <code>sysctl vm.overcommit_memory=1</code> for this to take effect. This has to be performed on the host that's running the deployment.</p>"},{"location":"getting_help/kubernetes/","title":"Kubernetes","text":""},{"location":"getting_help/kubernetes/#kubernetes","title":"Kubernetes","text":"<p>This section contains troubleshooting steps for deploying Assemblyline in a Kubernetes environment.</p> Connection timeouts to external domains <p>By default, coreDNS is configured to the Google's Public DNS when trying to resolve external domains outside the cluster.</p> <p>You can configure it to use a different DNS via: <code>sudo microk8s disable dns &amp;&amp; sudo microk8s enabled dns:1.1.1.1</code></p> <p>If this still poses an issue, refer to: Teleport - Troubleshooting Kubernetes Networking Issues</p> How can I monitor the status of the deployment/cluster? <p>The quickest way to monitor the status of your cluster is: <code>kubectl get pods -n &lt;assemblyline_namespace&gt;</code></p> <p>There are other tools such as k9s and FreeLens (an OSS fork of Lens/OpenLens) that allow you to monitor your cluster in a more user-friendly manner.</p> Are HPAs adjustable? <p>Depending on the amount of activity you're receiving, you'll likely have to tweak the TargetUsage andReqRam settings in your values.yaml for your particular deployment, either to cause it to scale faster or slower.</p> <p>Refer to: Kubernetes - Horizontal Pod Autoscaler for more information.</p> NGINX 504 but everything seems to be running <p>It's possible the domain you're accessing the UI with doesn't match the setting <code>configuration.ui.fqdn</code> in your values.yaml. If your setting is set to 'localhost' but you're accessing the UI using '192.0.0.1.nip.io', there is no ingress path using '192.0.0.1.nip.io' as a base.</p> <p>The simplest solution is to update your values.yaml to the appropriate FQDN and redeploy.</p> Is it possible to mount an internal root CA bundle into core components to use? <p>Yes! This would involve creating a configmap containing your CA bundle and using <code>coreVolumes, coreMounts, and coreEnv</code> in your values.yaml to pass that information onto the core deployments.</p> <p>An example of this configuration would be: </p><pre><code>coreEnv:\n    - name: REQUESTS_CA_BUNDLE\n    value: /usr/share/internal-ca\ncoreMounts:\n    - name: internal-certs\n    mountPath: /usr/share/internal-ca\ncoreVolumes:\n    - name: internal-certs\n    configMap:\n        name: internal-certs-config\n</code></pre><p></p> Ignoring ingress because of error while validating ingress class\" ingress=\"&lt;namespace&gt;/&lt;release&gt;-ingress\" error=\"no object matching key \"nginx\" in local store <p>This error can typically happen if the Assemblyline ingress' class name doesn't match the class name of the ingress controller on the system. You can fix this by assigning the name of the class to <code>ingressClassName</code> in your values.yaml and re-deploy.</p> Service pods aren't getting scheduled because there are labels missing <p>You can add global labels to service pods by using the <code>core.scaler.additional_labels</code> or <code>core.scaler.privileged_services_additional_labels</code>: </p><pre><code>core:\n  scaler:\n    additional_labels:\n      - \"service_section=normal\"\n    privileged_services_additional_labels:\n      - \"service_section=privileged\"\n</code></pre><p></p> <p>Alternatively, you can set labels on an individual container basis by using the UI/API (1)</p>"},{"location":"getting_help/services/","title":"Services","text":""},{"location":"getting_help/services/#services","title":"Services","text":"<p>This section contains troubleshooting steps for service components of Assemblyline</p>"},{"location":"getting_help/services/#general","title":"General","text":"TASK PRE-EMPTED <p>A service task can pre-empt for a number of reasons, such as:</p> <ol> <li>Container being killed due to OOM (Out of Memory)<ul> <li>Increase service memory limits</li> <li>Debug service memory usage with sample</li> </ul> </li> <li>Service instance ran beyond the service timeout<ul> <li>Increase service timeout</li> <li>Debug service runtime with sample</li> </ul> </li> <li>The service result didn't make it back to service-server<ul> <li>Check the state of service-server deployments</li> <li>Inspect network policies</li> </ul> </li> </ol> Is there a limit resources to allocate to a service instance? <p>No! The sky's the limit (or more accurately), you're bound to the resources allocated on your cluster. That being said, it's best practice to use what you need rather than what you want (especially if it's unwarranted).</p> Can I deploy a 4.X service on a 4.Y Assemblyline system (where X,Y are system versions: X &lt; Y) <p>Yes and no. You can add a service with to the system with a lesser system version but it won't be enabled due to potential compatibility issues. It's advised to rebuild the service and tag with the system version that you want to deploy on.</p> Every time I update my service, the values/parameters are getting reset.. <p>This can stem from loading the service initially with production values which causes an issue with the service's service_delta. It's best practice to add a service using the default manifest and update the values after it's been registered.</p> <p>If you have many services that require an update in values or if there's many values to update in a single service, consider using the Assemblyline client.</p>"},{"location":"getting_help/services/#service-updater","title":"Service Updater","text":"My service doesn't seem to be getting any signatures for analysis.. <p>Check the following on your service's updater instance:</p> <ol> <li>Does your service have the following environment variables set: updates_host, updates_port, updates_key<ul> <li>Edit the deployment manually</li> <li>Disable the service, delete the related deployments from Kubernetes, restart scaler, then re-enable service</li> </ul> </li> <li>Is there any downloaded signatures in <code>/tmp/updater/update_dir*/*/</code> of the updater container?<ul> <li>If empty, it suggests there was a problem pulling the signatures from Assemblyline</li> <li>If some sources are missing, it suggests there was a problem pulling signatures from that source</li> </ul> </li> </ol> I've added my signature sources, but nothing shows up when searching the Signature index.. <ol> <li>Are the source links accessible from the cluster?</li> <li>Is the authentication for each source valid?</li> <li>How frequently is the service updater configured to check for source updates?</li> </ol> If I modify signatures using the Assemblyline client or the API, will the service get these rules? <p>Yes! Changes made involving our API trigger messaging events made to other components to Assemblyline causing the system to be more responsive. In the case of services with updaters, they'll be notified immediately and the service will gather the new rules after a submissions has processed.</p>"},{"location":"installation/classification_engine/","title":"Classification engine","text":""},{"location":"installation/classification_engine/#classification-engine","title":"Classification engine","text":"<p>Assemblyline can do record-based access control for submission, files, results, and even up to individual sections and tags of said results. It was built to support the Government of Canada levels of security and the Traffic Light Protocol but can also be modified at will.</p> <p>Once turned on, the classification engine will do the following changes to the system:</p> <ul> <li>Users will have to be assigned a maximum classification level that they can see as well as the groups they are members of</li> <li>Each submission to the system will have to have a classification level</li> <li>The User Interface will:<ul> <li>Show the effective classification of each submission, file, result, and result sections</li> <li>Have a dedicated help section that will explain how classification conflicts are resolved</li> <li>Let you pick a classification while submitting a file</li> <li>Automatically hide portions of the result for a user that does not have enough privileges to see them</li> </ul> </li> </ul>"},{"location":"installation/classification_engine/#configuration","title":"Configuration","text":"<p>The classification engine has many parameters that can be customized so you can get record-based access controls that fit your organization.</p> <p>Here is an exhaustive configuration file of the classification engine that explains every single parameter:</p> Exhaustive classification configuration <pre><code># Turn on/off classification enforcement. When this flag is off, this\n#  completely disables the classification engine, any documents added while\n#  the classification engine is off getting the default unrestricted value\nenforce: false\n\n# When this flag is on, the classification engine will automatically create\n#  groups based on the domain part of a user's email address\n#  EX:\n#     For user with email: test@local.host\n#     The group \"local.host\" will be valid in the system\ndynamic_groups: false\n\n# List of Classification Levels:\n#   This is a graded list; a smaller number is less restricted than a higher number\n#   A user must be allowed a classification level &gt;= to be able to view the data\nlevels:\n  # List of alternate names for the current marking. If a user submits a file with\n  #  those markings, the classification will automatically rename it to the value\n  #  specified in the name\n  - aliases:\n      - UNRESTRICTED\n      - UNCLASSIFIED\n      - U\n      - TLP:W\n      - TLP:WHITE\n    # Stylesheet applied in the UI for the current classification level\n    css:\n      # Name of the color scheme used for display\n      # Possible values: default, primary, secondary, success, info, warning, error\n      color: default\n\n      # Deprecated parameters (Use color instead)\n      #  These were used in the old UI but are still valid in the new UI because if\n      #  the new UI cannot find the color param, it will use the label param and\n      #  strips \"label-\" part.\n      banner: alert-default\n      label: label-default\n      text: text-muted\n\n    # Description of the classification level\n    description:\n      Subject to standard copyright rules, TLP:CLEAR information may be distributed\n      without restriction.\n\n    # Integer value of the Classification level (higher is more classified)\n    lvl: 100\n\n    # Long name of the classification level\n    name: TLP:CLEAR\n\n    # Short name of the classification level\n    short_name: TLP:C\n  - aliases: []\n    css:\n      color: success\n    description:\n      Recipients may share TLP:GREEN information with peers and partner organizations\n      within their sector or community, but not via publicly accessible channels.\n      Information in this category can be circulated widely within a particular\n      community. TLP:GREEN information may not be released outside of the community.\n    lvl: 110\n    name: TLP:GREEN\n    short_name: TLP:G\n  - aliases:\n      - RESTRICTED\n    css:\n      color: warning\n    description:\n      Recipients may only share TLP:AMBER information with members of their\n      own organization and with clients or customers who need to know the information\n      to protect themselves or prevent further harm.\n    lvl: 120\n    name: TLP:AMBER\n    short_name: TLP:A\n\n# List of required tokens:\n#   A user requesting access to an item must have all the\n#   required tokens the item has, to gain access to it\nrequired:\n  # List of alternate names for the token\n  - aliases: []\n\n    # Description of the token\n    description: Produced using a commercial tool with limited distribution\n\n    # Long name for the token\n    name: COMMERCIAL\n\n    # Short name for the token\n    short_name: CMR\n\n    # (optional) The minimum classification level an item must have\n    #  for this token to be valid. So, because this token has a value\n    #  of 120, once it's selected, the classification level automatically\n    #  jumps to TPL:A which was set to 120 in the previous section.\n    require_lvl: 120\n\n# List of groups:\n#   A user requesting access to an item must be part of a least\n#   of one the group the item is part of to gain access\ngroups:\n  # List of aliases for the group\n  - aliases:\n      - ANY\n    # (optional) This is a special flag that when set to true if any other groups\n    #  are selected in a classification, this group will automatically be selected\n    #  as well.\n    auto_select: true\n\n    # Description of the group\n    description: Employees of CSE\n\n    # Long name for the group\n    name: CSE\n\n    # Short name for the group\n    short_name: CSE\n\n    # (optional) Assuming that this group is the only group selected, this is the\n    #   display name that will be used in the classification\n    #   NOTE: values must be in the aliases of this group and only this group\n    solitary_display_name: ANY\n\n# List of sub-groups:\n#   A user requesting access to an item must be part of a least\n#   of one the sub-group the item is part of to gain access\nsubgroups:\n  # List of aliases for the subgroups\n  - aliases: []\n\n    # Description of the sub-group\n    description: Member of Incident Response team\n\n    # Long name of the sub-group\n    name: IR TEAM\n\n    # Short name of the sub-group\n    short_name: IR\n  - aliases: []\n    description: Member of the Canadian Centre for Cyber Security\n\n    # (optional) This is a special flag that auto select the corresponding\n    #   group when this sub-group is selected\n    require_group: CSE\n\n    name: CCCS\n    short_name: CCCS\n\n    # (optional) This is a special flag that makes sure that none other then\n    #   the corresponding group is selected when this sub-group is selected\n    limited_to_group: CSE\n\n# Default restricted classification\nrestricted: TLP:A//CMR\n\n# Default unrestricted classification:\n#   When no classification is provided or the classification engine is\n#   disabled, this is the classification value each item will get\nunrestricted: TLP:C\n</code></pre>"},{"location":"installation/classification_engine/#enabling-it-in-your-system","title":"Enabling it in your system","text":"<p>By default, the classification engine is disabled in the system, but it can easily be enabled by creating a new config map in Kubernetes.</p> <p>Info</p> <p>For this documentation we will enable a classification engine that supports only the traffic light protocol.</p> <p>In your deployment directory, create a file named <code>objects.yaml</code> with the following content:</p> objects.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assemblyline-extra-config\ndata:\n  classification: |\n    enforce: true\n    levels:\n      - aliases:\n          - UNRESTRICTED\n          - U\n          - TLP:W\n          - TLP:WHITE\n        css:\n          color: default\n        description:\n          Subject to standard copyright rules, TLP:CLEAR information may be\n          distributed without restriction.\n        lvl: 100\n        name: TLP:CLEAR\n        short_name: TLP:C\n      - aliases: []\n        css:\n          color: success\n        description:\n          Recipients may share TLP:GREEN information with peers and partner\n          organizations within their sector or community, but not via publicly\n          accessible channels. Information in this category can be circulated\n          widely within a particular community. TLP:GREEN information may not\n          be released outside of the community.\n        lvl: 110\n        name: TLP:GREEN\n        short_name: TLP:G\n      - aliases:\n          - RESTRICTED\n          - R\n        css:\n          color: warning\n        description:\n          Recipients may only share TLP:AMBER information with members of their\n          own organization and with clients or customers who need to know the\n          information to protect themselves or prevent further harm.\n        lvl: 120\n        name: TLP:AMBER\n        short_name: TLP:A\n\n    required: []\n    groups: []\n    subgroups: []\n\n    restricted: TLP:A\n    unrestricted: TLP:C\n</code></pre> <p>Use kubectl to apply the <code>objects.yaml</code> file to your system:</p> ApplianceCluster <pre><code>sudo microk8s kubectl apply -f ~/git/deployment/objects.yaml --namespace=al\n</code></pre> <pre><code>kubectl apply -f &lt;deployment_directory&gt;/objects.yaml --namespace=al\n</code></pre> <p>Then you must tell your pods to use the classification engine from the newly created config map. Add the following block to the <code>values.yaml</code> or your deployment:</p> Partial values.yaml to load the custom classification.yml file <pre><code>...\ncoreEnv:\n  - name: CLASSIFICATION_CONFIGMAP\n    value: assemblyline-extra-config\n  - name: CLASSIFICATION_CONFIGMAP_KEY\n    value: classification\ncoreMounts:\n  - name: al-extra-config\n    mountPath: /etc/assemblyline/classification.yml\n    subPath: classification\ncoreVolumes:\n  - name: al-extra-config\n    configMap:\n      name: assemblyline-extra-config\n...\n</code></pre> <p>You will also need to ensure services have your <code>classification.yml</code> file mounted. Add the following to your <code>values.yaml</code>:</p> Partial values.yaml to load the custom classification.yml file for services <pre><code>...\nconfiguration:\n  ...\n  core:\n    ...\n    scaler:\n      service_defaults:\n        mounts:\n          - name: assemblyline-extra-config\n            path: \"/etc/assemblyline/classification.yml\"\n            resource_type: configmap\n            resource_name: assemblyline-extra-config\n            resource_key: classification\n...\n</code></pre> <p>Finally update your deployment using <code>helm upgrade command</code>:</p> ApplianceCluster <pre><code>sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al\n</code></pre> <pre><code>helm upgrade assemblyline &lt;assemblyline-helm-chart&gt;/assemblyline -f &lt;deployment_directory&gt;/values.yaml -n al\n</code></pre> <p>Important</p> <p>It takes a while for all the containers to be restarted so be patient and it will eventually show up in the UI.</p>"},{"location":"installation/deployment/","title":"Choose your deployment type","text":""},{"location":"installation/deployment/#choose-your-deployment-type","title":"Choose your deployment type","text":"<p>Assemblyline has two distinctive deployment types:</p> <p>Appliance: Single host deployment</p> <p>Cluster: Multi-host deployment</p> <p>Warning</p> <p>Keep in mind that you will need extra hosts for running external resources such as anti-virus products, or sandboxes (such as CAPE Sandbox). These complementary products are not mandatory but will greatly complement the static analysis and file extraction performed by Assemblyline.</p>"},{"location":"installation/deployment/#features","title":"Features","text":"<p>Both deployments are the same in terms of analysis capabilities however a cluster deployment can be scaled to scan multiple millions of files per day and offer redundancy and failover. If you are deploying in the cloud, a cluster will be easier to deploy (using cloud Kubernetes offerings). However, in a lab or to support an incident response team, any powerful computer with an appliance deployment will be able to process thousands of files a day.</p> <p>Tip</p> <p>If you choose to deploy a MicroK8s appliance, you will be able to scaled it up to a small cluster using multiple machines (nodes) if need be.</p>"},{"location":"installation/deployment/#deployment-feature-comparison","title":"Deployment Feature Comparison","text":"Appliance (Docker) Appliance (MicroK8s) Cluster Support all analysis capabilities  Yes  Yes  Yes Single host installation  Yes  Yes  No Easy step by step installation  Yes  Yes  No Simple to deploy and manage  Yes  No  No Multiple host installation  No  Optional  Yes Auto-scaling of core components  No  Optional  Yes High volume throughput  No  No  Yes Cloud provider support (AKS, EKS, GKE...)  No  No  Yes Redundancy / Failover support  No  No  Yes Internal TLS/SSL support  No  Yes  Yes"},{"location":"installation/deployment/#installation-stack","title":"Installation stack","text":"<p>Both clustered and MicroK8s deployments use a very similar stack in the background which allows them to share the same Helm chart. Only small changes in the values.yml file are required to differentiate them.</p> <p>As for the Docker compose appliance deployment, it uses a simpler stack which is easier to maintain and reset but offer less features to scale to high capacity.</p> <p></p>"},{"location":"installation/deployment/#installation-instructions","title":"Installation instructions","text":"<p>Now that you know the difference between the two types of deployment, you can refer to their respective installation instruction to get you started.</p> <ul> <li>Appliance installation (Docker)</li> <li>Appliance installation (Microk8s)</li> <li>Cluster installation</li> </ul> <p>Tip</p> <p>Consider reading the configuration section of the documentation before jumping into the installation instruction. This will help you understand all the different options you can modify during the installation process.</p>"},{"location":"installation/deployment/#our-current-biggest-deployment","title":"Our current biggest deployment","text":"<p>Kibana cluster overview dashboard of the number of files ingested, completed, skipped, duplicates, etc. on our biggest deployment:</p> <p></p> <ul> <li>Auto-scalable 12\u201372 nodes cluster (16 cores/64 GB RAM per node)</li> <li>Deployed in the cloud</li> <li>Receives up to 3.5M+ unique submissions a day (Avg. 1.5M)</li> <li>Plenty of downtime during the night</li> <li>Rarely uses the full node capacity</li> <li>Interacts with sandbox and antivirus systems outside of Assemblyline infrastructure</li> <li>A mix of file types, a mix of static and dynamic analysis</li> <li>Datastore consists of 12 x 1TB nodes</li> </ul> <p>Number of nodes required on a typical day for our biggest deployment:</p> <p></p>"},{"location":"installation/deployment/#sandbox-infrastructure-both-cuckoo-and-cape","title":"Sandbox infrastructure (both Cuckoo and CAPE)","text":"<p>Nest: 16 CPUs / 64 GB RAM, 200 GB storage</p> <p>Victims: 4 CPUs / 14 GB RAM, 64 GB storage</p>"},{"location":"installation/deployment/#antivirus-infrastructure","title":"Antivirus infrastructure","text":"<ul> <li>6 antivirus products</li> <li>~5 servers per product (~30 servers)</li> <li>8 CPUs / 16 GB RAM, 32 GB storage</li> </ul>"},{"location":"installation/deployment_migration/","title":"Appliance Migration","text":""},{"location":"installation/deployment_migration/#docker-compose-kubernetes-system-migration","title":"Docker Compose \u2192 Kubernetes System Migration","text":"<p>This guide assumes you're moving from a vanilla Docker-based deployment to a Kubernetes deployment.</p> <p>No processing on either system should be occurring during this migration process</p>"},{"location":"installation/deployment_migration/#docker-system","title":"Docker System","text":""},{"location":"installation/deployment_migration/#create-a-docker-service-for-backups","title":"Create a Docker service for backups","text":"<p>Create a <code>backup</code> container that has access to the datastore's API and the filestore volume.</p> <p>You can modify the <code>docker-compose.yaml</code> of your deployment type like the following:</p> <pre><code>  # Add a new service called 'backup' to facilitate backing up Assemblyline\n  backup:\n    build:\n      context: privilege-core-image\n      args:\n        version: ${AL_VERSION}\n        registry: ${REGISTRY}\n      cache_from:\n        - cccs/assemblyline-core:${AL_VERSION}\n    image: ${COMPOSE_PROJECT_NAME}_scaler:${AL_VERSION}\n    privileged: true\n    env_file: [\".env\"]\n    volumes:\n      - ${COMPOSE_ROOT}/config/config.yml:/etc/assemblyline/config.yml:ro\n      - backup:/mount/backup:rw       # Mount backup volume\n      - filestore:/data               # Mount filestore volume\n\n    networks: [core]\n    command: \"sleep 9999999999d\"      # Start up and keep running so we can shell into the container later\n\nvolumes:\n  ...\n  backup:                             # Add a backup volume\n</code></pre>"},{"location":"installation/deployment_migration/#backup-datastore-indices","title":"Backup Datastore indices","text":"<p>Shell into your <code>backup</code> container, open the AL CLI tool, and backup the datastore data to the mount.</p> System-onlyEverything <p>Info</p> <p>The following indices will be backed up:</p> <p><code>heuristic, service, service_delta, signature, user, user_avatar, user_favorites, user_settings, workflow</code></p> <pre><code>python -m assemblyline.run.cli\nbackup /mount/backup/al_system\nls /mount/backup/\n</code></pre> <p>All indices will be backed up</p> <pre><code>python -c \"from assemblyline.run.cli import ALCommandLineInterface\ncli = ALCommandLineInterface()\nfor index in cli.datastore.ds.get_models().keys():\n  # This will backup each index as a separate directory within the mounted backup folder\n  cli.do_backup(f'/mount/backup/al_{index} {index} force *:*')\n\"\n</code></pre>"},{"location":"installation/deployment_migration/#backup-filestore-directories","title":"Backup Filestore directories","text":"<p>Copy <code>al-cache</code> and <code>al-storage</code> to the backup volume</p> <pre><code>cp -r /data/ /mount/backup/filestore/\n</code></pre>"},{"location":"installation/deployment_migration/#kubernetes-system","title":"Kubernetes System","text":""},{"location":"installation/deployment_migration/#restoring-the-datastore","title":"Restoring the Datastore","text":""},{"location":"installation/deployment_migration/#mount-your-backup-using-corevolumes-and-coremounts","title":"Mount your backup using coreVolumes and coreMounts","text":"<p>Edit your <code>values.yaml</code> as follows and then perform a <code>helm upgrade</code>:</p> <pre><code>coreVolumes:\n  - name: assemblyline-backup\n    hostPath:\n      path: /var/lib/docker/volumes/al_backup/_data\n      type: Directory\ncoreMounts:\n  - name: assemblyline-backup\n    mountPath: /mount/backup\n</code></pre>"},{"location":"installation/deployment_migration/#restore-datastore-by-using-the-assemblyline-cli","title":"Restore Datastore by using the Assemblyline CLI","text":"<p>Shell into one of the core containers (ie. scaler) and perform the following:</p> System-onlyEverything <pre><code>python -m assemblyline.run.cli\nrestore /mount/backup/al_system\n</code></pre> <p>Not all systems are equal</p> <p>You will only need to restore with a subset of the folders listed below as your deployment isn't guaranteed to have data in every index (ie. alerts).</p> <p>Performing an <code>ls /mount/backup</code> will confirm what you have backed up.</p> <pre><code>python -c \"from assemblyline.run.cli import ALCommandLineInterface\nimport os\ncli = ALCommandLineInterface()\ndir = '/mount/backup'\nfor index in os.listdir(dir):\n  if os.path.isdir(f'{dir}/{index}'):\n    cli.do_restore(f'{dir}/{index}')\n\"\n</code></pre>"},{"location":"installation/deployment_migration/#migrating-the-filestore","title":"Migrating the Filestore","text":""},{"location":"installation/deployment_migration/#edit-filestore-deployment-in-kubernetes","title":"Edit Filestore Deployment in Kubernetes","text":"<p>Edit deployment and add the following hostpath mount:</p> <pre><code>spec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: filestore\n          ...\n          volumeMounts:\n            ...\n            - mountPath: /mount/backup\n              name: backup\n      ...\n      volumes:\n        ...\n        - hostPath:\n            path: /var/lib/docker/volumes/al_backup/_data\n            type: Directory\n          name: backup\n</code></pre>"},{"location":"installation/deployment_migration/#merge-filestore-data-with-existing-volume","title":"Merge Filestore data with existing volume","text":"<p>Shell into the filestore pod and copy the files from backup:</p> <pre><code>cp -r /mount/backup/filestore/* /export/\n</code></pre> <p>Once transfer is complete, you can remove the <code>volumeMount</code> and <code>volume</code> relating to the backup from the deployment.</p>"},{"location":"installation/deployment_migration/#migrate-system-configuration","title":"Migrate System Configuration","text":"<p>You can copy the contents of <code>config.yml</code> and place them in the <code>configuration</code> key of your <code>values.yaml</code></p> <p>Some configuration values are oriented towards the Docker Compose appliance, so change values where applicable.</p>"},{"location":"installation/deployment_migration/#extra-configurations","title":"Extra Configurations","text":"<p>If you want to use custom configurations to control tag safelisting or classification, you'll need to create a ConfigMap and modify your helm deployment to make use of them.</p> <p>Let's say I want to add the following custom configurations to Kubernetes:</p> classification.ymltag_safelist.yml <pre><code>enforce: true\n</code></pre> <pre><code>match:\n  network.dynamic.domain:\n    - localhost\n</code></pre> <p>You'll need to create a new file, say <code>objects.yaml</code>, with the following:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: assemblyline-extra-config\ndata:\n  tag_safelist: |\n    match:\n      network.dynamic.domain:\n        - localhost\n  classification: |\n    enforce: true\n</code></pre> <p>Followed by a <code>kubectl apply -f objects.yaml -n &lt;al_namespace&gt;</code></p> <p>Once the new ConfigMap has been created in your Kubernetes namespace, you can modify your <code>values.yaml</code> as follows:</p> <pre><code>coreEnv:\n  - name: CLASSIFICATION_CONFIGMAP\n    value: assemblyline-extra-config\n  - name: CLASSIFICATION_CONFIGMAP_KEY\n    value: classification\ncoreMounts:\n  - name: al-extra-config\n    mountPath: /etc/assemblyline/tag_whitelist.yml\n    subPath: tag_safelist\n    readOnly: true\n  - name: al-extra-config\n    mountPath: /etc/assemblyline/classification.yml\n    subPath: classification\n    readOnly: true\ncoreVolumes:\n  - name: al-extra-config\n    configMap:\n      name: assemblyline-extra-config\n</code></pre> <p>Followed by a <code>helm upgrade</code> to apply the configurations.</p>"},{"location":"installation/monitoring/","title":"Monitoring with ELK","text":""},{"location":"installation/monitoring/#monitoring-with-elk","title":"Monitoring with ELK","text":"<p>The Assemblyline helm chart gives you the option of pointing logs to an existing ELK stack or having Assemblyline create its own internal ELK for logging and metrics.</p> <p></p>"},{"location":"installation/monitoring/#elk-stack-configuration","title":"ELK Stack configuration","text":"<p>In the <code>values.yaml</code> file of your deployment, you can edit the following parameters to configure Assemblyline to send metrics and logs to a specific ELK stack.</p> <p>Choose the type of ELK stack deployment that corresponds the best to your setup:</p> Appliance Internal ELK stackCluster Internal ELK stackExternal ELK stack <p>Partial values.yaml config for an Appliance internal ELK stack</p> <pre><code>...\n# Have Assemblyline send logs to the configured ELK stack\nenableLogging: true\n\n# Have Assemblyline send metrics to the configured ELK stack\nenableMetrics: true\n\n# This would have Assemblyline send APM metrics to the\n#  configured ELK stack as well but it is very costly in\n#  terms of resources so only turn it on if you really\n#  need insight on API response time and core components\n#  operation timing.\nenableAPM: false\n\n# We are setting up an internal ELK stack so we can turn that on\ninternalELKStack: true\n\n# Because this is an appliance, we will reuse the same elastic\n#  database used for data to store logs as well\nseperateInternalELKStack: false\n\n# The internal ELK stack use elastic as its base username and\n#  does not verify TLS\nloggingUsername: elastic\nloggingTLSVerify: none\n...\n</code></pre> <p>Partial values.yaml config for a cluster internal ELK stack</p> <pre><code>...\n# Have Assemblyline send logs to the configured ELK stack\nenableLogging: true\n\n# Have Assemblyline send metrics to the configured ELK stack\nenableMetrics: true\n\n# This would have Assemblyline send APM metrics to the\n#  configured ELK stack as well but it is very costly in\n#  terms of resources so only turn it on if you really\n#  need insight on API response time and core components\n#  operation timing.\nenableAPM: false\n\n# We are setting up an internal ELK stack so we can turn that on\ninternalELKStack: true\n\n# Because this is a cluster, we will have Assemblyline spin up\n#  a completely different elastic database so the logging does not\n#  interfere with the performance of the data\nseperateInternalELKStack: true\n\n# The internal ELK stack use elastic as its base username and\n#  does not verify TLS\nloggingUsername: elastic\nloggingTLSVerify: none\n...\n</code></pre> <p>Partial values.yaml config for external ELK stack</p> <pre><code>...\n# Have Assemblyline send logs to the configured ELK stack\nenableLogging: true\n\n# Have Assemblyline send metrics to the configured ELK stack\nenableMetrics: true\n\n# This would have Assemblyline send APM metrics to the\n#  configured ELK stack as well but it is very costly in\n#  terms of resources so only turn it on if you really\n#  need insight on API response time and core components\n#  operation timing.\nenableAPM: false\n\n# We are setting up an external ELK stack so we will disable\n#  those settings\ninternalELKStack: false\nseperateInternalELKStack: false\n\n# -- EXTERNAL ELK Stack config --\n# Elastic host where the logs will be shipped to\nloggingHost: https://&lt;ELK_HOST&gt;:443/\n\n# Kibana dashboard location\nkibanaHost: https://&lt;ELK_HOST&gt;:443/kibana\n\n# Username that will be used to login to the elastic on your\n#  ELK stack\nloggingUsername: &lt;YOUR_ELK_USERNAME&gt;\n\n# Should you verify TLS on your ELK stack?\nloggingTLSVerify: \"full\"\n\n# Finally configure al_metrics to save metrics to your stack\nconfiguration:\n  core:\n    metrics:\n      elasticsearch:\n        hosts: [\"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@&lt;ELK_HOST&gt;:443\"]\n        # If you're using HTTPS and don't want certificate failures you can put\n        # your CA here\n        host_certificates: |\n          -----BEGIN CERTIFICATE-----\n          ...\n          -----END CERTIFICATE-----\n\n...\n</code></pre> <p>Finally update your deployment using <code>helm upgrade command</code>:</p> ApplianceCluster <pre><code>sudo microk8s helm upgrade assemblyline ~/git/assemblyline-helm-chart/assemblyline -f ~/git/deployment/values.yaml -n al\n</code></pre> <pre><code>helm upgrade assemblyline &lt;assemblyline-helm-chart&gt;/assemblyline -f &lt;deployment_directory&gt;/values.yaml -n al\n</code></pre>"},{"location":"installation/monitoring/#logstash-pipelines","title":"Logstash Pipelines","text":"<p>You can write custom pipelines to help enrich your data when passed through Logstash.</p> <p>You can set your custom Logstash pipeline under <code>customLogstashPipeline</code> in your <code>values.yaml</code> file of your deployment.</p> <p>Partial values.yaml to add a simple Logstash pipeline</p> <pre><code>...\n# Turn on Logstash support\nuseLogstash: true\ncustomLogstashPipeline: |\n  input {\n    beats {\n      port =&gt; 5044\n      codec =&gt; \"json\"\n    }\n  }\n\n  filter{\n    mutate { add_field =&gt; {\"sent_to_logstash\" =&gt; \"True\"} }\n  }\n\n  output {\n      elasticsearch{\n        hosts =&gt; \"http://elasticsearch:9200\"\n        index =&gt; \"assemblyline-logs\"\n        codec =&gt; \"json\"\n    }\n  }\n...\n</code></pre>"},{"location":"installation/monitoring/#kibana-dashboards","title":"Kibana Dashboards","text":"<p>Within Kibana, there is the ability to use dashboards to visualize your data into one consolidated view to make it easier for monitoring, like a hub.</p> <p>You can get our latest exported dashboards directly from the assemblyline-base source and then use Kibana import features to use them in your ELK Stack.</p>"},{"location":"installation/monitoring/#creation","title":"Creation","text":"<p>Dashboards are made up of visualizations, and these can come in different forms: graphs, metrics, gauges, tables, maps, etc.</p> <p>Each visualization requires an index pattern to get the data from and setting a date range, this throws all relevant data within the specified timeframe into a bucket to be used by the visualization.</p> <p>Dashboards can also be imported/exported for use across different ELKs but require dependencies like index patterns for them to function out of the box, otherwise requires editing the dashboard file.</p>"},{"location":"installation/monitoring/#navigation","title":"Navigation","text":"<p>All dashboards give you the ability to filter your data, like what you will find under the Discover tab of Kibana. This will allow you to filter a certain dashboard based on a query you give.</p> <p>If you want more info about using Kibana's filtering and navigation feature, check the explore your data documentation.</p>"},{"location":"installation/replay/","title":"Syncing Instances (Replay)","text":""},{"location":"installation/replay/#syncing-instances-replay","title":"Syncing Instances (Replay)","text":"<p>Replay is a new feature of Assemblyline that allows you to bridge the gap between online and offline deployments of Assemblyline. You will be able to start a scan on your online deployment, bundle the results, ship them to your offline enclave and continue the scans in the offline enclave.</p>"},{"location":"installation/replay/#how-it-works","title":"How it works","text":"<p>Replay has two main components:</p> <ul> <li>Replay Creator</li> <li>Replay Loader</li> </ul> <p>The <code>Replay Creator</code> component will monitor either the alerts, the submissions or both and bundle them once they are completed including all their results, errors, files, etc. Those bundles will have to be transfered with the method of your choice to your offline enclave where the <code>Replay Loader</code> component will monitor a directory loading the bundles into your offline instance. Once these bundles are transferred onto your target system, the loader will be able to tell your offline instance to resume the scan and rescan the files with your offline services if you tell it to.</p> <p></p>"},{"location":"installation/replay/#replay-configuration-file","title":"Replay configuration file","text":"<p>Replay has its own configuration file that is seperate from the usual <code>config.yml</code> file found in an Assemblyline instance because it can run in an <code>API</code> mode which does not require a full Assemblyline config. In all cases, Replay will load its configuration file from <code>/etc/assemblyline/replay.yml</code> by default or from the file path specified in the <code>REPLAY_CONFIG_PATH</code> environment variable.</p> <p>If you are using our helm chart to deploy Replay you will be able to simply put the content of your <code>replay.yml</code> file in the <code>replay:</code> section of the <code>values.yml</code> file but if you are using Replay in container or simple script mode you will have to create your own Replay config file and mount it at the right place. The examples below will show you how to do this.</p> <p>In the meantime, here's a fully descriptive example of the Replay config file.</p> <p>Default replay.yml file</p> <pre><code>####################################\n# Replay Creator configuration block\n####################################\ncreator:\n  # -------------------------------\n  # Alert input configuration block\n  # -------------------------------\n  alert_input:\n    # Is creator monitoring alerts for changes?\n    enabled: true\n    # Which queries are performed to tell if an alert is ready for Replay?\n    #  DEFAULT:\n    #     - The alert is not performing an extended scan\n    #     - All workflows on this system have executed on the alert\n    filter_queries:\n      - NOT extended_scan:submitted\n      - workflows_completed:true\n    # Number of concurrent worker threads the Replay workers will use to\n    # process alerts\n    threads: 6\n\n  # ------------------------------------\n  # Submission input configuration block\n  # ------------------------------------\n  submission_input:\n    # Is creator monitoring submissions for changes?\n    enabled: true\n    # Which queries are performed to tell if a submission is ready for Replay?\n    #  DEFAULT:\n    #     - There was a manual request for the submission to be replayed in\n    #       the metadata\n    filter_queries:\n      - metadata.replay:requested\n    # Number of concurrent worker threads the Replay workers will use to\n    # process submissions\n    threads: 6\n\n  # ------------------------------------\n  # Replay client configuration block (How Replay will connect to AL)\n  # ------------------------------------\n  client:\n    # Type of connection (direct or api )\n    type: direct\n    # Options for the connection (only applies to api mode)\n    options:\n      # API Key that will be used to connect\n      #  NOTE: creator needs READ privilege only\n      apikey: devkey:devpass\n      # Url to your Assemblyline server\n      host: https://localhost:443\n      # User that will connect via the API\n      user: admin\n      # Should the SSL connection be verified ?\n      verify: true\n\n  # Amount of time to lookback when checking for new alert/submission\n  # NOTE: This is a datemath lucene value (examples: *, now-1d, now-1h, ...)\n  lookback_time: '*'\n  # Assemblyline filestore object where the bundle files will be created\n  #  NOTE: This is similar to the urls found in the filestore configuration of the\n  #        config.yml file. Supported types are: S3, Azure Blob, FTP, local and sftp\n  output_filestore: file:///tmp/replay/output\n  # Working directory for the creator and creator_worker processes\n  working_directory: /tmp/replay/work\n\n\n####################################\n# Replay Loader configuration block\n####################################\nloader:\n  # ------------------------------------\n  # Replay client configuration block (How Replay will connect to AL)\n  # ------------------------------------\n  client:\n    # Type of connection (direct or api )\n    type: direct\n    # Options for the connection (only applies to api mode)\n    options:\n      # API Key that will be used to connect\n      #  NOTE: loader needs WRITE privilege only\n      apikey: devkey:devpass\n      # Url to your Assemblyline server\n      host: https://localhost:443\n      # User that will connect via the API\n      user: admin\n      # Should the SSL connection be verified ?\n      verify: true\n\n  # Directory where bundles that failed to load will be copied to\n  failed_directory: /tmp/replay/failed\n  # Directory from which we will monitor new files and load them in\n  # the target instance. This will probably be an external directory\n  # like an NFS mount...\n  input_directory: /tmp/replay/input\n  # Number of concurrent worker threads the loader worker will use\n  # to load the bundle in the target instance\n  input_threads: 6\n  # Minimum classification that will be applied to each loaded bundle\n  # regardless of their original classification\n  min_classification: null\n  # List of services that you which to rescan after the bundle is done importing\n  # NOTE: If for example you have a different set of Yara rules on the source and\n  #       target systems you could set that value to ['YARA']\n  rescan: []\n  # Working directory for the loader and loader_worker processes\n  working_directory: /tmp/replay/work\n</code></pre>"},{"location":"installation/replay/#helm-deployment","title":"Helm Deployment","text":"<p>There are multiple ways to configure Replay so unfortunately there is no one size fits all configuration. The easiest setup by far is to use the different flags that we have pre-configured in our helm chart. Make sure you pull our latest helm chart to be able to benefit of this easy setup.</p> <p>The helm chart deployment of Replay will automatically scale the worker components if there are too many bundles to create or load.</p>"},{"location":"installation/replay/#configure-the-source-instance","title":"Configure the source instance","text":"<p>The source instance is the one that will run the Replay Creator components. These components will have to write the alert and submission bundle somewhere. Conveniently, Replay uses the filestore feature of Assemblyline so you can use any type of storage that it supports to save your files.</p> <ul> <li>S3</li> <li>Azure blob</li> <li>FTP</li> <li>SFTP</li> </ul> <p>When you know where your files are going to be saved to, edit the <code>values.yml</code> file of your helm deployment and add the following lines:</p> <p>Partial values.yaml config for Replay creator setup</p> <pre><code>...\n\n# Turn on Replay\nuseReplay: true\n\n# Set Replay mode to creator\nreplayMode: \"creator\"\n\n# Set the output folder for the creator worker\nreplay:\n  creator:\n    output_filestore: &lt;AN ASSEMBLYLINE FILESTORE LINK - SEE REPLAY CONFIGURATION FILE&gt;\n\n...\n</code></pre> <p>When this is done you can just perform an Helm upgrade to apply the changes to your instance.</p>"},{"location":"installation/replay/#configure-the-target-instance","title":"Configure the target instance","text":"<p>The target instance is the one that will run the Replay Loader components. These components will have to have direct access to a directory where the files transfered over are dropped. This limits what can be use so <code>NFS</code> is our recommended way for now.</p> <p>Edit the <code>values.yml</code> file of your helm deployment and add the following lines:</p> <p>Partial values.yaml config for Replay loader setup</p> <pre><code>...\n\n# Turn on Replay\nuseReplay: true\n\n# Set Replay mode to loader\nreplayMode: \"loader\"\n\n# Make sure input folder is accessible\nreplayLivenessCommand: \"ls /tmp/replay/input\"\n\n# Load input folder\nreplayLoaderVolume:\n  - name: replay-data\n    nfs:\n      server: localhost\n      path: /path/to/folder\n\n...\n</code></pre> <p>When this is done you can just perform an Helm upgrade to apply the changes to your instance.</p>"},{"location":"installation/replay/#alternate-deployment","title":"Alternate Deployment","text":"<p>If you are not using our helm chart to deploy Assemblyline, perhaps you are using a Docker Compose deployment, there are alternatives on how to run the different Replay components</p> Virtual Env using APIDocker container using APIDocker container direct DB Access <p>If you are using the API mode to launch Replay, you do not have to launch Replay in a container and can just use a normal python process in a virtual environment if you choose so.</p> <p>Important</p> <ul> <li>You will need python 3.9+ to run the Assemblyline code, if you don't have it, use the container mode.</li> <li>The user using the Replay creator needs an API Key with read access where as the user using the Replay loader needs an API Key with write access.</li> <li>Because you are launching Replay directly as a process the replay.yml configuration has to be in <code>/etc/assemblyline/</code> folder.</li> </ul> <p>First of all, create your virtual environment and install the pre-requisite packages: </p><pre><code>python3.9 -m venv ~/replay_venv\n~/replay_venv/bin/pip3.9 install assemblyline assemblyline-core assemblyline_client\n</code></pre><p></p> <p>Then you can create your <code>/etc/assemblyline/replay.yml</code> configuration file and tell Replay to use the API mode:</p> <p>Partial replay.yml configuration</p> <pre><code>creator:\n  client:\n    type: api\n    options:\n      host: \"https://&lt;YOUR SOURCE SERVER DOMAIN/IP&gt;:443\",\n      apikey: \"&lt;PASTE YOUR SOURCE API KEY&gt;\"\n      user: &lt;YOUR SOURCE USERNAME&gt;\n      verify: false\nloader:\n  client:\n    type: api\n    options:\n      host: \"https://&lt;YOUR SOURCE SERVER DOMAIN/IP&gt;:443\",\n      apikey: \"&lt;PASTE YOUR SOURCE API KEY&gt;\"\n      user: &lt;YOUR SOURCE USERNAME&gt;\n      verify: false\n</code></pre> <p>Finally, just run whichever components are relevant depending on the Assemblyline instance you are on:</p> Source Assemblyline instanceTarget Assemblyline instance <p>Run the creator: </p><pre><code>~/replay_venv/bin/python3.9 -m assemblyline_core.replay.creator.run\n</code></pre> And its worker: <pre><code>~/replay_venv/bin/python3.9 -m assemblyline_core.replay.creator.run_worker\n</code></pre><p></p> <p>Run the loader: </p><pre><code>~/replay_venv/bin/python3.9 -m assemblyline_core.replay.loader.run\n</code></pre> And its worker: <pre><code>~/replay_venv/bin/python3.9 -m assemblyline_core.replay.loader.run_worker\n</code></pre><p></p> <p>If you are using the API mode to launch Replay, you will not be able to use the already existing Assemblyline containers to use Replay, you will have to build your own.</p> <p>Important</p> <ul> <li>The user using the Replay creator needs an API Key with read access where as the user using the Replay loader needs an API Key with write access.</li> <li>We will assume the configuration files for Replay are in a <code>replay_config</code> folder in you home directory.</li> </ul> <p>With the following Dockerfile in <code>~/replay_config/Dockerfile</code>:</p> <pre><code>FROM cccs/assemblyline-core:stable\n\npip install --no-cache-dir --user assemblyline_client\n</code></pre> <p>Build your Replay container:</p> <pre><code>(cd ~/replay_config/ &amp;&amp; docker build --no-cache -t cccs/assemblyline-replay .)\n</code></pre> <p>Then you can create your <code>~/replay_config/replay.yml</code> configuration file and tell Replay to use the API mode:</p> <p>Partial replay.yml configuration</p> <pre><code>creator:\n  client:\n    type: api\n    options:\n      host: \"https://&lt;YOUR SOURCE SERVER DOMAIN/IP&gt;:443\",\n      apikey: \"&lt;PASTE YOUR SOURCE API KEY&gt;\"\n      user: &lt;YOUR SOURCE USERNAME&gt;\n      verify: false\nloader:\n  client:\n    type: api\n    options:\n      host: \"https://&lt;YOUR SOURCE SERVER DOMAIN/IP&gt;:443\",\n      apikey: \"&lt;PASTE YOUR SOURCE API KEY&gt;\"\n      user: &lt;YOUR SOURCE USERNAME&gt;\n      verify: false\n</code></pre> <p>Finally, just run which ever components are relevant depending on the Assemblyline instance you are on:</p> Source Assemblyline instanceTarget Assemblyline instance <p>Run the creator: </p><pre><code>docker run --rm \\\n--name assemblyline-replay-creator \\\n-v ~/replay_config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.creator.run\n</code></pre> And its worker: <pre><code>docker run --rm \\\n--name assemblyline-replay-creator-worker \\\n-v ~/replay_config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.creator.run_worker\n</code></pre><p></p> <p>Run the loader: </p><pre><code>docker run --rm \\\n--name assemblyline-replay-loader \\\n-v ~/replay_config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.loader.run\n</code></pre> And its worker: <pre><code>docker run --rm \\\n--name assemblyline-replay-loader-worker \\\n-v ~/replay_config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.loader.run_worker\n</code></pre><p></p> <p>This scenario is mainly going to be used if you have a Docker Compose appliance of Assemblyline so you can just run a couple extra containers to get Replay going. Since you have direct access to Assemblyline's Database and queues you do not have to create a special container and can just use the core container.</p> <p>Important</p> <p>We will assume that all you are running an appliance out of you home folder (<code>~/appliance/</code>). You should adjust the paths correctly to fit your installation.</p> <p>First of all, you will need to create your <code>replay.yml</code> file in your <code>~/appliance/config</code> directory:</p> <p>Partial replay.yml configuration</p> <pre><code>creator:\n  client:\n    type: direct\nloader:\n  client:\n    type: direct\n</code></pre> <p>Then you can just run which ever components are relevant depending on the Assemblyline instance you are on:</p> Source Assemblyline instanceTarget Assemblyline instance <p>Run the creator: </p><pre><code>docker run --rm \\\n--name assemblyline-replay-creator \\\n-v ~/appliance/config/classification.yml:/etc/assemblyline/classification.yml:ro \\\n-v ~/appliance/config/config.yml:/etc/assemblyline/config.yml:ro \\\n-v ~/appliance/config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.creator.run\n</code></pre> And its worker: <pre><code>docker run --rm \\\n--name assemblyline-replay-creator-worker \\\n-v ~/appliance/config/classification.yml:/etc/assemblyline/classification.yml:ro \\\n-v ~/appliance/config/config.yml:/etc/assemblyline/config.yml:ro \\\n-v ~/appliance/config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.creator.run_worker\n</code></pre><p></p> <p>Run the loader: </p><pre><code>docker run --rm \\\n--name assemblyline-replay-loader \\\n-v ~/appliance/config/classification.yml:/etc/assemblyline/classification.yml:ro \\\n-v ~/appliance/config/config.yml:/etc/assemblyline/config.yml:ro \\\n-v ~/appliance/config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.loader.run\n</code></pre> And its worker: <pre><code>docker run --rm \\\n--name assemblyline-replay-loader-worker \\\n-v ~/appliance/config/classification.yml:/etc/assemblyline/classification.yml:ro \\\n-v ~/appliance/config/config.yml:/etc/assemblyline/config.yml:ro \\\n-v ~/appliance/config/replay.yml:/etc/assemblyline/replay.yml:ro \\\n-v /tmp/replay/files:/tmp/replay/output \\\n-v /tmp/replay/creator:/tmp/replay/work \\\ncccs/assemblyline-core:stable \\\npython -m assemblyline_core.replay.loader.run_worker\n</code></pre><p></p>"},{"location":"installation/appliance/docker/","title":"Docker","text":""},{"location":"installation/appliance/docker/#appliance-in-docker","title":"Appliance in Docker","text":"<p>This is the documentation for an appliance instance of the Assemblyline platform suited for very small single machine deployment.</p>"},{"location":"installation/appliance/docker/#setup-requirements","title":"Setup requirements","text":"<p>Caveat</p> <p>The documentation provided here assumes that you are installing your appliance on one of the following systems:</p> <ul> <li>Debian: Ubuntu 22.04, Ubuntu 24.04</li> <li>RHEL: RHEL 8.5</li> </ul> <p>You might have to change the commands a bit if you use other Linux distributions.</p> <p>The recommended minimum system requirement for this appliance is 4 CPUs and 8 GB of RAM.</p> <p>Note: If you have above the minimum system requirement, you can make performance adjustments such as setting <code>core.scaler.service_defaults.min_instances: 1</code> to ensure service readiness by default rather than on-demand scaling.</p>"},{"location":"installation/appliance/docker/#install-pre-requisites","title":"Install pre-requisites","text":"UbuntuRHEL 8.5 <ol> <li> <p>Follow the install guide provided by the official Docker documentation.</p> </li> <li> <p>Ensure the installation was successful by invoking the commands:     </p><pre><code>docker version\ndocker compose version\n</code></pre><p></p> </li> </ol> <p>Warning</p> <p>Many of the instructions below have been set to force yes and allow erasing for quick implementation.</p> <p>It is recommended that these flags be removed for production environments to avoid impacting production environments by missing key messages and warnings. Step 4 contains a firewall configuration, we strongly advise firewall settings should be managed and reviewed by your organization before deployment.</p> <ol> <li> <p>Follow the install guide provided by the official Docker documentation.</p> </li> <li> <p>Ensure the installation was successful by invoking the commands:     </p><pre><code>docker version\ndocker compose version\n</code></pre><p></p> </li> <li> <p>Upgrade Python3.9: </p><pre><code>dnf install -y python39\nalternatives --set python3 /usr/bin/python3.9\npython3 --version\n</code></pre><p></p> </li> <li> <p>Configure firewalld for Docker: </p><pre><code>sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/' /etc/firewalld/firewalld.conf\nfirewall-cmd --reload\nreboot\n</code></pre><p></p> </li> </ol>"},{"location":"installation/appliance/docker/#configure-docker-to-use-larger-address-pools","title":"Configure Docker to use larger address pools","text":"<ol> <li> <p>Create/Edit <code>/etc/docker/daemon.json</code> and add the following lines:</p> <pre><code>{\n  \"default-address-pools\":\n  [\n    {\"base\":\"10.201.0.0/16\",\"size\":24}\n  ]\n}\n</code></pre> <pre><code>echo '{\"default-address-pools\":[{\"base\":\"10.201.0.0/16\",\"size\":24}]}' | jq '.' | sudo tee /etc/docker/daemon.json\n</code></pre> </li> <li> <p>Restart Docker to acknowledge configuration: <code>service docker restart</code></p> </li> </ol>"},{"location":"installation/appliance/docker/#setup-your-assemblyline-appliance","title":"Setup your Assemblyline appliance","text":""},{"location":"installation/appliance/docker/#download-the-assemblyline-docker-compose-repository","title":"Download the Assemblyline Docker Compose repository","text":"<pre><code>git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.git ~/deployments/assemblyline\ncd ~/deployments/assemblyline\n</code></pre>"},{"location":"installation/appliance/docker/#setup-your-appliance","title":"Setup your appliance","text":"<p>The <code>config/config.yml</code> file in your deployment directory is already pre-configured for use with <code>docker compose</code> as a single node appliance. You can review the settings already configured but you should not have anything to change there.</p> <p>The <code>.env</code> file in your deployment directory is preconfigured with default passwords, you should definitely change them.</p>"},{"location":"installation/appliance/docker/#assign-profiles-depending-on-your-deployment-requirements","title":"Assign profiles depending on your deployment requirements","text":"<p>These following service profiles can be combined (unless otherwise specified) depending on your deployment requirements:</p> <ul> <li><code>minimal</code>: This setup includes the bare-minimum components for everything to be able to run. There will be no metrics collected and you will have to tail the log from the docker container logs.</li> <li><code>full</code>: This setup includes every single components and all metrics and logging capabilities. Metrics and logs will be gathered inside the same Elasticsearch instance as the processing data and you will have to access Kibana to view all of those.</li> <li><code>archive</code>: This deploys the Archiver component of Assemblyline but this requires <code>datastore.archive.enabled: true</code> in your <code>config.yml</code> otherwise the container will terminate.</li> </ul> <p>Note: The <code>minimal</code> and <code>full</code> profiles are mutually exclusive and are not to be used together.</p> <p>You can specify which profiles to use on the commandline using the <code>--profile</code> flag or set <code>COMPOSE_PROFILES</code> in your <code>.env</code> file as a comma separated list (default: <code>COMPOSE_PROFILES=minimal</code>)</p>"},{"location":"installation/appliance/docker/#deploy-assemblyline","title":"Deploy Assemblyline","text":"<p>Info</p> <p>The following instructions assume <code>.env</code> contains <code>COMPOSE_PROFILES</code>, otherwise the <code>--profile</code> flag can be used to override what's set in <code>.env</code></p>"},{"location":"installation/appliance/docker/#create-your-https-certs","title":"Create your https certs","text":"<pre><code>openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \"/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\"\n</code></pre>"},{"location":"installation/appliance/docker/#pull-necessary-docker-containers","title":"Pull necessary Docker containers","text":"<pre><code>cd ~/deployments/assemblyline\nsudo docker compose pull --ignore-buildable\nsudo env COMPOSE_BAKE=true docker compose build\nsudo docker compose -f bootstrap-compose.yaml pull\n</code></pre>"},{"location":"installation/appliance/docker/#finally-deploy-your-appliance","title":"Finally deploy your appliance","text":"<pre><code>cd ~/deployments/assemblyline\nsudo docker compose up -d --wait\nsudo docker compose -f bootstrap-compose.yaml up\n</code></pre> <p>Tip</p> <p>A helpful command for watching the containers as they start and enter a healthy state is <code>watch -n 1 docker ps</code>.</p> <p>Info</p> <p>Once the <code>docker compose</code> command on the bootstrap file complete, your cluster will be ready to use and you can login with the default admin user/password that you've set in your <code>.env</code> file</p>"},{"location":"installation/appliance/docker/#service-specific-configurations","title":"Service specific configurations","text":"<p>Certain services need special configurations to run efficiently in a Docker Compose appliance setup. Refer to the service management documentation to configure services through service parameters and service variables.</p> <ul> <li>URLDownloader: The <code>no_sandbox</code> option defaults to <code>False</code>, but will cause a <code>TimeoutExpired</code> in the internal google-chrome. Change this value to <code>True</code> if you encounter this issue.</li> </ul>"},{"location":"installation/appliance/docker/#docker-compose-cheat-sheet","title":"Docker Compose cheat sheet","text":""},{"location":"installation/appliance/docker/#updating-your-appliance","title":"Updating your appliance","text":"<pre><code>cd ~/deployments/assemblyline\nsudo docker compose pull --ignore-buildable\nsudo docker compose build\nsudo docker compose up -d\n</code></pre>"},{"location":"installation/appliance/docker/#changing-assemblyline-configuration-file","title":"Changing Assemblyline configuration file","text":"<p>Edit the <code>cd ~/deployments/assemblyline/config/config.yml</code> then:</p> <pre><code>cd ~/deployments/assemblyline\nsudo docker compose restart\n</code></pre>"},{"location":"installation/appliance/docker/#check-core-services-logs","title":"Check core services logs","text":"<p>For core components:</p> <pre><code>cd ~/deployments/assemblyline\nsudo docker compose logs\n</code></pre> <p>Or for a specific component:</p> <pre><code>cd ~/deployments/assemblyline\nsudo docker compose logs ui\n</code></pre>"},{"location":"installation/appliance/docker/#take-down-your-appliance","title":"Take down your appliance","text":"<p>Tip</p> <p>This will remove all containers related to your appliance but will not remove the volumes so you can bring it back up safely.</p> <pre><code>cd ~/deployments/assemblyline\nsudo docker compose stop\nsudo docker rm --force $(sudo docker ps -a --filter label=app=assemblyline -q)\nsudo docker compose down --remove-orphans\n</code></pre>"},{"location":"installation/appliance/docker/#bring-your-appliance-back-online","title":"Bring your appliance back online","text":"<pre><code>cd ~/deployments/assemblyline\nsudo docker compose up -d\n</code></pre>"},{"location":"installation/appliance/kubernetes-microk8s/","title":"MicroK8s","text":""},{"location":"installation/appliance/kubernetes-microk8s/#appliance-in-kubernetes","title":"Appliance in Kubernetes","text":"<p>Instructions below are tailored for an appliance setup using MicroK8s.</p> <p>This is the documentation for an appliance instance of the Assemblyline platform suited for smaller-scale deployments.</p>"},{"location":"installation/appliance/kubernetes-microk8s/#setup-requirements","title":"Setup requirements","text":"<p>Caveat</p> <p>The documentation provided here assumes that you are installing your appliance on an Ubuntu-based system and was only tested on Ubuntu 20.04, 22.04, and 24.04. You might have to change the commands a bit if you use other Linux distributions.</p> <p>The recommended minimum system requirement for this appliance is 6 CPU and 12 GB of RAM.</p> OnlineOffline <ol> <li>Install microk8s: <pre><code>sudo snap install microk8s --classic\n</code></pre></li> <li>Install microk8s addons: <pre><code>for addon in \"ingress\" \"hostpath-storage\" \"metrics-server\"; do\n    sudo microk8s enable \"$addon\"\ndone\n</code></pre></li> <li>Install Helm and set it up to use with microk8s: <pre><code>sudo snap install helm --classic\nsudo mkdir /var/snap/microk8s/current/bin\nsudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm\n</code></pre></li> </ol> <ol> <li> <p>Download offline packages (On an internet-connected system):</p> <p>Create a directory to house everything to be transferred </p><pre><code>mkdir al_deps\ncd al_deps\n</code></pre> Download offline snap packages <pre><code>for snap_pkg in \"microk8s\" \"helm\" \"kubectl\"\ndo\n    sudo snap download $snap_pkg\ndone\n</code></pre> Fetch helm charts <pre><code># Clone the Assemblyline helm chart repo\ngit clone https://github.com/CybercentreCanada/assemblyline-helm-chart\n\n# Download dependency helm charts\nhelm dependency update assemblyline-helm-chart/assemblyline/\n</code></pre> Fetch  container images. Be sure to check you have the correct image tags as specified microk8s-community-addons<p></p> All-in-OneStep-by-Step <pre><code># Calico CNI\nfor container_image in \"cni\" \"pod2daemon-flexvol\" \"node\"\ndo\n    docker pull calico/$container_image:v3.19.1 &amp;&amp; docker save calico/$container_image:v3.19.1 &gt;&gt; calico_$container_image.tar\ndone\ndocker pull calico/kube-controllers:v3.17.3 &amp;&amp; docker save calico/kube-controllers:v3.17.3 &gt;&gt; calico_kube-controllers.tar\n\n# microk8s add-ons, refer to images used in corresponding .yaml files (https://github.com/ubuntu/microk8s/tree/master/microk8s-resources/actions)\nexport ARCH=amd64\n\n# DNS\nfor container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\"\ndo\n    docker pull gcr.io/google_containers/$container_image-$ARCH:1.14.7 &amp;&amp; docker save gcr.io/google_containers/$container_image-$ARCH:1.14.7 &gt;&gt; $container_image.tar\ndone\n\n#coreDNS, storage, metrics-server, ingress, registry\ndocker pull k8s.gcr.io/pause:3.1 &amp;&amp; docker save k8s.gcr.io/pause:3.1 &gt;&gt; pause.tar\ndocker pull coredns/coredns:1.8.0 &amp;&amp; docker save coredns/coredns:1.8.0 &gt;&gt; coredns.tar\ndocker pull cdkbot/hostpath-provisioner-$ARCH:1.0.0 &amp;&amp; docker save cdkbot/hostpath-provisioner-$ARCH:1.0.0 &gt;&gt; storage.tar\ndocker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &amp;&amp; docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &gt;&gt; metrics.tar\ndocker pull k8s.gcr.io/ingress-nginx/controller:v1.1.0 &amp;&amp; docker save k8s.gcr.io/ingress-nginx/controller:v1.1.0 &gt;&gt; ingress.tar\ndocker pull registry:2.7.1 &amp;&amp; docker save registry:2.7.1 &gt;&gt; registry.tar\n\n\n# Assemblyline Core (release: 4.7.stable)\nfor al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\" \"rust\"\ndo\n    docker pull cccs/assemblyline-$al_image:4.7.stable &amp;&amp; docker save cccs/assemblyline-$al_image:4.7.stable &gt;&gt; al_$al_image.tar\ndone\n\n# Elastic\nES_REG=docker.elastic.co\nES_VER=7.17.3\nfor beat in \"filebeat\" \"metricbeat\"\ndo\n    docker pull $ES_REG/beats/$beat:$ES_VER &amp;&amp; docker save $ES_REG/beats/$beat:$ES_VER &gt;&gt; es_$beat.tar\ndone\nfor es in \"logstash\" \"kibana\" \"elasticsearch\"\ndo\n    docker pull $ES_REG/$es/$es:$ES_VER &amp;&amp; docker save $ES_REG/$es/$es:$ES_VER &gt;&gt; es_$es.tar\ndone\n\n# Filestore image (MinIO)\nfor minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\"\ndo\n    docker pull minio/$minio_image &amp;&amp; docker save minio/$minio_image &gt;&gt; minio_$minio_image.tar\ndone\n\n# Redis image\ndocker pull redis &amp;&amp; docker save redis &gt;&gt; redis.tar\n\n#Pull official service images\nfor svc_image in apivector apkaye batchdeobfuscator capa cape characterize configextractor deobfuscripter elf elfparser emlparser espresso extract floss frankenstrings iparse metapeek oletools pdfid peepdf pe pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara\ndo\n    docker pull cccs/assemblyline-service-$svc_image:stable &amp;&amp; docker save cccs/assemblyline-service-$svc_image:stable &gt;&gt; $svc_image.tar\ndone\n</code></pre> AssemblylineAL Helm Chart DependenciesCalico CNIMicroK8s addons <pre><code># Core\nfor al_image in \"core\" \"ui\" \"ui-frontend\" \"service-server\" \"socketio\"\ndo\n    docker pull cccs/assemblyline-$al_image:4.2.stable &amp;&amp; docker save cccs/assemblyline-$al_image:4.2.stable &gt;&gt; al_$al_image.tar\ndone\n\n# Services\nfor svc_image in apivector apkaye batchdeobfuscator capa cape characterize configextractor deobfuscripter elf elfparser emlparser espresso extract floss frankenstrings iparse metapeek oletools pdfid peepdf pe pixaxe safelist sigma suricata swiffer tagcheck torrentslicer unpacme unpacker vipermonkey virustotal-dynamic virustotal-static xlmmacrodeobfuscator yara\ndo\n    docker pull cccs/assemblyline-service-$svc_image:stable &amp;&amp; docker save cccs/assemblyline-service-$svc_image:stable &gt;&gt; $svc_image.tar\ndone\n</code></pre> ElasticMinIORedis <pre><code>ES_REG=docker.elastic.co\nES_VER=7.16.2\nfor beat in \"filebeat\" \"metricbeat\"\ndo\n    docker pull $ES_REG/beats/$beat:$ES_VER &amp;&amp; docker save $ES_REG/beats/$beat:$ES_VER &gt;&gt; es_$beat.tar\ndone\nfor es in \"logstash\" \"kibana\" \"elasticsearch\"\ndo\n    docker pull $ES_REG/$es/$es:$ES_VER &amp;&amp; docker save $ES_REG/$es/$es:$ES_VER &gt;&gt; es_$es.tar\ndone\n</code></pre> <pre><code>for minio_image in \"minio:RELEASE.2021-02-14T04-01-33Z\" \"mc:RELEASE.2021-02-14T04-28-06Z\"\ndo\n    docker pull minio/$minio_image &amp;&amp; docker save minio/$minio_image &gt;&gt; minio_$minio_image.tar\ndone\n</code></pre> <pre><code>docker pull redis &amp;&amp; docker save redis &gt;&gt; redis.tar\n</code></pre> <pre><code>for container_image in \"cni\" \"pod2daemon-flexvol\" \"node\"\ndo\n    docker pull calico/$container_image:v3.19.1 &amp;&amp; docker save calico/$container_image:v3.19.1 &gt;&gt; calico_$container_image.tar\ndone\ndocker pull calico/kube-controllers:v3.17.3 &amp;&amp; docker save calico/kube-controllers:v3.17.3 &gt;&gt; calico_kube-controllers.tar\n</code></pre> <p>Refer to: microk8s-community-addons </p><pre><code># Kubernetes DNS\nfor container_image in \"k8s-dns-kube-dns\" \"k8s-dns-dnsmasq-nanny\" \"k8s-dns-sidecar\"\ndo\n    docker pull gcr.io/google_containers/$container_image-$ARCH:1.14.7 &amp;&amp; docker save gcr.io/google_containers/$container_image-$ARCH:1.14.7 &gt;&gt; $container_image.tar\ndone\n\n#coreDNS, storage, metrics-server, ingress, registry\ndocker pull k8s.gcr.io/pause:3.1 &amp;&amp; docker save k8s.gcr.io/pause:3.1 &gt;&gt; pause.tar\ndocker pull coredns/coredns:1.8.0 &amp;&amp; docker save coredns/coredns:1.8.0 &gt;&gt; coredns.tar\ndocker pull cdkbot/hostpath-provisioner-$ARCH:1.0.0 &amp;&amp; docker save cdkbot/hostpath-provisioner-$ARCH:1.0.0 &gt;&gt; storage.tar\ndocker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &amp;&amp; docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &gt;&gt; metrics.tar\ndocker pull k8s.gcr.io/ingress-nginx/controller:v1.1.0 &amp;&amp; docker save k8s.gcr.io/ingress-nginx/controller:v1.1.0 &gt;&gt; ingress.tar\ndocker pull registry:2.7.1 &amp;&amp; docker save registry:2.7.1 &gt;&gt; registry.tar\n</code></pre><p></p> </li> <li> <p>Setup master node</p> <ol> <li> <p>Install microk8s     </p><pre><code>sudo snap ack microk8s_*.assert\nsudo snap install microk8s_*.snap --classic\n</code></pre><p></p> </li> <li> <p>Setup registry addon (Optional)     </p><pre><code># Add registry image to microk8s\nmicrok8s.ctr image pull registry.tar\nmicrok8s addon registry\n</code></pre><p></p> </li> <li> <p>Load images from disk and push to registry     </p><pre><code>REGISTRY=localhost:32000\n# Re-tag images and push to local registry\nfor image in $(docker image ls --format {{.Repository}}:{{.Tag}})\ndo\n    image_tag=$image\n    if [ $(grep -o '/' &lt;&lt;&lt; $image | wc -l) -eq 2 ]\n    then\n        image_tag=$(cut -d '/' -f 2- &lt;&lt;&lt; $image)\n    fi\n    sudo docker tag $image $REGISTRY/$image_tag &amp;&amp; docker push $REGISTRY/$image_tag\n    sudo docker image rm $image\n    sudo docker image rm $REGISTRY/$image_tag\ndone\n</code></pre><p></p> </li> <li> <p>Modify Container Registry Endpoints     </p><pre><code>sudo vim /var/snap/microk8s/current/args/containerd-template.toml\n</code></pre>     Replace REGISTRY with the domain/port of your container registry (ie. localhost:32000)     <pre><code>    [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\n    [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"]\n        endpoint = [\"https://registry-1.docker.io\", \"http://&lt;REGISTRY&gt;\", ]\n    [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"*\"]\n        endpoint = [\"http://&lt;REGISTRY&gt;\"]\n</code></pre>     Restart containerd to acknowledge the changes     <pre><code>sudo systemctl restart containerd\n</code></pre><p></p> </li> <li>Enable microk8s add-ons     <pre><code>sudo microk8s reset &amp;&amp; sudo microk8s enable ingress dns ha-cluster storage metrics-server\n</code></pre></li> <li> <p>Fetch kubeconfig for administration     </p><pre><code>cp /var/snap/microk8s/current/credentials/client.config &lt;remote_destination&gt;\n</code></pre><p></p> </li> <li> <p>Install admin tools (separate or same host(s) for computing):     </p><pre><code>sudo snap ack helm_*.assert\nsudo snap install helm_*.snap --classic\n\nsudo snap ack kubectl_*.assert\nsudo snap install kubectl_*.snap --classic\n\n# Copy kubeconfig from cluster and make it accessible for kubectl/helm\nexport KUBECONFIG=$HOME/.kube/config\n\n# If installing on computing hosts\n# sudo mkdir /var/snap/microk8s/current/bin\n# sudo ln -s /snap/bin/helm /var/snap/microk8s/current/bin/helm\n# sudo ln -s /snap/bin/kubectl /var/snap/microk8s/current/bin/kubectl\n\n# (Optional) Install additional Kubernetes monitoring tools like k9s or FreeLens\n</code></pre><p></p> </li> </ol> </li> </ol> Adding more nodes (optional) <p>Note: This can be done before or after the system is live.</p> <p>Install required addon: </p><pre><code>sudo microk8s enable ha-cluster\n</code></pre><p></p> <p>From the master node, run: </p><pre><code>sudo microk8s add-node\n</code></pre><p></p> <p>This will generate a command with a token to be executed on a standby node.</p> <p>On your standby node, ensure the microk8s <code>ha-cluster</code> addon is enabled before running the command from the master to join the cluster.</p> <p>To verify the nodes are connected, run (on any node): </p><pre><code>sudo microk8s kubectl get nodes\n</code></pre><p></p> <p>Repeat this process for any additional standby nodes that are to be added.</p> <p>For more details, see: Clustering with MicroK8s</p>"},{"location":"installation/appliance/kubernetes-microk8s/#configuring-your-deployment","title":"Configuring your deployment","text":"<p>We will assume you are creating a directory named \"deployment\" where all referenced files are stored.</p> <pre><code>mkdir deployment; cd deployment\nwget https://raw.githubusercontent.com/CybercentreCanada/assemblyline-helm-chart/refs/heads/main/appliance/values.yaml\nwget https://raw.githubusercontent.com/CybercentreCanada/assemblyline-helm-chart/refs/heads/main/appliance/secrets.yaml\n</code></pre> <p>The <code>values.yaml</code> file in your deployment directory is already pre-configured for use with microk8s as a basic one node minimal appliance. Make sure you go through the file to adjust disk sizes and to turn on/off features to your liking. If you wish to login to assemblyline from a bare public IP address, the configuration still requires a FQDN. As mentioned in <code>values.yaml</code>, <code>nip.io</code> can provide an easy FQDN for a bare IP.</p> <pre><code>sed -i \"s/fqdn: \\\"localhost\\\"/fqdn: \\\"$(curl -s https:\\/\\/api.ipify.org).nip.io\\\"/\" ~/git/deployment/values.yaml\n</code></pre> <p>The <code>secret.yaml</code> file in your deployment directory is preconfigured with default passwords, you should change them.</p> <p>Tip</p> <p>The secrets are used to set up during bootstrap so make sure you change them before deploying the <code>al</code> chart.</p> <p>Warning</p> <p>Be sure to update any values as deemed necessary ie. FQDN</p>"},{"location":"installation/appliance/kubernetes-microk8s/#deploy-assemblyline-via-helm","title":"Deploy Assemblyline via Helm","text":""},{"location":"installation/appliance/kubernetes-microk8s/#add-the-assemblyline-chart-repository-to-helm","title":"Add the assemblyline chart repository to helm","text":"<pre><code>helm repo add assemblyline https://cybercentrecanada.github.io/assemblyline-helm-chart/\n</code></pre> <p>You may need to update the chart repositories to see the latest versions.</p> <pre><code>helm repo update\n</code></pre>"},{"location":"installation/appliance/kubernetes-microk8s/#create-a-namespace-for-your-assemblyline-install","title":"Create a namespace for your Assemblyline install","text":"<p>For this documentation, we will use <code>al</code> as the namespace.</p> <pre><code>sudo microk8s kubectl create namespace al\n</code></pre>"},{"location":"installation/appliance/kubernetes-microk8s/#deploy-the-secret-to-the-namespace","title":"Deploy the secret to the namespace","text":"<pre><code>sudo microk8s kubectl apply -f secrets.yaml --namespace=al\n</code></pre> <p>From this point on, you don't need the secrets.yaml file anymore. You should delete it so there is no file on disk containing your passwords.</p> <pre><code>rm secrets.yaml\n</code></pre>"},{"location":"installation/appliance/kubernetes-microk8s/#finally-lets-deploy-assemblylines-chart","title":"Finally, let's deploy Assemblyline's chart","text":"<p>For documentation, we will use <code>assemblyline</code> as the deployment name.</p> <pre><code>sudo microk8s helm install assemblyline assemblyline/assemblyline -f values.yaml -n al\n</code></pre> <p>Tip</p> <p>A helpful command for watching the containers as they become ready and running is <code>watch -n 1 microk8s kubectl get pods -n al</code>.</p> <p>Warning</p> <p>After you've ran the <code>helm install</code> command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.</p>"},{"location":"installation/appliance/kubernetes-microk8s/#updating-the-current-deployment","title":"Updating the current deployment","text":"<p>For details on updating your depyloment via helm see the update instructions in the kubernetes cluster documentation.</p>"},{"location":"installation/appliance/kubernetes-microk8s/#quality-of-life-improvements","title":"Quality of life improvements","text":""},{"location":"installation/appliance/kubernetes-microk8s/#cluster-management-tools","title":"Cluster Management Tools","text":""},{"location":"installation/appliance/kubernetes-microk8s/#sudoless-access-to-microk8s","title":"Sudoless access to MicroK8s","text":"<p>MicroK8s require you to add sudo in front of every command, you can add your user to the microk8s group so you don't have to.</p> <pre><code>sudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\n</code></pre> <p>You will need to reboot for these changes to take effect</p> <p>Temporarily, you can add the group to your current shell by running the following: </p><pre><code>newgrp microk8s\n</code></pre><p></p>"},{"location":"installation/appliance/kubernetes-microk8s/#alias-to-kubectl","title":"Alias to Kubectl","text":"<p>Since all is running inside microk8s you can create an alias to the kubectl command to make your life easier</p> <pre><code>sudo snap alias microk8s.kubectl kubectl\nkubectl config set-context --current --namespace=al\n</code></pre>"},{"location":"installation/appliance/kubernetes-microk8s/#alternative-installations","title":"Alternative Installations","text":"<p>We will officially only support microk8s installations for appliances, but you can technically install it on any local Kubernetes frameworks (k3s, minikube, kind...). That said there will be no documentation for these setups, and you will have to modify the <code>values.yaml</code> storage classes to fit with your desired framework.</p>"},{"location":"installation/cluster/general/","title":"Kubernetes","text":""},{"location":"installation/cluster/general/#general","title":"General","text":""},{"location":"installation/cluster/general/#pre-requisites","title":"Pre-requisites","text":"<ol> <li>A Kubernetes 1.18+ cluster that has an ingress controller. Assemblyline is known to work with the following Kubernetes providers:<ul> <li>Rancher</li> <li>AKS (Azure)</li> <li>EKS (Amazon)</li> <li>GKE (Google)</li> </ul> </li> <li>kubectl already configured for your cluster on your machine</li> <li>helm already configured for your cluster on your machine</li> </ol>"},{"location":"installation/cluster/general/#installation","title":"Installation","text":""},{"location":"installation/cluster/general/#1-get-assemblyline-helm-chart-ready","title":"1. Get Assemblyline Helm chart ready","text":"<ol> <li>Add the assemblyline chart repository to helm.    <pre><code>helm repo add assemblyline https://cybercentrecanada.github.io/assemblyline-helm-chart/\n</code></pre></li> <li>Make sure the chart repositories are up to date.    <pre><code>helm repo update\n</code></pre></li> <li>Create a new directory of your choice which will hold your personal deployment configuration. We will refer to it as <code>deployment_directory</code></li> </ol>"},{"location":"installation/cluster/general/#2-create-the-assemblyline-namespace","title":"2. Create the assemblyline namespace","text":"<p>When deploying an Assemblyline instance using our chart, it must be in its own namespace. For this documentation, we will use the <code>al</code> namespace.</p> <pre><code>kubectl create namespace al\n</code></pre>"},{"location":"installation/cluster/general/#3-setup-secrets","title":"3. Setup secrets","text":"<p>In the <code>deployment_directory</code> you've just created, create a <code>secrets.yaml</code> file which will contain the different passwords used by Assemblyline.</p> The secrets.yaml file should have the following format <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: assemblyline-system-passwords\ntype: Opaque\nstringData:\n  datastore-password:\n  logging-password:\n    # If this is the password for backends like azure blob storage, the password may need to be URL-encoded\n    # if it includes non-alphanumeric characters\n  filestore-password:\n  initial-admin-password:\n---\n# Initalizes secret with a temporary value, will be replaced by job upon helm install\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kibana-service-token\nstringData:\n  token: \"\"\n</code></pre> <p>Tip</p> <p>Here is an example of secrets.yaml file used for appliance deployments.</p> <p>When you're done setting the different passwords in your <code>secrets.yaml</code> file, upload it to your namespace:</p> <pre><code>kubectl apply -f &lt;deployment_directory&gt;/secrets.yaml --namespace=al\n</code></pre> <p>Warning</p> <p>From this point on, you will not need the <code>secret.yaml</code> file anymore. You should delete it.</p>"},{"location":"installation/cluster/general/#4-configure-your-deployment","title":"4. Configure your deployment","text":"<p>In your <code>deployment_directory</code>, create a <code>values.yaml</code> file which will contain the configuration specific to your deployment.</p> <p>Tip</p> <p>For an exhaustive view of all the possible parameters you can change the <code>values.yaml</code> you've created, refer to the assemblyline-helm-chart/assemblyline/values.yaml file.</p> <p>These are the strict minimum configuration changes you will need to do:</p> <ol> <li>Setup the ingress controller by changing the values of:<ul> <li><code>ingressAnnotations.cert-manager.io/issuer:</code> (Name of the issuer in K8s. This is for cert validation)</li> <li><code>tlsSecretName</code> (Name of the TLS cert in k8s. This is for cert validation)</li> <li><code>configuration.ui.fqdn</code> (Domain name for your al instance).</li> </ul> </li> <li>Setup the storage classes according to your Kubernetes cluster :<ul> <li><code>redisStorageClass</code> (Use SSD backed managed disks)</li> <li><code>log-storage.volumeClaimTemplate.storageClassName</code> (Use SSD backed managed disks)</li> <li><code>datastore.volumeClaimTemplate.storageClassName</code> (Use SSD backed managed disks)</li> <li><code>persistentStorageClass</code> (Use standard file sharing disks)</li> </ul> </li> <li>Decide where you want files stored, set the appropriate URI in the <code>configuration.filestore.*</code> fields. You should try to avoid using the internal filestore and use something like Azure blob store, Amazon S3...</li> <li>Enable/disable/configure logging features, (disabled by default).</li> </ol> This is an example values.yaml file to get you started <pre><code># 1. Setup the ingress controller\ningressAnnotations:\n  kubernetes.io/ingress.class: \"nginx\"\n  nginx.ingress.kubernetes.io/proxy-body-size: 100M\n  cert-manager.io/issuer: &lt;CHANGE_ME&gt;\ntlsSecretName: &lt;CHANGE_ME&gt;\n\n# 2. Setup the storage classes according to your Kubernetes cluster\nredisStorageClass: &lt;CHANGE_ME&gt;\ndatastore:\n  volumeClaimTemplate:\n    storageClassName: &lt;CHANGE_ME&gt;\nlog-storage:\n  volumeClaimTemplate:\n    storageClassName: &lt;CHANGE_ME&gt;\npersistantStorageClass: &lt;CHANGE_ME&gt;\n\n\n# 3. Decide where you want files stored\ninternalFilestore: false\n# Un-comment and setup if internal filestore used\n#filestore:\n#  persistence:\n#    size: 500Gi\n#    StorageClass: &lt;CHANGE_ME&gt;\n\n\n# 4. Enable/disable/configure logging features\nenableLogging: false\nenableMetrics: false\nenableAPM: false\ninternalELKStack: false\nseperateInternalELKStack: false\nloggingUsername: elastic\nloggingTLSVerify: \"none\"\n\n\n# Internal configuration for assemblyline components. See the assemblyline\n# administration documentation for more details.\n# https://cybercentrecanada.github.io/assemblyline4_docs/configuration/config_file/\nconfiguration:\n  # 1. Setup the ingress controller\n  submission:\n    max_file_size: 104857600\n  ui:\n    fqdn: \"localhost\"\n\n  # 3. Decide where you want files stored\n  filestore:\n    cache: [\"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&amp;use_ssl=False\"]\n    storage: [\"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&amp;use_ssl=False\"]\n\n  # 4. Enable/disable/configure logging features\n  logging:\n    log_level: WARNING\n</code></pre>"},{"location":"installation/cluster/general/#5-deploy-your-current-configuration","title":"5. Deploy your current configuration","text":"<p>Now that you've fully configured your <code>values.yaml</code> file, you can simply deploy it via helm by referencing the default assemblyline helm chart.</p> <pre><code>helm install assemblyline assemblyline/assemblyline -f &lt;deployment_directory&gt;/values.yaml -n al\n</code></pre> <p>Warning</p> <p>After you've ran the <code>helm install</code> command, the system has a lot of setting up to do (Creating database indexes, loading service, setting up default accounts, loading signatures ...). Don't expect it to be fully operational for at least the next 15 minutes.</p>"},{"location":"installation/cluster/general/#6-optional-cluster-management-tools","title":"6. (Optional) Cluster management tools","text":"<p>You can manage your deployment in Kubernetes using <code>kubectl</code> but it's typically laborious to type the commands to monitor or debug your instance. For that reason, we have a few tools that we recommend using.</p>"},{"location":"installation/cluster/general/#k9s-recommended","title":"k9s [Recommended]","text":"<p>You can install the k9s CLI using a package manager or installing from source</p>"},{"location":"installation/cluster/general/#freelens-ide","title":"FreeLens IDE","text":"<p>If the computer on which your microk8s deployment is installed has a desktop interface, we strongly suggest that you use an IDE like FreeLens to manage the system</p>"},{"location":"installation/cluster/general/#install-freelens","title":"Install FreeLens","text":"<p>You'll have to fetch the appropriate installation file from FreeLens releases and use your package manager to install manually:</p> <pre><code># Ubuntu\nsudo snap install -y /path/to/FreeLens*.deb\n</code></pre> <p>If monitoring from a Windows system, Microsoft's SmartScreen will detect the file as being unrecognized and block execution. This can be resolved by checking 'Unblock' in the file's properties.</p>"},{"location":"installation/cluster/general/#configure-freelens","title":"Configure FreeLens","text":"<p>After you run FreeLens for the first time, click the \"Add cluster\" menu/button, select the paste as text tab and paste the output of the following command:</p> <pre><code>sudo kubectl config view --raw\n</code></pre>"},{"location":"installation/cluster/general/#update-your-deployment","title":"Update your deployment","text":""},{"location":"installation/cluster/general/#updating-your-configuration","title":"Updating your configuration","text":"<p>If you wish to apply changes to your <code>values.yaml</code> file without changing the version of assemblyline installed you need to run the <code>helm upgrade</code> command with the current release version set.</p> <p>You can see your current release version by running <code>helm list</code>.</p> <p></p><pre><code>helm list\n</code></pre> Which produces an output similar to this. <pre><code>NAME          NAMESPACE  REVISION  UPDATED                                 STATUS    CHART                    APP VERSION\nassemblyline  al         2727      2026-01-29 11:00:08.07031363 -0500 EST  deployed  assemblyline-7.0.79-dev  4.7.0.dev79\n</code></pre><p></p> <p>The version we want is the one in the CHART column following the chart name. For this sample output that is 7.0.79-dev.</p> <p>Warning</p> <p>Not the APP VERSION column.</p> <p>The corresponding upgrade command would then be: </p><pre><code>helm upgrade assemblyline assemblyline/assemblyline -f values.yaml -n al --version 7.0.79-dev\n</code></pre> <pre><code>helm upgrade &lt;name of deployment&gt; &lt;chart name&gt; -f &lt;values file to apply&gt; -n &lt;namespace&gt; --version &lt;version to pin&gt;\n</code></pre><p></p>"},{"location":"installation/cluster/general/#applying-minor-updates","title":"Applying minor updates","text":"<p>To apply a minor update run the upgrade command with the corresponding chart version for the update you want applied. The chart version is the same as the assemblyline version with the leading <code>4.</code> and <code>stable</code> or <code>dev</code> markers removed. So Assemblyline release <code>4.7.2.stable2</code> would be chart version <code>7.2.2</code>. Development chart versions will have a <code>-dev</code> suffix added after all the version numbers so Assemblyline release <code>4.7.2.dev2</code> would be installed with chart version <code>7.2.2-dev</code></p> <p>Make sure any lines setting the <code>release</code> value have been removed from your <code>values.yaml</code> file.</p> <p>If you simply want to upgrade to the most recent safe release for assemblyline get your chart version as above and run the upgrade command with the major version pinned. For the sample system above the upgrade command would be:</p> <pre><code>helm upgrade assemblyline assemblyline/assemblyline -f values.yaml -n al --version \"^7\"\n</code></pre> <p>For the latest release of Assemblyline 4.7.XX.</p>"},{"location":"installation/cluster/general/#applying-major-updates","title":"Applying major updates","text":"<p>See the documentation section on that topic.</p>"},{"location":"installation/cluster/microk8s/","title":"MicroK8s","text":""},{"location":"installation/cluster/microk8s/#microk8s","title":"MicroK8s","text":"<p>This is the documentation for a cluster instance of the Assemblyline platform suited for larger deployments (minimum 3 compute nodes).</p> <p>Take the following into consideration while following the relevant  instructions from the Kubernetes appliance.</p> <p>The changes proposed alter the storage class used in a multi-node cluster setup using OpenEBS instead of the default hostpath-based storage class.</p>"},{"location":"installation/cluster/microk8s/#setup-requirements","title":"Setup Requirements","text":"OnlineOffline <pre><code>sudo microk8s enable openebs\n</code></pre> <ol> <li> <p>Pull the OpenEBS dependencies (internet-connected system) </p><pre><code>#OpenEBS\nREPO=\"openebs\"\nREPO_K8S=\"k8s.gcr.io/sig-storage\"\n\n# Pull extra OpenEBS images\nfor openebs_image in \"jiva:3.1.0\" \"m-exporter:3.1.0\" \"jiva-operator:3.1.0\" \"jiva-csi:3.1.0\"\ndo\n  docker pull $REPO/$openebs_image &amp;&amp; docker save $REPO/$openebs_image &gt;&gt; $REPO_$openebs_image.tar\ndone\n\n# Pull jiva addon images\nfor k8s_image in \"csi-attacher:v3.1.0\" \"livenessprobe:v2.3.0\" \"csi-provisioner:v3.0.0\" \"csi-resizer:v1.2.0\" \"csi-node-driver-registrar:v2.3.0\"\ndo\n  docker pull $REPO_K8S/$k8s_image &amp;&amp; docker save $REPO_K8S/$k8s_image &gt;&gt; $k8s_image.tar\ndone\n\n# Pull cleaner Bitnami image\ndocker pull bitnami/kubectl &amp;&amp; docker save bitnami/kubectl &gt;&gt; bitnami_kubectl.tar\n\n# Download package and dependencies of iscsi, needed for OpenEBS\napt-get -y install --print-uris open-iscsi | cut -d\\' -f2 | grep http:// &gt; open-iscsi.txt\nwget -i ./open-iscsi.txt\n</code></pre><p></p> </li> <li> <p>Install debian packages for open-iscsi </p><pre><code>dpkg -i *.deb\nsystemctl enable --now iscsid\n</code></pre><p></p> </li> <li> <p>Install OpenEBS manually via helm on the master node in MicroK8s </p><pre><code># Install OpenEBS helm chart\nCOMMON_PATH=\"/var/snap/microk8s/common/\"\nmicrok8s helm install openebs ./openebs-*.tgz --namespace openebs --create-namespace \\\n  --version 3.0.x \\\n  --set jiva.enabled=true \\\n  --set legacy.enabled=false \\\n  --set jiva.csiNode.kubeletDir=\"$COMMON_PATH/var/lib/kubelet/\" \\\n  --set localprovisioner.basePath=\"$COMMON_PATH/var/openebs/local\" \\\n  --set ndm.sparse.path=\"$COMMON_PATH/var/openebs/sparse\" \\\n  --set varDirectoryPath.baseDir=\"$COMMON_PATH/var/openebs\"\n</code></pre><p></p> </li> </ol> Adding more nodes <p>Note: This can be done before or after the system is live.</p> <p>Install required addon: </p><pre><code>sudo microk8s enable ha-cluster\n</code></pre><p></p> <p>From the master node, run: </p><pre><code>sudo microk8s add-node\n</code></pre><p></p> <p>This will generate a command with a token to be executed on a standby node.</p> <p>On your standby node, ensure the microk8s <code>ha-cluster</code> addon is enabled before running the command from the master to join the cluster.</p> <p>To verify the nodes are connected, run (on any node): </p><pre><code>sudo microk8s kubectl get nodes\n</code></pre><p></p> <p>Repeat this process for any additional standby nodes that are to be added.</p> <p>For more details, see: Clustering with MicroK8s</p>"},{"location":"installation/cluster/microk8s/#relevant-changes-to-helm-chart","title":"Relevant Changes to Helm Chart","text":"<p>The main difference between the cluster and the appliance in MicroK8s is the use of the openEBS storage class for an Elasticsearch cluster.</p> <p>The following changes will have to made to the helm chart.</p> <pre><code>redisStorageClass: openebs-hostpath\npersistantStorageClass: openebs-hostpath\n...\n\ndatastore:\n  replicas:  3\n  volumeClaimTemplate:\n    storageClassName: openebs-jiva-csi-default\n...\nfilestore:\n  persistence:\n    StorageClass: openebs-hostpath\n</code></pre>"},{"location":"installation/cluster/optimizations/","title":"High-Throughput Optimizations","text":""},{"location":"installation/cluster/optimizations/#high-throughput-optimizations","title":"High-Throughput Optimizations","text":"<p>Enough games. It's time to get serious and prepare your Assemblyline deployment to scan multiple millions of files.</p> <p>The following details the configurations required to deploy our current biggest production environment, which is the biggest Assemblyline 4 deployment that we are aware of. We also will discuss the rationale behind these decisions.</p>"},{"location":"installation/cluster/optimizations/#nodes","title":"Nodes","text":"<ul> <li>Don't use nodes that are too small because Elastic/Redis can use a lot of resources</li> <li>Minimum: 8 cores / 32 GB</li> <li>What we use: 16 cores / 64 GB</li> <li>The minimum amount of nodes required by your cluster is the number of Elastic pods that you have</li> <li>We have 12 Elastic pods so our deployment auto-scales from 12 nodes to 72 nodes</li> </ul>"},{"location":"installation/cluster/optimizations/#ingestion","title":"Ingestion","text":"<ul> <li>For high-volume ingestion, do not use <code>/api/v4/submit/</code></li> <li>Use this instead: <code>/api/v4/ingest/</code>. This API is tailored for rate-limiting if Assemblyline can't keep up. This API will queue submissions for processing later if there is a backlog.</li> <li>If ingestion slows down the UI, the ingestion rate is too high</li> <li>Set <code>separateIngestAPI:true</code> in your <code>values.yml</code> file to spin up dedicated pods for ingestion</li> </ul>"},{"location":"installation/cluster/optimizations/#file-storage","title":"File Storage","text":"<ul> <li>Do not use the provided minio container for file storage</li> <li>It's not that minio is not good, but rather we just haven't spent any effort making the HELM chart deploy minio correctly</li> <li>Use either Azure blob storage if you are on AKS or Amazon S3 if you are on AWS</li> <li>Deploy your own minio with redundancy or any other well-supported S3-compatible file storage</li> <li>Don't put your file storage secrets in your <code>values.yml</code> file. Use Kubernetes secrets instead.</li> </ul> <p>Example:</p> <pre><code>internalFilestore: false\nconfiguration:\n  filestore:\n    storage:\n      - \"azure://&lt;blob_store_name&gt;.blob.core.windows.net/storage?access_key=${FILESTORE_PASSWORD}\"\n    cache:\n      - \"azure://&lt;blob_store_name&gt;.blob.core.windows.net/cache?access_key=${FILESTORE_PASSWORD}\"\n</code></pre>"},{"location":"installation/cluster/optimizations/#redis","title":"Redis","text":"<ul> <li>All messaging passed to services and Dispatcher/Ingester-shared memory space is stored in Redis</li> <li>Redis is our only component that cannot be scaled</li> <li>You should tweak RAM / CPU / thread requirements to fit your need</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>redisVolatileIOThreads: 5\nredisVolatileReqCPU: 4\nredisVolatileLimCPU: 4\nredisVolatileReqRam: 4Gi\n\nredisPersistentIOThreads: 3\nredisPersistentReqCPU: 2\nredisPersistentLimCPU: 2\nredisPersistentReqRam: 8Gi\nredisPersistentLimRam: 32Gi\n</code></pre>"},{"location":"installation/cluster/optimizations/#dispatcher","title":"Dispatcher","text":"<ul> <li>You can change the number of threads the Dispatcher uses</li> <li>Make sure Dispatcher is reserved a full core and has enough RAM</li> <li>NOTE: It's a Python process so don't give it more than a core</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>disptacherShutdownGrace: 1800\ndispatcherResultThreads: 8\ndispatcherFinalizeThreads: 8\ndispatcherReqCPU: 1\ndispatcherLimCPU: 1\ndispatcherReqRam: 2Gi\ndispatcherLimRam: 4Gi\n</code></pre>"},{"location":"installation/cluster/optimizations/#expiry","title":"Expiry","text":"<ul> <li>With big data input comes big data deletion</li> <li>We give Expiry more cores and more workers to be able to expire all that data</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>expiryReqCPU: 2\nexpiryLimCPU: 4\nconfiguration:\n  core:\n    expiry:\n      workers: 50\n      delete_workers: 5\n</code></pre>"},{"location":"installation/cluster/optimizations/#scaling","title":"Scaling","text":"<ul> <li>Use <code>cpu_overallocation</code> to make sure the cloud node auto-scaler works. Use a value between 1.05 to 1.10 (105% to 110%).</li> <li><code>overallocation_node_limit</code> will determine your maximum amount of nodes</li> <li><code>min_instances</code> determines the minimum number of service pods loaded. We use 2 so our reaction time is faster but that costs more money.</li> <li><code>cpu_reservation</code> is the percentage of the required maximum CPU for a service that will be reserved by Kubernetes. The higher the value, the less time the services fight for CPU time as their CPU usage is reserved, but that comes at the price of a higher cost!</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>configuration:\n  core:\n    scaler:\n      cpu_overallocation: 1.05\n      overallocation_node_limit: 72\n      service_defaults:\n        min_instances: 2\n  services:\n    cpu_reservation: 0.7\n</code></pre>"},{"location":"installation/cluster/optimizations/#auto-scalers","title":"Auto-Scalers","text":"<ul> <li>The Scaler component is dedicated to managing services</li> <li>To make sure you have enough core components to handle the service load you can adjust the max number of components in the <code>values.yml</code> files.</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>dispatcherInstancesMax: 25\ningestAPIInstancesMax: 50\nserviceServerInstancesMax: 50\ndispatcherTargetUsage: 40\n</code></pre>"},{"location":"installation/cluster/optimizations/#datastore","title":"Datastore","text":"<ul> <li>Since you'll have more data you'll need more Elastic pods (replicas)</li> <li>To make the most out of those pods they will need more CPU. Match the request/limit of CPU so Elastic does not fight with services for CPU time.</li> <li>The size of the indices will be larger, Elastic will need more RAM to process the queries</li> <li>To take advantage of distributed computing, since Elastic has more nodes, it will need more shards so each node gets busy enough</li> <li>If you've deployed your cluster before adjusting the shard, you'll have to use the <code>fix_shards</code> CLI command to edit the shard count on affected indices</li> <li>Our biggest production system has 4.7TB of indices with 1.8 Billion documents</li> <li>We use the following values in <code>values.yml</code>:</li> </ul> <pre><code>elasticEmptyResultShards: 16\nelasticFileShards: 16\nelasticResultShards: 36\nelasticSubmissionShards: 24\ndatastore:\n  replicas: 12\n  resources:\n    requests:\n      cpu: 4\n      memory: 12Gi\n    limits:\n      cpu: 4\n      memory: 20Gi\n</code></pre>"},{"location":"installation/configuration/authentication/","title":"Authentication section","text":""},{"location":"installation/configuration/authentication/#authentication-section","title":"Authentication section","text":"<p>Assemblyline comes with a built-in user management database, so no external identity sources are required. However, to facilitate user management in larger organizations you can integrate Assemblyline with external identity providers.</p> <p>The authentication section (<code>auth:</code>) of the configuration files contains all the different parameters that you can change to turn on/off the different authentication features that Assemblyline supports.</p> Default values for the authentication section <pre><code>...\nauth:\n  allow_2fa: true\n  allow_apikeys: true\n  allow_extended_apikeys: true\n  allow_security_tokens: true\n  internal:\n    enabled: true\n    failure_ttl: 60\n    max_failures: 5\n    password_requirements:\n      lower: false\n      min_length: 12\n      number: false\n      special: false\n      upper: false\n    signup:\n      enabled: false\n      notify:\n        activated_template: null\n        api_key: null\n        authorization_template: null\n        base_url: null\n        password_reset_template: null\n        registration_template: null\n      smtp:\n        from_adr: null\n        host: null\n        password: null\n        port: 587\n        tls: true\n        user: null\n      valid_email_patterns:\n      - .*\n      - .*@localhost\n  ldap:\n    auto_create: true\n    auto_sync: true\n    base: ou=people,dc=assemblyline,dc=local\n    bind_pass: null\n    bind_user: null\n    email_field: mail\n    enabled: false\n    group_lookup_query: (&amp;(objectClass=Group)(member=%s))\n    image_field: jpegPhoto\n    image_format: jpeg\n    name_field: cn\n    uid_field: uid\n    uri: ldap://localhost:389\n  oauth:\n    enabled: false\n    gravatar_enabled: true\n    providers:\n      auth0:\n        access_token_url: https://{TENANT}.auth0.com/oauth/token\n        api_base_url: https://{TENANT}.auth0.com/\n        authorize_url: https://{TENANT}.auth0.com/authorize\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://{TENANT}.auth0.com/.well-known/jwks.json\n        user_get: userinfo\n      azure_ad:\n        access_token_url: https://login.microsoftonline.com/common/oauth2/token\n        api_base_url: https://login.microsoft.com/common/\n        authorize_url: https://login.microsoftonline.com/common/oauth2/authorize\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://login.microsoftonline.com/common/discovery/v2.0/keys\n        user_get: openid/userinfo\n      google:\n        access_token_url: https://oauth2.googleapis.com/token\n        api_base_url: https://openidconnect.googleapis.com/\n        authorize_url: https://accounts.google.com/o/oauth2/v2/auth\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://www.googleapis.com/oauth2/v3/certs\n        user_get: v1/userinfo\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/authentication/#parameter-definitions","title":"Parameter definitions","text":"<p>The <code>auth</code> configuration block has a few parameters at the top level that help you turn on or off a few security features supported in the system.</p> <p>Here is an example of a configuration block with those top-level parameters and an explanation of what they do:</p> Top-level parameters <pre><code>auth:\n    # Turns on/off two-factor authentication in the system\n    allow_2fa: true\n\n    # Turn on/off usage of API Keys in the system\n    #  NOTE: if you turn this off, this will severely limit API access\n    allow_apikeys: true\n\n    # Turn on/off usage of extended API via the API keys\n    allow_extended_apikeys: true\n\n    # Turn on/off usage of security token as two-factor authentication (ex: yubikeys)\n    allow_security_tokens: true\n</code></pre>"},{"location":"installation/configuration/authentication/#internal-authenticator","title":"Internal authenticator","text":"<p>The configuration block at <code>auth.internal</code> allows you to configure the Assemblyline internal authenticator.</p> <p>Here is an example of a configuration block with inline comments about the purpose of every single parameter:</p> Internal auth configuration example <pre><code>auth:\n    internal:\n        # Enable or disable the internal authenticator\n        enabled: true\n\n        # Time in seconds the user will have to wait after\n        # too many authentication failures\n        failure_ttl: 60\n\n        # Number of authentication failures before temporarily\n        # locking down the user\n        max_failures: 5\n\n        # Password complexity requirements for the system\n        password_requirements:\n        # Are lowercase characters mandatory?\n        lower: false\n\n        # What is the minimal password length\n        min_length: 12\n\n        # Are numbers mandatory?\n        number: false\n\n        # Are special characters mandatory?\n        special: false\n\n        # Are uppercase characters mandatory?\n        upper: false\n\n        signup:\n        # Can a user automatically signup for the system\n        enabled: false\n\n        # Configuration block for GC Notify signup and password reset\n        # see: https://notification.canada.ca/\n        notify:\n            activated_template: null\n            api_key: null\n            authorization_template: null\n            base_url: null\n            password_reset_template: null\n            registration_template: null\n\n        # Configuration block for SMTP signup and password reset\n        smtp:\n            # Email address used for sender\n            from_adr: null\n\n            # Host of the SMTP server\n            host: null\n\n            # Password for the SMTP server\n            password: null\n\n            # Port of the SMTP server\n            port: 587\n\n            # Should we communicate with SMTP server via TLS?\n            tls: true\n\n            # User to authenticate to the SMTP server\n            user: null\n\n        # Email patterns that will be allowed to\n        #  automatically signup for an account\n        valid_email_patterns:\n        - .*\n        - .*@localhost\n</code></pre>"},{"location":"installation/configuration/authentication/#ldap-authentication","title":"LDAP Authentication","text":"<p>The configuration block at <code>auth.ldap</code> allows you to easily add authentication via your LDAP server. The LDAP authentication module will be able to automatically assign roles, classification, avatar, name, and email address based on the properties of the LDAP user and the groups it is a member of.</p> <p>Here is an example configuration block to add to your configuration file that will allow you to connect to the docker-test-openldap server from: https://github.com/rroemhild/docker-test-openldap</p> LDAP configuration example <pre><code>auth:\n    internal:\n        # Disable internal login, you could also leave it on if you want\n        enabled: false\n    ldap:\n        # Should LDAP be enabled or not?\n        enabled: true\n\n        # Auto-create users if they are missing, this means\n        #  that if a user exists in LDAP, Assemblyline will create an\n        #  account for it upon the first login\n        auto_create: true\n\n        # Should we automatically sync roles, classification, avatar\n        #  email, name... with the LDAP server upon each login?\n        auto_sync: true\n\n        # Automatic role and classification assignments\n        auto_properties:\n            # any user with a @localhost.local email will be given\n            #  TLP:Amber classification and any user with a @example.com\n            #  email will be made administrator in the system\n            - field: email\n              pattern: .*@localhost\\.local$\n              type: classification\n              value: \"TLP:A\"\n            - field: email\n              pattern: .*@example\\.com$\n              type: role\n              value: \"admin\"\n\n        # Base DN for the users\n        base: ou=people,dc=planetexpress,dc=com\n\n        # Password used to query the LDAP server\n        bind_pass: null\n\n        # User use to query the LDAP server\n        bind_user: null\n\n        # Name of the field containing the email address\n        email_field: mail\n\n        # How the group lookup is queried\n        group_lookup_query: (&amp;(objectClass=Group)(member=%s))\n\n        # Name of the field containing the user's avatar\n        image_field: jpegPhoto\n\n        # Type of image used to store the avatar\n        image_format: jpeg\n\n        # Name of the field containing the user's name\n        name_field: cn\n\n        # Field name for the UID\n        uid_field: uid\n\n        # URI to the LDAP server\n        uri: ldaps://&lt;ldap_ip_or_domain&gt;:636\n</code></pre>"},{"location":"installation/configuration/authentication/#oauth-authentication","title":"OAuth Authentication","text":"<p>The configuration block at <code>auth.oauth</code> allows you to add OAuth authentication to your system. Assemblyline OAuth module is configurable enough to allow you to use almost any OAuth provider.</p> <p>It has been thoroughly tested with:</p> <ul> <li>Microsoft Accounts</li> <li>Google Accounts</li> <li>Auth0</li> <li>Microsoft Azure Active Directory Accounts</li> </ul> Exhaustive OAuth configuration example <pre><code>auth:\n    internal:\n        # Disable internal login, you could also leave it on if you want\n        enabled: false\n    oauth:\n        # Should OAuth authentication be enabled or not\n        enabled: true\n\n        # Should we try to pull the user's avatar using gravatar\n        gravatar_enabled: false\n\n        # OAuth providers configuration block, you can have as many OAuth\n        #  providers as you want\n        providers:\n            # Name of the provider displayed in the UI\n            local_provider:\n                # Auto-create users if they are missing, this means\n                #  that if a user exists in the OAuth provider, Assemblyline\n                #  will create an account for it upon the first login\n                #     WARNING: If you set it to true for let's say Google's\n                #              OAuth provider, anyone with a google account\n                #              essentially has access to your system\n                auto_create: true\n\n                # Should we automatically sync roles, classification, avatar\n                #  email, name... with the OAuth provider upon each login?\n                auto_sync: true\n\n                # Automatic role and classification assignments\n                auto_properties:\n                    # any user with a @localhost.local email will be given\n                    #  TLP:Amber classification\n                    - field: email\n                      pattern: .*@localhost\\.local$\n                      type: classification\n                      value: \"TLP:A\"\n                    # any user within the admins-sg will be made\n                    #  administrator in the system\n                    - field: groups\n                      pattern: ^admins-sg$\n                      type: role\n                      value: admin\n\n                # Assemblyline will auto configure other endpoints using this URL\n                openid_connect_discovery_url: null\n\n                # URL used to get the access token\n                access_token_url: https://oauth2.localhost/token\n\n                # Base URL for downloading the user's and groups info\n                api_base_url: https://openidconnect.localhost/\n\n                # URL used to authorize access to a resource\n                authorize_url: https://localhost/oauth2/auth\n\n                # ID of your application to authenticate to the OAuth\n                #  provider\n                client_id: null\n\n                # Password to your application to authenticate to the\n                #  OAuth provider\n                client_secret: null\n\n                # Keyword arguments passed to the different URLs\n                #  (to set the scope for example)\n                client_kwargs:\n                    scope: openid email profile\n\n                # URL used to verify if a returned JWKS token is valid\n                jwks_uri: https://localhost/oauth2/certs\n\n                # Name of the field that will contain the user ID\n                uid_field: uid\n\n                # Should we generate a random username for the\n                #  authenticated user?\n                uid_randomize: false\n\n                # How many digits should we add at the end of the username?\n                uid_randomize_digits: 0\n\n                # What is the delimiter used by the random name generator?\n                uid_randomize_delimiter: \"-\"\n\n                # Regex used to parse and email address and capture parts\n                #  to create a user ID out of it\n                uid_regex: ^(.*)@(\\w*).*$\n\n                # Format of the user ID based on the captured parts from the regex\n                uid_format: '{}-{}'\n\n                # Should we use the new callback method?\n                use_new_callback_format: true\n\n                # Path from the base_url to fetch the user info\n                user_get: user/info\n\n                # Path from the base to fetch the group info\n                user_groups: group/info\n\n                # Field return by the group info API call that contains the\n                #  list of groups\n                user_groups_data_field: null\n\n                # Name of the field in the list of groups that contains the\n                #  name of the group\n                user_groups_name_field: null\n\n                # Field name that maps the group membership field in id token\n                groups_id_token_field: null\n</code></pre>"},{"location":"installation/configuration/authentication/#using-openid-connect-providers","title":"Using OpenID Connect Providers","text":"<p>OpenID Connect (OIDC) Providers have a configuration endpoint that can be used to automatically configure <code>api_base_url</code>, <code>jwks_url</code>, <code>access_token_url</code>, and <code>authorize_url</code>. You can set the value of <code>openid_connect_discovery_url</code> to this configuration endpoint and Assemblyline will automatically configure the other URLs.</p> <p>For OIDC Providers with ID tokens, Assemblyline will parse user and group information when available. To get group information, Assemblyline will look for the <code>groups</code> key in the ID token by default. You can also configure Assemblyline to parse group information using another key by setting the <code>groups_id_token_field</code>. If Assemblyline is able to parse user/group information from an ID token, it will skip using the user_get/group_get endpoint.</p>"},{"location":"installation/configuration/authentication/#example-configurations","title":"Example Configurations","text":"<p>Below are some example configurations for popular OAuth providers.</p> Auth0GoogleKeycloakMicrosoft <pre><code>auth:\n    oauth:\n        enabled: true\n\n        # Setup the auto0 provider\n        providers:\n            auth0:\n                # It is safe to auto-create users here\n                # because it is your OAuth tenant\n                auto_create: true\n                auto_sync: true\n\n                # Put your client ID and secret here\n                client_id: &lt;YOUR_CLIENT_ID&gt;\n                client_secret: &lt;YOUR_CLIENT_SECRET&gt;\n\n                client_kwargs:\n                    scope: openid email profile\n\n                # Set your tenant's name in the following URLs\n                access_token_url: https://&lt;TENANT_NAME&gt;.auth0.com/oauth/token\n                api_base_url: https://&lt;TENANT_NAME&gt;.auth0.com/\n                authorize_url: https://&lt;TENANT_NAME&gt;.auth0.com/authorize\n                jwks_uri: https://&lt;TENANT_NAME&gt;.auth0.com/.well-known/jwks.json\n\n                user_get: userinfo\n</code></pre> <pre><code>auth:\n    oauth:\n        enabled: true\n\n        # Setup the Google provider\n        providers:\n            google:\n                # It is safe to auto-create users here\n                # because it is your OAuth tenant\n                auto_create: true\n                auto_sync: true\n\n                # Put your client ID and secret here\n                client_id: &lt;YOUR_CLIENT_ID&gt;\n                client_secret: &lt;YOUR_CLIENT_SECRET&gt;\n\n                client_kwargs:\n                    scope: openid email profile\n\n                # Point Assemblyline to Google's OIDC configuration endpoint to setup authentication\n                openid_connect_discovery_url: \"https://accounts.google.com/.well-known/openid-configuration\"\n\n                # Or manually configure the endpoints\n                # access_token_url: https://oauth2.googleapis.com/token\n                # api_base_url: https://openidconnect.googleapis.com/\n                # authorize_url: https://accounts.google.com/o/oauth2/v2/auth\n                # jwks_uri: https://www.googleapis.com/oauth2/v3/certs\n                # user_get: \"v1/userinfo\"\n</code></pre> <p>Note</p> <p>You need to configure a group membership mapper with token claim name <code>groups</code> to the client scope. You also need to provide the client ID, client secret and the Keycloak configuration URL that corresponds to your realm and client.</p> <pre><code>auth:\n  oauth:\n    enabled: true\n    providers:\n        # Name of the provider displayed in the UI\n        keycloak:\n          # Assemblyline will auto configure other endpoints using this URL\n          openid_connect_discovery_url: https://&lt;KEYCLOAK&gt;/.well-known/openid-configuration\n\n          # Put your client ID and secret here\n          client_id: &lt;CLIENT_ID&gt;\n          client_secret: &lt;CLIENT_SECRET&gt;\n\n          client_kwargs:\n            scope: openid email profile\n\n          # Field name that maps the group membership field in id token\n          groups_id_token_field: groups\n</code></pre> <pre><code>auth:\n    oauth:\n        enabled: true\n\n        # Setup the Microsoft provider\n        providers:\n            microsoft:\n                # It is safe to auto-create users here\n                # because it is your OAuth tenant\n                auto_create: true\n                auto_sync: true\n\n                # Put your client ID and secret here\n                client_id: &lt;YOUR_CLIENT_ID&gt;\n                client_secret: &lt;YOUR_CLIENT_SECRET&gt;\n\n                client_kwargs:\n                    scope: openid email profile\n\n                # Point Assemblyline to Microsoft's OIDC configuration endpoint to setup authentication\n                # If using Azure AD, replace 'common' with your tenant ID\n                openid_connect_discovery_url: \"https://login.microsoftonline.com/common/v2.0/.well-known/openid-configuration\"\n\n                # Or manually configure the endpoints\n                # access_token_url: https://login.microsoftonline.com/common/oauth2/v2.0/token\n                # api_base_url: https://graph.microsoft.com\n                # authorize_url: https://login.microsoftonline.com/common/oauth2/v2.0/authorize\n                # jwks_uri: https://login.microsoftonline.com/common/discovery/v2.0/keys\n                # user_get: \"oidc/userinfo\"\n</code></pre>"},{"location":"installation/configuration/authentication/#saml-authentication","title":"SAML Authentication","text":"<p>The configuration block at <code>auth.saml</code> allows you to add authentication with your SAML server.</p> <p>Here is an example configuration block to add to your configuration file that will allow you to connect to the BoxyHQ's MockSAML server.</p> SAML configuration example <pre><code>auth:\n  saml:\n    enabled: true\n    settings:\n      sp:\n        entity_id: https://saml.example.com/entityid/assemblyline\n        assertion_consumer_service:\n          url: https://localhost/api/v4/auth/saml/acs/\n      idp:\n        entity_id: https://saml.example.com/entityid/assemblyline\n        single_sign_on_service:\n          url: https://mocksaml.com/api/namespace/assemblyline/saml/sso\n        x509cert: |\n          -----BEGIN CERTIFICATE-----\n          MIIC4jCCAcoCCQC33wnybT5QZDANBgkqhkiG9w0BAQsFADAyMQswCQYDVQQGEwJV\n          SzEPMA0GA1UECgwGQm94eUhRMRIwEAYDVQQDDAlNb2NrIFNBTUwwIBcNMjIwMjI4\n          MjE0NjM4WhgPMzAyMTA3MDEyMTQ2MzhaMDIxCzAJBgNVBAYTAlVLMQ8wDQYDVQQK\n          DAZCb3h5SFExEjAQBgNVBAMMCU1vY2sgU0FNTDCCASIwDQYJKoZIhvcNAQEBBQAD\n          ggEPADCCAQoCggEBALGfYettMsct1T6tVUwTudNJH5Pnb9GGnkXi9Zw/e6x45DD0\n          RuRONbFlJ2T4RjAE/uG+AjXxXQ8o2SZfb9+GgmCHuTJFNgHoZ1nFVXCmb/Hg8Hpd\n          4vOAGXndixaReOiq3EH5XvpMjMkJ3+8+9VYMzMZOjkgQtAqO36eAFFfNKX7dTj3V\n          pwLkvz6/KFCq8OAwY+AUi4eZm5J57D31GzjHwfjH9WTeX0MyndmnNB1qV75qQR3b\n          2/W5sGHRv+9AarggJkF+ptUkXoLtVA51wcfYm6hILptpde5FQC8RWY1YrswBWAEZ\n          NfyrR4JeSweElNHg4NVOs4TwGjOPwWGqzTfgTlECAwEAATANBgkqhkiG9w0BAQsF\n          AAOCAQEAAYRlYflSXAWoZpFfwNiCQVE5d9zZ0DPzNdWhAybXcTyMf0z5mDf6FWBW\n          5Gyoi9u3EMEDnzLcJNkwJAAc39Apa4I2/tml+Jy29dk8bTyX6m93ngmCgdLh5Za4\n          khuU3AM3L63g7VexCuO7kwkjh/+LqdcIXsVGO6XDfu2QOs1Xpe9zIzLpwm/RNYeX\n          UjbSj5ce/jekpAw7qyVVL4xOyh8AtUW1ek3wIw1MJvEgEPt0d16oshWJpoS1OT8L\n          r/22SvYEo3EmSGdTVGgk3x3s+A0qWAqTcyjr7Q4s/GKYRFfomGwz0TZ4Iw1ZN99M\n          m0eo2USlSRTVl7QHRTuiuSThHpLKQQ==\n          -----END CERTIFICATE-----\n</code></pre>"},{"location":"installation/configuration/config_file/","title":"Configuration YAML file","text":""},{"location":"installation/configuration/config_file/#configuration-yaml-file","title":"Configuration YAML file","text":"<p>Assemblyline 4 configuration is done using a YAML file (<code>config.yml</code>) which is deployed to all containers when they are launched.</p>"},{"location":"installation/configuration/config_file/#specification-and-defaults","title":"Specification and defaults","text":"<p>The full specification of the file is defined here. The Object Data Model (ODM) converts the python model to a YAML file which looks like the following by default:</p> Default configuration file values <pre><code>auth:\n  allow_2fa: true\n  allow_apikeys: true\n  allow_extended_apikeys: true\n  allow_security_tokens: true\n  apikey_max_dtl: null\n  internal:\n    enabled: true\n    failure_ttl: 60\n    max_failures: 5\n    password_requirements:\n      lower: false\n      min_length: 12\n      number: false\n      special: false\n      upper: false\n    signup:\n      enabled: false\n      notify:\n        activated_template: null\n        api_key: null\n        authorization_template: null\n        base_url: null\n        password_reset_template: null\n        registration_template: null\n      smtp:\n        from_adr: null\n        host: null\n        password: null\n        port: 587\n        tls: true\n        user: null\n      valid_email_patterns:\n      - .*\n      - .*@localhost\n  ldap:\n    admin_dn: null\n    auto_create: true\n    auto_properties: []\n    auto_sync: true\n    base: ou=people,dc=assemblyline,dc=local\n    bind_pass: null\n    bind_user: null\n    email_field: mail\n    enabled: false\n    group_lookup_query: (&amp;(objectClass=Group)(member=%s))\n    group_lookup_with_uid: false\n    image_field: jpegPhoto\n    image_format: jpeg\n    name_field: cn\n    uid_field: uid\n    uri: ldap://localhost:389\n  oauth:\n    enabled: false\n    gravatar_enabled: true\n    providers:\n      auth0:\n        access_token_url: https://{TENANT}.auth0.com/oauth/token\n        api_base_url: https://{TENANT}.auth0.com/\n        authorize_url: https://{TENANT}.auth0.com/authorize\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://{TENANT}.auth0.com/.well-known/jwks.json\n        user_get: userinfo\n      azure_ad:\n        access_token_url: https://login.microsoftonline.com/common/oauth2/token\n        api_base_url: https://login.microsoft.com/common/\n        authorize_url: https://login.microsoftonline.com/common/oauth2/authorize\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://login.microsoftonline.com/common/discovery/v2.0/keys\n        user_get: openid/userinfo\n      google:\n        access_token_url: https://oauth2.googleapis.com/token\n        api_base_url: https://openidconnect.googleapis.com/\n        authorize_url: https://accounts.google.com/o/oauth2/v2/auth\n        client_id: null\n        client_kwargs:\n          scope: openid email profile\n        client_secret: null\n        jwks_uri: https://www.googleapis.com/oauth2/v3/certs\n        user_get: v1/userinfo\n  saml:\n    attributes:\n      email_attribute: email\n      fullname_attribute: name\n      group_type_mapping: {}\n      groups_attribute: groups\n      roles_attribute: roles\n    auto_create: true\n    auto_sync: true\n    enabled: false\n    lowercase_urlencoding: false\n    settings:\n      debug: false\n      idp:\n        entity_id: https://mocksaml.com/api/saml/metadata\n        single_sign_on_service:\n          binding: urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\n          url: https://mocksaml.com/api/saml/sso\n      sp:\n        assertion_consumer_service:\n          binding: urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\n          url: https://localhost/api/v4/auth/saml/acs/\n        entity_id: https://assemblyline/sp\n        name_id_format: urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified\n      strict: false\ncore:\n  alerter:\n    alert_ttl: 90\n    default_group_field: file.sha256\n    delay: 300\n    filtering_group_fields:\n    - file.name\n    - status\n    - priority\n    non_filtering_group_fields:\n    - file.md5\n    - file.sha1\n    - file.sha256\n    process_alert_message: assemblyline_core.alerter.processing.process_alert_message\n    threshold: 500\n  archiver:\n    alternate_dtl: 0\n    minimum_required_services: []\n    use_webhook: false\n    webhook:\n      ca_cert: null\n      headers: []\n      method: POST\n      password: null\n      proxy: null\n      retries: 3\n      ssl_ignore_errors: false\n      uri: https://archiving-hook\n      username: null\n  dispatcher:\n    max_inflight: 1000\n    timeout: 900\n  expiry:\n    badlisted_tag_dtl: 0\n    batch_delete: false\n    delay: 0\n    delete_batch_size: 2000\n    delete_storage: true\n    delete_workers: 2\n    iteration_max_tasks: 50\n    safelisted_tag_dtl: 0\n    sleep_time: 15\n    workers: 20\n  ingester:\n    cache_dtl: 2\n    default_max_extracted: 100\n    default_max_supplementary: 100\n    default_resubmit_services: []\n    default_services: []\n    default_user: internal\n    description_prefix: Bulk\n    expire_after: 1296000\n    get_whitelist_verdict: assemblyline.common.signaturing.drop\n    incomplete_expire_after_seconds: 3600\n    incomplete_stale_after_seconds: 1800\n    is_low_priority: assemblyline.common.null.always_false\n    max_inflight: 500\n    sampling_at:\n      critical: 500000\n      high: 1000000\n      low: 10000000\n      medium: 2000000\n    stale_after_seconds: 86400\n    whitelist: assemblyline.common.null.whitelist\n  metrics:\n    apm_server:\n      server_url: null\n      token: null\n    elasticsearch:\n      cold: 30\n      delete: 90\n      host_certificates: null\n      hosts: null\n      unit: d\n      warm: 2\n    export_interval: 5\n    redis: &amp;id001\n      host: 127.0.0.1\n      port: 6379\n  plumber:\n    notification_queue_interval: 1800\n    notification_queue_max_age: 86400\n  redis:\n    nonpersistent: *id001\n    persistent:\n      host: 127.0.0.1\n      port: 6380\n  scaler:\n    additional_labels: null\n    cpu_overallocation: 1\n    linux_node_selector:\n      field: []\n      label: []\n    memory_overallocation: 1\n    overallocation_node_limit: null\n    privileged_services_additional_labels: null\n    service_defaults:\n      backlog: 100\n      environment:\n      - name: SERVICE_API_HOST\n        value: http://service-server:5003\n      - name: AL_SERVICE_TASK_LIMIT\n        value: inf\n      growth: 60\n      min_instances: 0\n      shrink: 30\n  updater:\n    job_dockerconfig:\n      cpu_cores: 1\n      ram_mb: 1024\n      ram_mb_min: 256\n    registry_configs:\n    - name: registry.hub.docker.com\n      proxies: {}\ndatasources:\n  al:\n    classpath: assemblyline.datasource.al.AL\n    config: {}\n  alert:\n    classpath: assemblyline.datasource.alert.Alert\n    config: {}\ndatastore:\n  archive:\n    enabled: false\n    indices:\n    - file\n    - submission\n    - result\n  cache_dtl: 5\n  hosts:\n  - http://elastic:devpass@localhost:9200\n  type: elasticsearch\nfilestore:\n  archive:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-archive&amp;use_ssl=False\n  cache:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&amp;use_ssl=False\n  storage:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&amp;use_ssl=False\nlogging:\n  export_interval: 5\n  heartbeat_file: /tmp/heartbeat\n  log_as_json: true\n  log_directory: /var/log/assemblyline/\n  log_level: INFO\n  log_to_console: true\n  log_to_file: false\n  log_to_syslog: false\n  syslog_host: localhost\n  syslog_port: 514\nretrohunt:\n  api_key: ChangeThisDefaultRetroHuntAPIKey!\n  dtl: 30\n  enabled: false\n  max_dtl: 0\n  tls_verify: true\n  url: https://hauntedhouse:4443\nservices:\n  allow_insecure_registry: false\n  categories:\n  - Antivirus\n  - Dynamic Analysis\n  - External\n  - Extraction\n  - Filtering\n  - Internet Connected\n  - Networking\n  - Static Analysis\n  cpu_reservation: 0.25\n  default_auto_update: false\n  default_timeout: 60\n  image_variables: {}\n  preferred_update_channel: stable\n  registries: []\n  safelist:\n    enabled: true\n    enforce_safelist_service: false\n    hash_types:\n    - sha1\n    - sha256\n  stages:\n  - FILTER\n  - EXTRACT\n  - CORE\n  - SECONDARY\n  - POST\n  - REVIEW\n  update_image_variables: {}\nsubmission:\n  default_max_extracted: 500\n  default_max_supplementary: 500\n  default_temporary_keys:\n    email_body: union\n    passwords: union\n  dtl: 30\n  emptyresult_dtl: 5\n  file_sources: []\n  max_dtl: 0\n  max_extraction_depth: 6\n  max_file_size: 104857600\n  max_metadata_length: 4096\n  max_temp_data_length: 4096\n  metadata:\n    archive: {}\n    ingest:\n      INGEST: {}\n    submit: {}\n  profiles:\n  - name: static\n    display_name: \"Static Analysis [OFFLINE]\"\n    summary: \"Quick scan; keep it local\"\n    description: |\n      **Summary**\n\n      Quick, local-only scan with no execution.\n\n      **What it does**\n\n      Analyzes files using internal and open-source tools (e.g., YARA, CAPA) to inspect their structure, metadata, and embedded indicators without running any code.\n\n      **When to use it**\n      - Rapid triage\n      - Checking sensitive or proprietary files that must never leave the local network\n\n      **Limitations**\n      - Low detection rate for packed or heavily obfuscated malware\n      - Cannot observe runtime behavior or command-and-control (C2) logic\n    params:\n      services:\n        selected:\n        - Filtering\n        - Antivirus\n        - Static Analysis\n        - Extraction\n        - Networking\n  - name: static_with_dynamic\n    display_name: \"Static + Dynamic Analysis [OFFLINE]\"\n    summary: \"See behavior; keep it local\"\n    description: |\n      **Summary**\n\n      Local sandbox detonation with behavioral visibility.\n\n      **What it does**\n\n      Combines static analysis with full dynamic execution in a local sandbox to observe process creation, file system changes, registry activity, and system interactions.\n\n      **When to use it**\n      - Standard malware investigation\n      - Understanding what a file does at runtime without risking data leakage to third-party APIs\n\n      **Limitations**\n      - Malware may evade or delay execution if it detects the sandbox environment\n      - Limited visibility into network-based indicators without internet access\n    params:\n      services:\n        selected:\n        - Filtering\n        - Antivirus\n        - Static Analysis\n        - Extraction\n        - Networking\n        - Dynamic Analysis\n  - name: static_with_internet\n    display_name: \"Static Analysis [ONLINE]\"\n    summary: \"Is this a known threat? (Quick check)\"\n    description: |\n      **Summary**\n\n      Quick reputation check using global intelligence sources.\n\n      **What it does**\n\n      Performs metadata and hash lookups against external services (e.g., VirusTotal, Google Threat Intelligence) without executing the file.\n\n      **When to use it**\n      - Quickly determining whether a file is already known malicious\n      - Prioritizing triage based on global reputation\n\n      **Limitations**\n      - Potential data leakage via hash or metadata queries\n      - Unique samples may alert adversaries that analysis is occurring\n    params:\n      services:\n        selected:\n        - Filtering\n        - Antivirus\n        - Static Analysis\n        - Extraction\n        - Networking\n        - Internet Connected\n  - name: static_and_dynamic_with_internet\n    display_name: \"Static + Dynamic Analysis [ONLINE]\"\n    summary: \"Full deep-dive; allow network traffic\"\n    description: |\n      **Summary**\n\n      Complete analysis with execution and internet access.\n\n      **What it does**\n\n      Executes files in a sandbox with live internet connectivity to capture command-and-control traffic, network indicators, and runtime behavior, while also leveraging external reputation services.\n\n      **When to use it**\n      - Deep investigation of unknown or high-risk samples\n      - Identifying network IOCs and full malware lifecycle behavior\n\n      **Limitations**\n      - Privacy and data exposure risk\n      - Sample or metadata may be shared with third-party services\n    params:\n      service_spec:\n        CAPE:\n          routing: internet\n        URLDownloader:\n          proxy: localhost_proxy\n      services:\n        selected:\n        - Filtering\n        - Antivirus\n        - Static Analysis\n        - Extraction\n        - Networking\n        - Internet Connected\n        - Dynamic Analysis\n  tag_types:\n    attribution:\n    - attribution.actor\n    - attribution.campaign\n    - attribution.exploit\n    - attribution.implant\n    - attribution.family\n    - attribution.network\n    - av.virus_name\n    - file.config\n    - technique.obfuscation\n    behavior:\n    - file.behavior\n    ioc:\n    - network.email.address\n    - network.static.ip\n    - network.static.domain\n    - network.static.uri\n    - network.dynamic.ip\n    - network.dynamic.domain\n    - network.dynamic.uri\n  temporary_keys: {}\n  verdicts:\n    highly_suspicious: 700\n    info: 0\n    malicious: 1000\n    suspicious: 300\nsystem:\n  constants: assemblyline.common.constants\n  organisation: ACME\n  type: production\nui:\n  ai_backends:\n    api_connections:\n    - api_type: openai\n      chat_url: https://api.openai.com/v1/chat/completions\n      headers:\n        Content-Type: application/json\n      model_name: gpt-3.5-turbo\n      proxies: null\n      verify: true\n    - api_type: openai\n      chat_url: https://api.openai.com/v1/chat/completions\n      headers:\n        Content-Type: application/json\n      model_name: gpt-4\n      proxies: null\n      verify: true\n    enabled: false\n    function_params:\n      assistant:\n        max_tokens: 1024\n        options:\n          frequency_penalty: 0\n          presence_penalty: 0\n          temperature: 0\n          top_p: 1\n        system_message: '## Context\n\n          You are the Assemblyline (AL) AI Assistant. You help people answer their\n          questions and other requests interactively\n\n          regarding Assemblyline. $(EXTRA_CONTEXT)\n\n\n          ## Style Guide\n\n\n          - Your output must be formatted in standard Markdown syntax\n\n          - Highlight important information using backticks\n\n          - Your answer must be written in plain $(LANG).\n\n          '\n      code:\n        max_tokens: 1024\n        options:\n          frequency_penalty: 0\n          presence_penalty: 0\n          temperature: 0\n          top_p: 1\n        system_message: '## Context\n\n\n          You are an assistant that provides explanation of code snippets found in\n          AssemblyLine (AL),\n\n          a malware detection and analysis tool. $(EXTRA_CONTEXT)\n\n\n          ## Style Guide\n\n\n          - Your output must be formatted in standard Markdown syntax\n\n          - Highlight important information using backticks\n\n          - Your answer must be written in plain $(LANG).\n\n          '\n        task: 'Take the code file below and give me a two part result:\n\n\n          - The first part is a short summary of the intent behind the code titled\n          \"## Summary\"\n\n          - The second part is a detailed explanation of what the code is doing titled\n          \"## Detailed Analysis\"\n\n          '\n      detailed_report:\n        max_tokens: 2048\n        options:\n          frequency_penalty: 0\n          presence_penalty: 0\n          temperature: 0\n          top_p: 1\n        system_message: '## Context\n\n\n          You are an assistant that summarizes the output of AssemblyLine (AL), a\n          malware detection and analysis tool.\n\n          Your role is to extract information of importance and discard what is not.\n          $(EXTRA_CONTEXT)\n\n\n          ## Style Guide\n\n\n          - Your output must be formatted in standard Markdown syntax\n\n          - Highlight important information using backticks\n\n          - Your answer must be written in plain $(LANG).\n\n          '\n        task: \"Take the Assemblyline report below in yaml format and create a two\\\n          \\ part result:\\n\\n- The first part is a one or two paragraph executive summary\\\n          \\ titled \\\"## Executive Summary\\\" which\\n  provides some high level highlights\\\n          \\ of the results\\n- The second part is a detailed description of the observations\\\n          \\ found in the report, this section\\n  is titled \\\"## Detailed Analysis\\\"\\\n          \\n\"\n      executive_summary:\n        max_tokens: 1024\n        options:\n          frequency_penalty: 0\n          presence_penalty: 0\n          temperature: 0\n          top_p: 1\n        system_message: '## Context\n\n\n          You are an assistant that summarizes the output of AssemblyLine (AL), a\n          malware detection and analysis tool. Your role\n\n          is to extract information of importance and discard what is not. $(EXTRA_CONTEXT)\n\n\n          ## Style Guide\n\n\n          - Your output must be formatted in standard Markdown syntax\n\n          - Highlight important information using backticks\n\n          - Your answer must be written in plain $(LANG).\n\n          '\n        task: 'Take the Assemblyline report below in yaml format and summarize the\n          information found in the\n\n          report into a one or two paragraph executive summary. DO NOT write any headers\n          in your output.\n\n          '\n  alerting_meta:\n    important:\n    - original_source\n    - protocol\n    - subject\n    - submitted_url\n    - source_url\n    - url\n    - web_url\n    - from\n    - to\n    - cc\n    - bcc\n    - ip_src\n    - ip_dst\n    - source\n    subject:\n    - subject\n    url:\n    - submitted_url\n    - source_url\n    - url\n    - web_url\n  allow_malicious_hinting: false\n  allow_raw_downloads: true\n  allow_replay: false\n  allow_url_submissions: true\n  allow_zip_downloads: true\n  api_proxies: {}\n  audit: true\n  audit_login: false\n  banner: null\n  banner_level: info\n  community_feed: https://alpytest.blob.core.windows.net/pytest/community.json\n  debug: false\n  default_quotas:\n    concurrent_api_calls: 10\n    concurrent_async_submissions: 0\n    concurrent_submissions: 5\n    daily_api_calls: 0\n    daily_submissions: 0\n  default_zip_password: infected\n  discover_url: null\n  download_encoding: cart\n  email: null\n  enforce_quota: true\n  external_links: []\n  external_sources: []\n  fqdn: localhost\n  ingest_max_priority: 250\n  read_only: false\n  read_only_offset: ''\n  rss_feeds:\n  - https://alpytest.blob.core.windows.net/pytest/stable.json\n  - https://alpytest.blob.core.windows.net/pytest/services.json\n  - https://alpytest.blob.core.windows.net/pytest/community.json\n  - https://alpytest.blob.core.windows.net/pytest/blog.json\n  secret_key: This is the default flask secret key... you should change this!\n  services_feed: https://alpytest.blob.core.windows.net/pytest/services.json\n  session_duration: 3600\n  statistics:\n    alert:\n    - al.attrib\n    - al.av\n    - al.behavior\n    - al.domain\n    - al.ip\n    - al.yara\n    - file.name\n    - file.md5\n    - owner\n    submission:\n    - params.submitter\n  tos: null\n  tos_lockout: false\n  tos_lockout_notify: null\n  url_submission_auto_service_selection:\n  - URLDownloader\n  url_submission_headers:\n    User-Agent: Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko)\n      Chrome/110.0.0.0 Safari/537.36\n  url_submission_proxies: {}\n  validate_session_ip: true\n  validate_session_useragent: true\n</code></pre>"},{"location":"installation/configuration/config_file/#layers-of-the-configuration-file","title":"Layers of the configuration file","text":"<p>The configuration file is built in layers:</p> <ol> <li>The ODM converts the python classes to the default values as shown above</li> <li>The default assemblyline helm chart values.yaml file changes certain of these values to adapt them to a Kubernetes deployment</li> <li>Your deployment's <code>values.yaml</code> file change the values to their final form</li> </ol>"},{"location":"installation/configuration/config_file/#changing-the-configuration-file","title":"Changing the configuration file","text":"<p>If you want to change the <code>config.yml</code> file that will be deployed in the containers, it will have to be done through the <code>configuration</code> section found in the <code>values.yml</code> file of your deployment.</p> <p>Example</p> <p>Let's say that you would want to change the log level in the system to <code>ERROR</code> an up.</p> <p>First of you would edit the <code>values.yaml</code> file of your personal deployment to add the changes to the configuration section: </p><pre><code>...\nconfiguration:\n  logging:\n    log_level: ERROR\n...\n</code></pre><p></p> <p>Then you would simply deploy that new <code>values.yaml</code> file using the <code>helm upgrade</code> command specific to your deployment:</p> <ul> <li>Cluster deployment update</li> <li>Appliance deployment update</li> </ul>"},{"location":"installation/configuration/config_file/#exhaustive-configuration-file-documentation","title":"Exhaustive configuration file documentation","text":"<p>All parameters of each configuration section will be thoroughly documented in their respective pages.</p> <p>Here are the links to the different section documentations:</p> <ul> <li>Authentication (auth:)</li> <li>Core components  (core:)</li> <li>Data sources (datasources:)</li> <li>Database (datastore:)</li> <li>File storage (filestore:)</li> <li>Logging (logging:)</li> <li>Retrohunt (retrohunt:)</li> <li>Services (services:)</li> <li>Submission (submission:)</li> <li>System (system:)</li> <li>User Interface (ui:)</li> </ul>"},{"location":"installation/configuration/core/","title":"Core component section","text":""},{"location":"installation/configuration/core/#core-component-section","title":"Core component section","text":"<p>The core components configuration section (<code>core:</code>) of the configuration file contains all the different parameters that you can change to modify the behaviour of each core components.</p> Default values for the core section <pre><code>...\ncore:\n  alerter:\n    alert_ttl: 90\n    constant_alert_fields:\n    - alert_id\n    - file\n    - ts\n    default_group_field: file.sha256\n    delay: 300\n    filtering_group_fields:\n    - file.name\n    - status\n    - priority\n    non_filtering_group_fields:\n    - file.md5\n    - file.sha1\n    - file.sha256\n    process_alert_message: assemblyline_core.alerter.processing.process_alert_message\n  dispatcher:\n    max_inflight: 1000\n    timeout: 900\n  expiry:\n    batch_delete: false\n    delay: 0\n    delete_storage: true\n    sleep_time: 15\n    workers: 20\n  ingester:\n    cache_dtl: 2\n    default_max_extracted: 100\n    default_max_supplementary: 100\n    default_resubmit_services: []\n    default_services: []\n    default_user: internal\n    description_prefix: Bulk\n    expire_after: 1296000\n    get_whitelist_verdict: assemblyline.common.signaturing.drop\n    incomplete_expire_after_seconds: 3600\n    incomplete_stale_after_seconds: 1800\n    is_low_priority: assemblyline.common.null.always_false\n    max_inflight: 500\n    sampling_at:\n      critical: 500000\n      high: 1000000\n      low: 10000000\n      medium: 2000000\n    stale_after_seconds: 86400\n    whitelist: assemblyline.common.null.whitelist\n  metrics:\n    apm_server:\n      server_url: null\n      token: null\n    elasticsearch:\n      cold: 30\n      delete: 90\n      host_certificates: null\n      hosts: null\n      unit: d\n      warm: 2\n    export_interval: 5\n    redis: &amp;id001\n      host: 127.0.0.1\n      port: 6379\n  redis:\n    nonpersistent: *id001\n    persistent:\n      host: 127.0.0.1\n      port: 6380\n  scaler:\n    additional_labels: null\n    cpu_overallocation: 1\n    memory_overallocation: 1\n    overallocation_node_limit: null\n    service_defaults:\n      backlog: 100\n      environment:\n      - name: SERVICE_API_HOST\n        value: http://service-server:5003\n      - name: AL_SERVICE_TASK_LIMIT\n        value: inf\n      growth: 60\n      min_instances: 0\n      shrink: 30\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/core/#alerter","title":"Alerter","text":"<p>The configuration block at <code>core.alerter</code> contains all the configuration parameters that the Assemblyline alerter component can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Alerter configuration example <pre><code>core:\n  alerter:\n    # Time to live in days for alerts in the system\n    alert_ttl: 90\n\n    # List of fields that should keep the same value in between normal and extended scan\n    #  NOTE: You should not have to change those ever, this might get removed in the future\n    constant_alert_fields:\n    - alert_id\n    - file\n    - ts\n\n    # Default field to group alerts with in the UI\n    default_group_field: file.sha256\n\n    # Delay applied to the alert UI to leave time to the extended scan to complete\n    delay: 300\n\n    # List of fields allowed to be used for grouping that are present in every single alerts\n    filtering_group_fields:\n    - file.name\n    - status\n    - priority\n\n    # List of fields allowed to be used for grouping that are present only in a subset of alerts\n    non_filtering_group_fields:\n    - file.md5\n    - file.sha1\n    - file.sha256\n\n    # Python class used to process alert messages\n    #   NOTE: This should not be changed unless you've built your own container with\n    #         extra packages with alert processing capability\n    process_alert_message: assemblyline_core.alerter.processing.process_alert_message\n</code></pre>"},{"location":"installation/configuration/core/#dispatcher","title":"Dispatcher","text":"<p>The configuration block at <code>core.dispatcher</code> contains all the configuration parameters that the Assemblyline dispatcher component can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Dispatcher configuration example <pre><code>core:\n  dispatcher:\n    # Maximum amount of concurrent submission processing in the system\n    max_inflight: 1000\n\n    # Use in earlier version of dispatcher (To be removed)\n    timeout: 900\n</code></pre>"},{"location":"installation/configuration/core/#expiry","title":"Expiry","text":"<p>The configuration block at <code>core.expiry</code> contains all the configuration parameters that the Assemblyline expiry component can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Expiry configuration example <pre><code>core:\n    expiry:\n      # Perform delete operation in batch when changing day instead of throughout the day\n      #   NOTE: Not deleting during the day will make the system more responsive during the day\n      #         but will significantly reduce performance when changing to a new day until\n      #         all delete operations are done.\n      batch_delete: false\n\n      # Delay in hours applied to the deletion schedule\n      delay: 0\n\n      # Should data expiry operation get rid of files as well?\n      delete_storage: true\n\n      # Time to sleep (sec) in between runs when there is not data to delete\n      sleep_time: 15\n\n      # Number of workers used to delete data\n      workers: 20\n</code></pre>"},{"location":"installation/configuration/core/#ingester","title":"Ingester","text":"<p>The configuration block at <code>core.ingester</code> contains all the configuration parameters that the Assemblyline ingester component can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Ingester configuration example <pre><code>core:\n    ingester:\n      # Number of days ingested files are valid in the ingester cache\n      cache_dtl: 2\n\n      # Maximum number of extracted files for ingested submissions\n      default_max_extracted: 100\n\n      # Maximum number of supplementary files for ingested submissions\n      default_max_supplementary: 100\n\n      # UNUSED\n      default_resubmit_services: []\n      default_services: []\n      default_user: internal\n      description_prefix: Bulk\n\n      # Seconds before a previously ingested file will be considered new again,\n      #  the file will then be processed as if never seen before.\n      expire_after: 1296000\n\n      # Function import path for method to determine whitelisting.\n      #  Files selected by this function will be dropped.\n      #  See the default function for signature.\n      get_whitelist_verdict: assemblyline.common.signaturing.drop\n\n      # Special version of 'expire_after' applied when the previous run of a file had errors.\n      incomplete_expire_after_seconds: 3600\n\n      # Special version of 'stale_after_seconds' applied when the previous run of a file had errors.\n      incomplete_stale_after_seconds: 1800\n\n      # Function import path for method to determine low-priority filter.\n      #  Files selected by this function will be forced to low-priority.\n      #  See the default function for signature.\n      is_low_priority: assemblyline.common.null.always_false\n\n      # How many submissions should ingester try to submit concurrently.\n      max_inflight: 500\n\n      # How long should X queue be before the sampling (randomly dropping files) starts.\n      #  At the given value sampling will start, and grow gradually more agressive until 3 times\n      #  the value given. At 3 times the value given the system will proccess as many files\n      #  as possible, and all others will be discarded.\n      sampling_at:\n        critical: 500000\n        high: 1000000\n        low: 10000000\n        medium: 2000000\n\n      # Seconds before a previously ingested file will be considered stale,\n      #  the file will be reprocessed but the priority will be modified depending\n      #  on the score from the previous run.\n      stale_after_seconds: 86400\n\n      # File whitelist import path. Imported object will be passed to the 'get_whitelist_verdict'\n      #  function. The default verdict function will use this as a file whitelist.\n      #  See default value for sample.\n      whitelist: assemblyline.common.null.whitelist\n</code></pre>"},{"location":"installation/configuration/core/#metrics","title":"Metrics","text":"<p>The configuration block at <code>core.metrics</code> contains all the configuration parameters that the Assemblyline metrics gathering components can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Metrics configuration example <pre><code>core:\n    metrics:\n      # APM specific settings\n      apm_server:\n        # URL to the APM server\n        server_url: null\n\n        # Token use to connect to the APM server\n        token: null\n\n      # Elasticsearch specific configuration\n      elasticsearch:\n        # Number of `unit` document spend time in ILM cold storage\n        cold: 30\n\n        # Number of `unit` after which documents are deleted using ILM\n        delete: 90\n\n        # Cert used to connect to Elastic\n        host_certificates: null\n\n        # List of Elatic hosts\n        hosts: null\n\n        # Time unit for ILM cold, warm and delete\n        unit: d\n\n        # Number of `unit` document spend time in ILM warm storage\n        warm: 2\n\n      # Interval at which the metrics components export their data\n      export_interval: 5\n\n      # Redis specific configuration\n      redis:\n        # Host of the redis server\n        host: 127.0.0.1\n\n        # Port of the redis server\n        port: 6379\n</code></pre>"},{"location":"installation/configuration/core/#redis","title":"Redis","text":"<p>The configuration block at <code>core.redis</code> contains all the configuration parameters used by Assemblyline components to connect to redis.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Redis configuration example <pre><code>core:\n    redis:\n      # Configuration of the non-persistent redis (use mainly for messaging)\n      nonpersistent:\n        # Host of the redis server\n        host: 127.0.0.1\n\n        # Port of the redis server\n        port: 6379\n\n      # Configuration of the persistent redis (use mainly for task queuing)\n      persistent:\n        # Host of the redis server\n        host: 127.0.0.1\n\n        # Port of the redis server\n        port: 6380\n</code></pre>"},{"location":"installation/configuration/core/#scaler","title":"Scaler","text":"<p>The configuration block at <code>core.scaler</code> contains all the configuration parameters that the Assemblyline scaler component can take.</p> <p>Here is an example configuration block with inline comments about the purpose of every single parameters:</p> Scaler configuration example <pre><code>core:\n    scaler:\n      # Percentage of CPU overallocation, represented as 0-1 float\n      cpu_overallocation: 0.5\n      # Percentage of RAM overallocation, represented as 0-1 float\n      memory_overallocation: 1\n      # If the system has this many nodes or more overallocation is ignored\n      overallocation_node_limit: 3\n      # Additional labels to be applied to services('=' delimited)\n      additional_labels:\n        - env=production\n      service_defaults:\n        # How many files in a service queue are considered a backlog.\n        #  This weights how important scaling up a service is relative\n        #  to its queue. You probably don't want to change this.\n        backlog: 100\n\n        # List of environment variables set for all services.\n        #  Usually used for deployment related environment variables.\n        #  Service specific environent variables can be set in the\n        #  service manifest or in the UI for a particular system.\n        environment:\n        - name: SERVICE_API_HOST\n          value: http://service-server:5003\n        - name: AL_SERVICE_TASK_LIMIT\n          value: inf\n\n        # Roughly how many seconds to wait before a service\n        #  scales up to meet increased demand.\n        growth: 60\n\n        # The minimum number of instances of every service that\n        #  should be kept running at all times.\n        min_instances: 0\n\n        # Roughly how many seconds to wait before a service scales down when\n        #  instances are consistently idle.\n        shrink: 30\n</code></pre>"},{"location":"installation/configuration/datasources/","title":"Data sources section","text":""},{"location":"installation/configuration/datasources/#data-sources-section","title":"Data sources section","text":"<p>The Data Sources configuration section (<code>datasources:</code>) of the configuration file contains all the different parameters that you can change to add/modify data sources used by the hash search API.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Datasources section configuration example <pre><code>...\n# The data source section is essentially a key/value pair of source and their configuration\ndatasources:\n  # Source name (al)\n  al:\n    # Path to the module that will process the query received by the hash search API\n    classpath: assemblyline.datasource.al.AL\n\n    # Dictionary holding the configuration for the module\n    config: {}\n\n  # Source name (alert)\n  alert:\n    # Path to the module that will process the query received by the hash search API\n    classpath: assemblyline.datasource.alert.Alert\n\n    # Dictionary holding the configuration for the module\n    config: {}\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/datastore/","title":"Database section","text":""},{"location":"installation/configuration/datastore/#database-section","title":"Database section","text":"<p>The Database configuration section (<code>datastore:</code>) of the configuration file contains all the different parameters that you can change modify how to connect to the database and to modify the Index Lifecycle Management (ILM) parameters.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Datastore section configuration example <pre><code>...\ndatastore:\n  # List of elastic hosts to connect to\n  hosts:\n  - http://elastic:devpass@localhost\n\n  # Datastore Archive feature configuration\n  archive:\n    # Are we enabling Achiving features across indices?\n    enabled: true\n\n    # List of indices the ILM Applies to\n    indices:\n    - file\n    - submission\n    - result\n\n  # Index Lifecycle management configuration block\n  ilm:\n    # After how many days do documents go in the ILM managed indexes\n    days_until_archive: 15\n\n    # Is ILM enabled or not?\n    enabled: false\n\n    # Index specific ILM configuration\n    indexes:\n      alert:\n        # After how many `unit` documents goes in cold storage\n        cold: 15\n\n        # After how many `unit` documents are deleted\n        delete: 30\n\n        # Time unit definition for the current index\n        unit: d\n\n        # After how many `unit` documents goes in warm storage\n        warm: 5\n      error:\n        cold: 15\n        delete: 30\n        unit: d\n        warm: 5\n      file:\n        cold: 15\n        delete: 30\n        unit: d\n        warm: 5\n      result:\n        cold: 15\n        delete: 30\n        unit: d\n        warm: 5\n      submission:\n        cold: 15\n        delete: 30\n        unit: d\n        warm: 5\n\n    # Show saving new document update it's archive counterpart\n    #  NOTE: Setting this to false makes it faster but it will be possible to have\n    #        duplicate documents\n    update_archive: false\n\n  # Type of datastore (only elasticsearch is supported so far, do not change)\n  type: elasticsearch\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/filestore/","title":"File storage section","text":""},{"location":"installation/configuration/filestore/#file-storage-section","title":"File storage section","text":"<p>The file storage configuration section (<code>filestore:</code>) of the configuration file contains URLs to the different filestores and cachestore used by Assemblyline.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Filestore section configuration example <pre><code>...\n# Assemblyline uses a multistage file storing system. When multiple filestores are defined for\n# a single type, Assemblyline will save to all levels at once when adding files but when\n# retrieving file will try one level at the time in order until it finds the file.\n#\n# This allows you to have different retention schedule on the different levels and have faster\n# filestore store only files that are currently scanning in the system but slower ones to keep\n# more files but to look them up less often.\n\nfilestore:\n  # List of filestores used for malware archive\n  archive:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-archive&amp;use_ssl=False\n\n  # List of URLs to connect to the cache filestore\n  cache:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&amp;use_ssl=False\n\n  # List of URLs to connect to the data filestore\n  storage:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&amp;use_ssl=False\n...\n</code></pre>"},{"location":"installation/configuration/filestore/#supported-transports","title":"Supported Transports","text":"<p>All transport connection strings are composed of:</p> <ul> <li>protocol</li> <li>host</li> <li>port</li> <li>basic authentication (username, password)</li> </ul> <p>Example</p> <p><code>{protocol}://{username}:{password}@{host}:{port}/</code></p> <p>For every transport protocol, there may be a specific set of parameters for Assemblyline</p>"},{"location":"installation/configuration/filestore/#azure","title":"Azure","text":"<ul> <li>allow_directory_access: <code>boolean</code></li> <li>client_id: <code>string</code></li> <li>client_secret: <code>string</code></li> <li>connection_attempts: <code>integer</code></li> <li>tenant_id: <code>string</code></li> <li> <p>use_default_credentials: <code>boolean</code></p> <p>Example</p> <p><code>azure://assemblyline.blob.core.windows.net/?allow_directory_access=false&amp;client_id=&amp;client_secret=&amp;connection_attempts=&amp;tenant_id=&amp;use_default_credentials=false</code></p> </li> </ul>"},{"location":"installation/configuration/filestore/#ftp","title":"FTP","text":"<ul> <li> <p>use_tls: <code>boolean</code></p> <p>Example</p> <p><code>ftp://assemblyline:21/?use_tls=</code></p> </li> </ul>"},{"location":"installation/configuration/filestore/#http","title":"HTTP","text":"<ul> <li>pki: <code>string</code></li> <li> <p>verify: <code>boolean</code> | <code>string</code></p> <p>Example</p> <pre><code>http://assemblyline/?verify=&amp;pki=\nhttps://assemblyline/?verify=&amp;pki=\n</code></pre> </li> </ul>"},{"location":"installation/configuration/filestore/#local","title":"Local","text":"<ul> <li> <p>normalize: <code>boolean</code></p> <p>Example</p> <p><code>file://localhost//mnt/al_storage?normalize=</code></p> </li> </ul>"},{"location":"installation/configuration/filestore/#s3","title":"S3","text":"<ul> <li>accesskey: <code>string</code></li> <li>aws_region: <code>string</code></li> <li>boto_defaults: <code>boolean</code></li> <li>connection_attempts: <code>integer</code></li> <li>secretkey: <code>string</code></li> <li>s3_bucket: <code>string</code></li> <li>verify: <code>string</code> | <code>boolean</code></li> <li> <p>use_ssl: <code>boolean</code></p> <p>Example</p> <p><code>s3://minio:9000/?s3_bucket=al-storage&amp;aws_region=&amp;accesskey=&amp;secretkey=&amp;boto_defaults=false&amp;connection_attempts=&amp;verify=true&amp;use_ssl</code></p> <p>S3 Certificate Verification</p> <p>For S3-compatible file storage solutions, it is possible to enable verification through the <code>verify</code> parameter. The value of the parameter can either be a boolean or a path to the certificate on disk.</p> <p>This also assumes that if certificate isn't part of the system certificates, then you'll need to mount it using <code>coreMounts</code> &amp; <code>coreVolumes</code> and set <code>verify</code> to the path of the CA specified in the mount. You would also need to modify <code>configuration.core.scaler.service_defaults.mounts</code> to ensure privileged services have access to those certificates as well.</p> </li> </ul>"},{"location":"installation/configuration/filestore/#sftp","title":"SFTP","text":"<ul> <li>private_key: <code>string</code></li> <li>private_key_pass: <code>string</code></li> <li> <p>validate_host: <code>boolean</code></p> <p>Example</p> <p><code>sftp://localhost:22/?private_key=&amp;private_key_pass=&amp;validate_host=false</code></p> </li> </ul> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/logging/","title":"Logging section","text":""},{"location":"installation/configuration/logging/#logging-section","title":"Logging section","text":"<p>The logging configuration section (<code>logging:</code>) of the configuration file allows you to modify the log level and where the logs will be shipped in the system .</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Logging section configuration example <pre><code>...\nlogging:\n  # Interval at which the container heartbeat is written\n  export_interval: 5\n\n  # Location of the container heartbeat\n  heartbeat_file: /tmp/heartbeat\n\n  # Should logs use a JSON format\n  # (mainly used to parse logs into kibana, otherwise set to false to make them readable)\n  log_as_json: true\n\n  # Location on disk where the logs are stored if log_to_file enabled\n  log_directory: /var/log/assemblyline/\n\n  # Minimum log level\n  # (DEBUG, INFO, WARNING, ERROR)\n  log_level: INFO\n\n  # Should logs be shown in the console?\n  # You should have that to true if running inside containers\n  log_to_console: true\n\n  # Should you write logs to files?\n  # Set this to false when running inside a container\n  log_to_file: false\n\n  # Should you send logs to a syslog server?\n  log_to_syslog: false\n\n  # Host of the syslog server\n  syslog_host: localhost\n\n  # Port of the syslog server\n  syslog_port: 514\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/malware_archive/","title":"Malware Archive section","text":""},{"location":"installation/configuration/malware_archive/#malware-archive-section","title":"Malware Archive section","text":"<p>Refer to the Malware Archive to get an understanding of how this feature works.</p> <p>Configuring the Malware Archive functionality is done by modifying the data storage section (<code>datastore</code>).</p> <p>The minimum required change to turn on the Malware Archive is to set the <code>datastore.archive.enabled</code> flag to <code>true</code>. This will show the Archive link in the left navbar, activate the Archiver core component and create the archive indices in the database.</p> <p>Optionally, you can also add a separate entry for the archive in the <code>filestore</code> so your archived files are not mixed with the non-archived files. Having a separate archive filestore will make some operations in the system faster. You can also configure the Archiver behaviour by setting a minimum service selection for all archived submission which means that if you try to archive a submission where not all those services ran, the Archiver will create a derived submission with the added services and will archive that new submission instead once it's done.</p> Malware Archive configuration example <pre><code>...\ncore:\n  archiver:\n    # List of services that must be selected for a submission to be archived. (optional)\n    #   If a service is missing, the archiver will re-submit\n    #   the file with the added services and archive that\n    minimum_required_services: ['Extract', 'Safelist', ...]\n...\ndatastore:\n  archive:\n    # Are we enabling the Archiving features? (required)\n    enabled: true\n...\nfilestore:\n  # List of filestores used for the malware archive (optional)\n  archive:\n  - s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-archive&amp;use_ssl=False\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more details on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/plugins/","title":"UI plugin section","text":""},{"location":"installation/configuration/plugins/#ui-plugin-section","title":"UI plugin section","text":"<p>The plugin configuration section (<code>uiPlugins:</code>) of the <code>values.yaml</code> allows you to enable and configure both the built-in and your own custom plugins.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> UI plugin section configuration example <pre><code>...\nuiPlugins: # configure/setup external lookup type plugins.\n  lookup:\n    # Enable lookup plugins\n    # These are disabed by default. Set to `true` to enable them (note: individual plugins must still be enabled).\n    enabled: false\n\n    # Mapping of plugins to setup.\n    # You can add your own custom plugins to this mapping as long as they implement the correct interface.\n    plugins:\n\n      &lt;lookupName&gt;: # The name of the lookup plugin (eg. virustotal). This will be used in the deployment definitions.\n\n        # Enable or disable this particular plugin.\n        # Note: to enable, the upper level `uiPlugins.lookup.enabled` must also be `true`.\n        enabled: false\n\n        # The registry and image name of the plugin image.\n        image: cccs/assemblyline-ui-plugin-lookup-virustotal\n\n        # (optional) The image tag to use. If left blank it will default to the current Assemblyline version.\n        imageTag:\n\n        # The number of instances to run.\n        instances: 1\n\n        # CPU and RAM requests and limits.\n        reqCPU: 250m\n        reqRam: 256Mi\n        limRam: 256Mi\n\n        # Port the plugin container listens on.\n        containerPort: 8000\n\n        # (optional) Supply the name of an existing configMap that contains configuration data for the plugin.\n        # Use this OR the below `configMapData`.\n        configMapName:\n\n        # (optional) Configuration data for the plugin to be exposed as environment variables.\n        configMapData: |-\n          VT_VERIFY: true\n          MAX_TIMEOUT: \"3\"\n          API_URL: \"https://www.virustotal.com/api/v3\"\n          FRONTEND_URL: \"https://www.virustotal.com/gui/search\"\n          CLASSIFICATION: null\n\n        # (optional) Secret configuration data for the plugin to be exposed as environment variables.\n        # Use this OR the one of the below `secretName`, `secretKeys`.\n        secretData: |-\n          VT_API_KEY: \"\"\n\n        # (optional) Supply the name of an existing secret that contains configuration data for the plugin.\n        secretName:\n\n        # (optional) Individually use keys from existing secrets\n        # supplied as a list, eg:\n        # - varName: VT_API_KEY\n        #   secretName: ui-plugin-lookup-virustotal-apikey\n        #   secretKey: API_KEY\n        secretKeys: []\n\n        # (optional) Security context for the plugin to run under.\n        securityContext:\n          user: 1000\n          group: 1000\n          fsGroup: 1000\n      ...\n...\n\n# add the configured plugins to the AL UI config section.\nconfiguration:\n  ...\n  ui:\n    external_sources:\n\n    # Display name of the lookup source.\n    - name: virustotal\n\n      # Full url to the plugin microservice api.\n      # (this will be the lowercase `lookupName` key with a `ui-plugin-lookup-` prefix)\n      url: http://ui-plugin-lookup-virustotal:8000\n\n      # (optional) Minimum classification require to access the upstream service.\n      classification:\n\n      # (optional) Maximum classification that can be sumbitted to the upstream service.\n      max_classification:\n      ...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/retrohunt/","title":"Retrohunt section","text":""},{"location":"installation/configuration/retrohunt/#retrohunt-section","title":"Retrohunt section","text":"<p>Refer to the Retrohunt architecture to get an understanding of how this feature works.</p> <p>This section goes over how to configure the deployment instance to ensure the Retrohunt component is configured properly.</p> <p>The Retrohunt section (<code>retrohunt:</code>) of the configuration file contains all the different parameters that you can change.</p> Retrohunt configuration example <pre><code>...\nretrohunt:\n  # Is the Retrohunt functionality enabled on the frontend?\n  enabled: false\n\n  # Number of days retrohunt jobs will remain in the system by default\n  dtl: 30\n\n  # Maximum number of days retrohunt jobs will remain in the system\n  max_dtl: 0\n\n  # Base URL for service API\n  url: https://hauntedhouse:4443\n\n  # Service API Key\n  api_key: ChangeThisDefaultRetroHuntAPIKey!\n\n  # Should tls certificates be verified\n  tls_verify: true\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more details on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/services/","title":"Service section","text":""},{"location":"installation/configuration/services/#service-section","title":"Service section","text":"<p>The service configuration section (<code>services:</code>) of the configuration file allows you to modify the parameters on how services are executed in the system.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Service section configuration example <pre><code>...\nservices:\n  # Are we allowed to pull service containers from insecure registries\n  allow_insecure_registry: false\n\n  # List of valid categories for services\n  # You should not change this except to add a category maybe?\n  categories:\n  - Antivirus\n  - Dynamic Analysis\n  - External\n  - Extraction\n  - Filtering\n  - Internet Connected\n  - Networking\n  - Static Analysis\n\n  # Percentage of CPU resevation scaler will do for each service (1 = 100%)\n  cpu_reservation: 0.25\n\n  # Default service execution timeout\n  default_timeout: 60\n\n  # Set of environment varables applied to the service containers\n  # while loaded from updater or scaler\n  image_variables: {}\n\n  # Type of services the updater will be looking for when looking for service update\n  # (dev or stable)\n  preferred_update_channel: stable\n\n  # List of available\n  stages:\n  - FILTER\n  - EXTRACT\n  - CORE\n  - SECONDARY\n  - POST\n  - REVIEW\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/submission/","title":"Submission section","text":""},{"location":"installation/configuration/submission/#submission-section","title":"Submission section","text":"<p>Assemblyline supports configuration options that control how submissions are handled within the system.</p> <p>These options are defined under the <code>submission:</code> section of your system configuration file.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> Submission section configuration example <pre><code>...\nsubmission:\n  # Maximum amount of extracted files for a submission\n  default_max_extracted: 500\n\n  # Maximum amount of supplementary files for a submission\n  default_max_supplementary: 500\n\n  # Default amount of days submissions live in the system\n  dtl: 30\n\n  # Maximum amount of days submissions live in the system\n  max_dtl: 0\n\n  # Maximum extraction depth service can go\n  max_extraction_depth: 6\n\n  # Maximum file size allowed in the system\n  max_file_size: 104857600\n\n  # Maximum size of each metadata entry\n  max_metadata_length: 4096\n\n  # Types of tags to be included in the submission summary in\n  # the attribution, behaviour and ioc sectiona.\n  tag_types:\n    attribution:\n    - attribution.actor\n    - attribution.campaign\n    - attribution.exploit\n    - attribution.implant\n    - attribution.family\n    - attribution.network\n    - av.virus_name\n    - file.config\n    - technique.obfuscation\n    behavior:\n    - file.behavior\n    ioc:\n    - network.email.address\n    - network.static.ip\n    - network.static.domain\n    - network.static.uri\n    - network.dynamic.ip\n    - network.dynamic.domain\n    - network.dynamic.uri\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/submission/#file-sources","title":"File Sources","text":"<p>File Sources let administrators define reusable ingestion templates that transform a user-provided input (hash, URL, ticket ID, etc.) into a file or URL descriptor for analysis. They support both direct HTTP downloads and deferred URL retrieval through AL-URI files.</p>"},{"location":"installation/configuration/submission/#how-it-works","title":"How it works","text":""},{"location":"installation/configuration/submission/#template-expansion","title":"Template expansion","text":"<p>Assemblyline replaces the replace_pattern (for example, <code>{INPUT}</code>) inside url and data with the user\u2019s submitted value. The system also validates the input type against the source\u2019s hash_types or any custom hash_patterns.</p>"},{"location":"installation/configuration/submission/#ingestion-mode","title":"Ingestion mode","text":"Direct download (<code>download_from_url: true</code>)AL-URI hand-off (<code>download_from_url: false</code>) <p>The system performs the HTTP request using the configured method, headers, proxies, and verify, saves the result to a temporary file, and optionally checks failure_pattern to catch false-positive success pages.</p> <p>Assemblyline creates a lightweight AL-URI descriptor instead of downloading the file. This descriptor is submitted so a URL-capable service (for example, URLDownloader) can fetch the content during analysis.</p> <p>Requires <code>ui.allow_url_submissions: true</code> in the UI configuration.</p>"},{"location":"installation/configuration/submission/#optional-passworded-zip-handling","title":"Optional passworded ZIP handling","text":"<p>If <code>password</code> is defined and the downloaded file is a ZIP, Assemblyline attempts to extract it automatically if the archive contains a single file and is smaller than <code>submission.max_file_size</code>. If extraction fails or multiple files are present, the normal Extract service handles it later.</p>"},{"location":"installation/configuration/submission/#post-processing-and-policy-enforcement","title":"Post-processing and policy enforcement","text":"<ul> <li>Elevate the submission\u2019s classification to at least the source\u2019s classification. (<code>classification</code>)</li> <li>Merge any source metadata; if none is provided, record original_source in the submission metadata (<code>metadata</code>).</li> <li>Append all select_services to the submission\u2019s selected-services list to ensure proper routing. (<code>select_services</code>)</li> <li>If the input was a hash and the downloaded file\u2019s computed hash differs, rename the file to match its actual hash.</li> </ul>"},{"location":"installation/configuration/submission/#first-match-wins","title":"First-match wins","text":"<p>File sources are evaluated sequentially; once one successfully resolves the input, further sources are skipped.</p>"},{"location":"installation/configuration/submission/#submission-profiles","title":"Submission Profiles","text":"<p>You can configure the system to share pre-configured submission parameters to your users. This is useful for onboarding new users to the system because they would only need to specify the profile they'd like to use and provide a file or resource for analysis.</p> <p></p> <p>The system out-of-the-box includes three submission profiles to select from but these can be overridden by using the <code>submission.profiles</code> configuration.</p> <p>The \"Custom Analysis\" profile is reserved to users that have the <code>submission_customize</code> role.</p> <p>This role allows users to customize their submission as they see fit. Without that role, you're restricted based on the profile configurations set by the administrator.</p> <p></p>"},{"location":"installation/configuration/submission/#can-i-use-profiles-to-enforce-certain-parameters-to-be-set","title":"Can I use profiles to enforce certain parameters to be set?","text":"<p>Yes! As an administrator, you can specify exactly what parameters are preset for users and you can specify what parameters are allowed to be changed. This can be useful if you want to impose restrictions on certain users from making problematic submissions based on their parameters.</p> <p>For example, based on the following profile configuration:</p> <pre><code>submission:\n  ...\n  profiles:\n    # Only perform static analysis\n    - name: \"static\"\n      description: \"Analyze files using static analysis techniques and extract information from the file without executing it, such as metadata, strings, and structural information.\"\n      display_name: \"Static Analysis\"\n      classification: \"TLP:CLEAR//REL TO GROUPA\"\n      restricted_params:\n        submission: [\"classification\"]\n        Extract: [\"continue_after_extract\"]\n      params:\n        services:\n          selected: [\"Static Analysis\", \"Extract\"]\n          excluded: [\"Dynamic Analysis\"]\n        service_spec:\n          Extract:\n            password: infected\n</code></pre> <p>There is exactly one profile specified for the entire system and only members of <code>GROUPA</code> should have access to that profile.</p> <p>Within that profile, the members that don't have the <code>submission_customize</code> role aren't able to configure the submission's classification and provide a password to the Extract service but the service selection is fixed to the services in the \"Static Analysis\" category along with the Extract service (which sets <code>continue_after_extract: false</code> by default on all submissions under this profile).</p> <p>What happens with service parameters that are not specified in the profile?</p> <p>Any service parameters that aren't declared in the profile will default to values set by system administrators. These can be found by navigating to the Services menu and looking at the \"User Specified Parameters\" within the \"Parameters\" tab of a service you've selected.</p> <p>What this would look like in the UI for a limited user is:</p> <p>The following is based on the above example configuration</p> <ul> <li>The \"Dynamic Analysis\" category is omitted from Service Selection</li> <li><code>continue_after_extract</code> parameter for the Extract service has been removed</li> <li><code>password</code> parameter for the Extract service has a custom default set for the profile</li> <li>The classification picker is disabled because it's explicitly a restricted parameter that the user can't change within that profile.</li> </ul> <p></p>"},{"location":"installation/configuration/submission/#metadata-validation","title":"Metadata Validation","text":"<p>You can require certain metadata fields to be present in all submissions by defining them under <code>submission.metadata</code>.</p> <p>You can configure the system to enforce metadata validation and presence when performing ingestion and archiving. This is a useful feature if you're looking to harmonize the metadata from different sources under a common scheme.</p> <p>A lot of the configuration is around the parameters of the ODM fields that Assemblyline uses internally for it's own data validation, so an example of configuring a field using a regex pattern would look like:</p> <pre><code>validation_type: regex\nvalidation_params:\n  validation_regex: ^blee\n</code></pre> <p>So if you wanted to enforce the presence of a metadata field named <code>bloo</code> on submission and the value has to match that pattern, the configuration would be:</p> <pre><code>submission:\n  metadata:\n    submit: # Submission made using Submit API\n      bloo: # Field name\n        validation_type: regex\n        validation_params:\n          validation_regex: ^blee\n        required: true # Mandatory field to be set on submission\n</code></pre> <p>The configuration also supports applying strict metadata enforcement, which means a submitter can't add new metadata that the system isn't aware of relative to the scheme:</p> <pre><code>submission:\n  metadata:\n    ingest:\n      INGEST: # \"type\" parameter when using Ingest API (default: INGEST)\n        epoch:\n          validation_type: int\n        name:\n          validation_type: text\n    submit:\n      name:\n        validation_type: text\n  strict_schemes: [\"INGEST\"]\n</code></pre> <p>In the above example, if you're ingesting files under the <code>INGEST</code> type, then you can only set the <code>epoch</code> or <code>name</code> metadata. If there are any additional fields other than those two then the API will return an error. However, you are able to add additional metadata fields when using the Submit API but the <code>name</code> field still has to be a string/text type.</p> <p>A configuration that's specific to the Ingest API is the use of a <code>submission.metadata.ingest._default</code>. This configuration is used to apply baseline validation rules across anyone using the Ingest API, including those who might have their own validation scheme based on ingest type.</p> <p>For example, in the following:</p> <pre><code>submission:\n  metadata:\n    ingest:\n      _default:\n        owner:\n          validation_type: text\n          required: true\n      INGEST: # \"type\" parameter when using Ingest API (default: INGEST)\n        epoch:\n          validation_type: int\n        name:\n          validation_type: text\n</code></pre> <p>Someone who submits with <code>type: INGEST</code> has to have an <code>owner</code> field in their metadata of type <code>text</code> and can optionally provide an <code>epoch</code> and <code>name</code> meta which needs to be of the specified type. However, someone who submits with <code>type: TEST</code> only has to provide an <code>owner</code> field in the metadata for the validation to pass and for their submission to proceed to analysis.</p>"},{"location":"installation/configuration/submission/#can-i-combine-this-with-submission-profiles","title":"Can I combine this with Submission Profiles?","text":"<p>Yes! As the administrator of the system, you can configure profiles to have a fixed <code>type</code> parameter for submissions made over the Ingest API. This allows you to configure a set of profiles for an ingester and should the ingester choose a profile that performs metadata validation, they're forced to abide to the scheme you've set for them.</p> <p>For example, you may have the configuration:</p> <pre><code>submission:\n  metadata:\n    ingest:\n      _default:\n        owner:\n          validation_type: text\n          required: true\n      data_collection:\n        authorization:\n          validation_type: text\n          required: true\n  profiles:\n    - name: \"data_collection_profile\"\n      description: \"Profile used for data collection that is tied to metadata validation where \"authorization\" must be provided in the metadata.\"\n      display_name: \"Data Collection\"\n      classification: \"TLP:CLEAR//REL TO COLLECTORS\"\n      params:\n        type: \"data_collection\"\n</code></pre> <p>This configuration indicates that you've shared a profile called \"Data Collection\" to members of the <code>COLLECTORS</code> group which presets the <code>type</code> parameter of their submissions to <code>data_collection</code> which is associated to a metadata validation scheme that you have configured.</p> <p>From the perspective of the user, if they're submitting using that profile they have to provide the necessary metadata (along with the resource to perform analysis) otherwise their submission will be rejected.</p>"},{"location":"installation/configuration/system/","title":"System section","text":""},{"location":"installation/configuration/system/#system-section","title":"System section","text":"<p>The system configuration section (<code>system:</code>) of the configuration file allows you to modify the parameters of your system.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> System section configuration example <pre><code>...\nsystem:\n  # Path path to the constants module\n  constants: assemblyline.common.constants\n\n  # Organisation Name\n  organisation: ACME\n\n  # Type of system. If different then production, watermark will be shown in the UI\n  type: production\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/ui/","title":"User Interface section","text":""},{"location":"installation/configuration/ui/#user-interface-section","title":"User Interface section","text":"<p>The user interface configuration section (<code>ui:</code>) of the configuration file allows you to modify the parameters of the user interface and API Server.</p> <p>Since this section is quite simple, we will list the default configuration at the same time as we describe the different values.</p> User interface section configuration example <pre><code>...\nui:\n  # AI Integration configuration block\n  ai:\n    # URL to the chat completion API, you can change this to your own if you are not using the default\n    #  OpenAI endpoints.\n    chat_url: https://api.openai.com/v1/chat/completions\n\n    # Configuration of the code analysis feature\n    code:\n      # System message sent to the OpenAI API describing how OpenAI should interprete the messages received\n      system_message: \"...\"\n      # Maximum number of tokens returned as a response by the OpenAI API\n      max_tokens: 512\n      # Other optional parameters sent to the API\n      options:\n        frequency_penalty: 0\n        presence_penalty: 0\n        temperature: 1\n        top_p: 1\n\n    # Configuration of the Detailed AL report analysis (used in hybrid reporting)\n    #     Same type of configuration block as the code analysis\n    detailed_report: {...}\n\n    # Configuration of the Executive Summary analysis (used in sumission and file detail views)\n    #     Same type of configuration block as the code analysis\n    executive_summary: {...}\n\n    # Enabled/disable AI integration\n    enabled: False\n\n    # Headers sent to the OpenAI API\n    headers:\n        Content-Type: \"application/json\"\n\n    # Model used for the AI Integration\n    model_name: \"gpt-3.5-turbo\"\n\n    # Should the SSL cert to the OpenAI API endpoint be verified?\n    verify: True\n\n  # Show the malicious hinting checkbox when submitting files\n  allow_malicious_hinting: false\n\n  # Allow malicious files to be download in raw format\n  allow_raw_downloads: true\n\n  # Allow URL submissions to be processed in the system\n  allow_url_submissions: true\n\n  # Audit API queries\n  audit: true\n\n  # String to be displayed in the banner\n  banner: null\n\n  # Color of the banner (info, success, error, warning)\n  banner_level: info\n\n  # Turn on/off  debug mode\n  debug: false\n\n  # Default encoding for downloaded files\n  download_encoding: cart\n\n  # Email address users can reach the admins at\n  email: null\n\n  # Enforce API and submissions quotas or not\n  enforce_quota: true\n\n  # Domain for your deployment (Especially important for kubernetes deployments)\n  fqdn: localhost\n\n  # Maximum submission priority for ingestion tasks\n  ingest_max_priority: 250\n\n  # Make the UI read only\n  # (Not supported in the new UI yet)\n  read_only: false\n\n  # Time offset for queries done in raed only mode\n  # (Not supported in the new UI yet)\n  read_only_offset: ''\n\n  # Secret key for your flask app (API)\n  # You should definitely change this!\n  secret_key: This is the default flask secret key... you should change this!\n\n  # Timeout after which a stale session is no longer valid\n  session_duration: 3600\n\n  # Fields to generate statistics on\n  statistics:\n    # Statistics in the alert view\n    alert:\n    - al.attrib\n    - al.av\n    - al.behavior\n    - al.domain\n    - al.ip\n    - al.yara\n    - file.name\n    - file.md5\n    - owner\n\n    # Statistics in the submission view\n    submission:\n    - params.submitter\n\n  # Terms of service for the deployment (in markdown format)\n  tos: null\n\n  # Lockout the user after they agree to the terms of service\n  # (requires an admin to enable their account)\n  tos_lockout: false\n\n  # List of email addresses to notify when a user agreed to the TOS\n  # and its account is locked out\n  tos_lockout_notify: null\n\n  # Headers added to fetch the files during URL submissions\n  url_submission_headers: {}\n\n  # Proxy configuration to use while fetching the file during URL submissions\n  url_submission_proxies: {}\n\n  # Should we validate that the session comes from the same IP?\n  validate_session_ip: true\n\n  # Should we validate that the session uses the same user agent\n  validate_session_useragent: true\n...\n</code></pre> <p>Tip</p> <p>Refer to the changing the configuration file documentation for more detail on where and how to change the configuration of the system.</p>"},{"location":"installation/configuration/kubernetes/internal_encryption/","title":"Internal Encryption (TLS/SSL)","text":""},{"location":"installation/configuration/kubernetes/internal_encryption/#internal-encryption-tlsssl","title":"Internal Encryption (TLS/SSL)","text":"<p>If enabling this feature post-deployment, consider pausing the system before proceeding to upgrade</p> <p>As of 4.3.1.0, the Assemblyline Helm Chart supports enabling end-to-end encryption internally between components.</p> <p>The flag <code>enableInternalEncryption</code> found in the values.yaml is disabled by default (as to not conflict with pre-existing deployments) but can be enabled. Enabling the feature causes Helm to generate a Root CA for the release (<code>{ .Release.Name }.internal-generated-ca</code>) that's only used to sign certificates for servers in Assemblyline.</p>"},{"location":"installation/configuration/kubernetes/internal_encryption/#changes-for-other-helm-charts","title":"Changes for other Helm charts","text":"<p>Enabling this feature will require some changes to the values used by the accompanying charts that we use.</p> <p>For example, to enable the feature with the default values using Helm-generated Root CA for verification:</p> <p>Generated certificates are stored in Secrets as <code>&lt;server_hostname&gt;-cert</code></p> <p>Each Secret has two keys: <code>tls.crt</code>, <code>tls.key</code> which has their respective public certificate and private key</p> ElasticsearchKibanaMinIO <p>In this example, I'll mention the changes to the <code>datastore</code> but this will also apply to <code>log-storage</code> when <code>seperateInternalELKStack: true</code></p> 7.17.38+ <pre><code>datastore:\n  # Change of protocol for readiness probes\n  protocol: https\n\n  # By default, we configure Elasticsearch instances (including the log-storage) to use\n  # the contents of '/usr/share/elasticsearch/config/http_ssl/' when settings up HTTPS\n  # This can be overridden by setting the environment variables directly, see values.yaml for more details\n  extraVolumes: |\n    - name: elastic-certificates\n      emptyDir: {}\n    - name: datastore-master-cert\n      secret:\n        secretName: datastore-master-cert\n  extraVolumeMounts: |\n    - name: elastic-certificates\n      mountPath: /usr/share/elasticsearch/config/certs\n    - name: datastore-master-cert\n      mountPath: /usr/share/elasticsearch/config/http_ssl\n      readOnly: true\n</code></pre> <pre><code>datastore:\n  # Change of protocol for readiness probes\n  protocol: https\n  esConfig:\n    elasticsearch.yml: |\n      ingest.geoip.downloader.enabled: false\n      logger.level: WARN\n      xpack.security.enabled: true\n      xpack.security.transport.ssl.enabled: true\n      xpack.security.transport.ssl.verification_mode: certificate\n      xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12\n      xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12\n      xpack.security.http.ssl.enabled: ${DATASTORE_SSL_ENABLED}\n      xpack.security.http.ssl.verification_mode: full\n      xpack.security.http.ssl.key: ${DATASTORE_SSL_KEY}\n      xpack.security.http.ssl.certificate: ${DATASTORE_SSL_CERTIFICIATE}\n\n  # By default, we configure Elasticsearch instances (including the log-storage) to use\n  # the contents of '/usr/share/elasticsearch/config/http_ssl/' when settings up HTTPS\n  # This can be overridden by setting the environment variables directly, see values.yaml for more details\n  extraVolumes: |\n    - name: elastic-certificates\n      emptyDir: {}\n    - name: datastore-master-cert\n      secret:\n        secretName: datastore-master-cert\n  extraVolumeMounts: |\n    - name: elastic-certificates\n      mountPath: /usr/share/elasticsearch/config/certs\n    - name: datastore-master-cert\n      mountPath: /usr/share/elasticsearch/config/http_ssl\n      readOnly: true\n</code></pre> 7.17.38+ <pre><code># Internally hosted Kibana instance. For more details, see: https://github.com/elastic/helm-charts/tree/7.17/kibana#configuration\nkibanaHost: https://kibana/kibana\nkibana:\n  elasticsearchHosts: https://log-storage-master:9200\n\n  # For Filebeat, Metricbeat, APM, and Kibana, we mount the Root CA in '/etc/certs/ca.crt'\n  # This can be overridden by replacing the ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES environment variable\n  kibanaConfig:\n    kibana.yml: |\n      server.ssl:\n        enabled: true\n        certificate: /etc/certs/kibana/tls.crt\n        key: /etc/certs/kibana/tls.key\n        certificateAuthorities: /etc/certs/ca.crt\n      elasticsearch:\n        hosts: ['${ELASTICSEARCH_HOSTS}']\n        username: ${ELASTICSEARCH_USERNAME}\n        password: ${ELASTICSEARCH_PASSWORD}\n  extraVolumes:\n    - name: root-ca\n      secret:\n        secretName: \"&lt;release_name&gt;.internal-generated-ca\"\n    - name: kibana-cert\n      secret:\n        secretName: kibana-cert\n  extraVolumeMounts:\n    - name: root-ca\n      mountPath: /etc/certs/ca.crt\n      subPath: tls.crt\n    - name: kibana-cert\n      mountPath: /etc/certs/kibana/\n      readOnly: true\n</code></pre> <pre><code># Internally hosted Kibana instance. For more details, see: https://github.com/elastic/helm-charts/tree/7.17/kibana#configuration\nkibanaHost: https://kibana/kibana\nkibana:\n  elasticsearchHosts: https://log-storage-master:9200\n\n  # For Filebeat, Metricbeat, APM, and Kibana, we mount the Root CA in '/etc/certs/ca.crt'\n  # This can be overridden by replacing the ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES environment variable\n  extraVolumes:\n    - name: init-script\n      configMap:\n        name: init-kibana-token\n    - name: root-ca\n      secret:\n        secretName: \"&lt;release_name&gt;.internal-generated-ca\"\n  extraVolumeMounts:\n    - name: root-ca\n      mountPath: /etc/certs/ca.crt\n      subPath: tls.crt\n  kibanaConfig:\n    kibana.yml: |\n      server.ssl:\n        enabled: true\n        certificate: /etc/certs/kibana/tls.crt\n        key: /etc/certs/kibana/tls.key\n        certificateAuthorities: /etc/certs/ca.crt\n      elasticsearch:\n        hosts: ['${ELASTICSEARCH_HOSTS}']\n        serviceAccountToken: '${ELASTICSEARCH_SERVICEACCOUNTTOKEN}'\n        ssl.certificateAuthorities: '${ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES:[]}'\n  extraVolumes:\n    - name: root-ca\n      secret:\n        secretName: \"&lt;release_name&gt;.internal-generated-ca\"\n    - name: kibana-cert\n      secret:\n        secretName: kibana-cert\n  extraVolumeMounts:\n    - name: root-ca\n      mountPath: /etc/certs/ca.crt\n      subPath: tls.crt\n    - name: kibana-cert\n      mountPath: /etc/certs/kibana/\n      readOnly: true\n</code></pre> <pre><code># Internally hosted MinIO filestore. For more details, see: https://github.com/minio/charts/tree/main/minio#configuration\nfilestore:\n  tls:\n    enabled: true\n    certSecret: filestore-cert\n    publicCrt: tls.crt\n    privateKey: tls.key\n</code></pre>"},{"location":"installation/configuration/kubernetes/internal_encryption/#changes-to-assemblyline-configuration","title":"Changes to Assemblyline Configuration","text":"<p>Since we're enabling HTTPS on all internal endpoints, this means we also have to update the URLs that we use for connecting to the various databases.</p> <p>This includes, but is not limited to:</p> <ul> <li><code>configuration.metrics.apm_server.server_url: http \u2192 https</code></li> <li><code>configuration.metrics.elasticsearch.hosts: http \u2192 https (for all hosts)</code></li> <li><code>configuration.datastore.hosts: http \u2192 https (for all hosts)</code></li> <li><code>configuration.filestore.*: use_ssl=False \u2192 use_ssl=True (for all hosts)</code></li> </ul> <p>Filestore Configurations</p> <p>When enabling <code>internalEncryption</code> and if using the self-generated Root CA for verification, you'll need to set the <code>verify</code> parameter to point to the location of the CA on disk (<code>/etc/assemblyline/ssl/al_root-ca.crt</code>.)</p> <p>If the CA required for verification isn't the one generated by Helm, then you'll need to mount that CA to the core containers using <code>coreVolumes</code> &amp; <code>coreMounts</code>. You'll also need to add the mounts to <code>configuration.core.scaler.service_defaults.mounts</code> so privileged services will have access to the CA for verification.</p>"},{"location":"installation/configuration/kubernetes/internal_encryption/#changes-to-ingress-controller","title":"Changes to Ingress Controller","text":"<p>Because we're enabling full encryption to everything Assemblyline-related, this also includes traffic going from the ingress controller to the appropriate services. (ie. '/' \u2192 <code>frontend</code>, '/api/' \u2192 <code>ui</code>, etc.)</p> <p>A simple way of doing this is setting the <code>ingressAnnotation</code> in your values.yaml to force the ingress controller to forward the request over HTTPS rather than HTTP.</p> <p>In microk8s, using their built-in ingress, you'd have to add:</p> <p><code>nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"</code></p> <p>However, this particular annotation doesn't work for all ingress controllers, so you'll need to refer to the respective documentation to find the right annotation configuration. For example, the annotation for NGINX installed via <code>helm</code> would be:</p> <p><code>nginx.org/ssl-services: \"ui,frontend,kibana,socketio\"</code>.</p>"},{"location":"installation/configuration/kubernetes/internal_encryption/#what-does-this-look-like-in-practice","title":"What does this look like in practice?","text":"<p>For example this is what your changes would look like if:</p> <ul> <li>Enabling internal encryption (<code>enableInternalEncryption: true</code>)</li> <li>Using an internal minIO filestore that the chart deploys  (<code>internalFilestore: true</code>)</li> <li>Using the helm-generated CA for the various different servers within the cluster (<code>datastore</code> and/or <code>log-storage</code>, <code>kibana</code>, <code>filestore</code>)</li> </ul> <p>Configuration depends on the version of the Assemblyline helm chart used in your deployment</p> 4.2.04.3.0 <pre><code>- enableInternalEncryption: false\n+ enableInternalEncryption: true\n\n  internalFilestore: true\n\n  configuration:\n    core:\n      ...\n      metrics:\n        apm_server:\n-         server_url: \"http://apm:8200\"\n+         server_url: \"https://apm:8200\"\n        elasticsearch:\n          hosts:\n-           [\"http://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@${LOGGING_HOST}\"]\n+           [\"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@${LOGGING_HOST}\"]\n    datastore:\n      ...\n-     hosts: [\"http://elastic:${ELASTIC_PASSWORD}@datastore-master:9200\"]\n+     hosts: [\"https://elastic:${ELASTIC_PASSWORD}@datastore-master:9200\"]\n    filestore:\n      ...\n      cache:\n        [\n-         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&amp;use_ssl=False\",\n+         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&amp;use_ssl=True&amp;verify=/etc/assemblyline/ssl/al_root-ca.crt\",\n        ]\n      storage:\n        [\n-         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&amp;use_ssl=False\",\n+         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&amp;use_ssl=True&amp;verify=/etc/assemblyline/ssl/al_root-ca.crt\",\n        ]\n  ...\n\n# Equivalent changes for `datastore` would be made for `log-storage`\n  datastore:\n+   protocol: https\n    ...\n    extraVolumes: |\n      - name: elastic-certificates\n        emptyDir: {}\n+     - name: datastore-master-cert\n+       secret:\n+         secretName: datastore-master-cert\n    extraVolumeMounts: |\n      - name: elastic-certificates\n        mountPath: /usr/share/elasticsearch/config/certs\n+     - name: datastore-master-cert\n+       mountPath: /usr/share/elasticsearch/config/http_ssl\n+       readOnly: true\n  ...\n\n- kibanaHost: http://kibana/kibana\n+ kibanaHost: https://kibana/kibana\n  kibana:\n-   elasticsearchHosts: http://log-storage-master:9200\n+   elasticsearchHosts: https://log-storage-master:9200\n    ...\n+   kibanaConfig:\n+     kibana.yml: |\n+       server.ssl:\n+         enabled: true\n+         certificate: /etc/certs/kibana/tls.crt\n+         key: /etc/certs/kibana/tls.key\n+         certificateAuthorities: /etc/certs/ca.crt\n+       elasticsearch:\n+         hosts: ['${ELASTICSEARCH_HOSTS}']\n+         username: ${ELASTICSEARCH_USERNAME}\n+         password: ${ELASTICSEARCH_PASSWORD}\n+   extraVolumes:\n+     - name: root-ca\n+       secret:\n+         secretName: \"&lt;release_name&gt;.internal-generated-ca\"\n+     - name: kibana-cert\n+       secret:\n+         secretName: kibana-cert\n+   extraVolumeMounts:\n+     - name: root-ca\n+       mountPath: /etc/certs/ca.crt\n+       subPath: tls.crt\n+     - name: kibana-cert\n+       mountPath: /etc/certs/kibana/\n+       readOnly: true\n  ...\n\n  filestore:\n    ...\n+   tls:\n+     enabled: true\n+     certSecret: filestore-cert\n+     publicCrt: tls.crt\n+     privateKey: tls.key\n</code></pre> <pre><code>- enableInternalEncryption: false\n+ enableInternalEncryption: true\n\n  internalFilestore: true\n\n  configuration:\n    core:\n      ...\n      metrics:\n        apm_server:\n-         server_url: \"http://apm:8200\"\n+         server_url: \"https://apm:8200\"\n        elasticsearch:\n          hosts:\n-           [\"http://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@${LOGGING_HOST}\"]\n+           [\"https://${LOGGING_USERNAME}:${LOGGING_PASSWORD}@${LOGGING_HOST}\"]\n    datastore:\n      ...\n-     hosts: [\"http://elastic:${ELASTIC_PASSWORD}@datastore-master:9200\"]\n+     hosts: [\"https://elastic:${ELASTIC_PASSWORD}@datastore-master:9200\"]\n    filestore:\n      ...\n      cache:\n        [\n-         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&amp;use_ssl=False\",\n+         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-cache&amp;use_ssl=True&amp;verify=/etc/assemblyline/ssl/al_root-ca.crt\",\n        ]\n      storage:\n        [\n-         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&amp;use_ssl=False\",\n+         \"s3://${INTERNAL_FILESTORE_ACCESS}:${INTERNAL_FILESTORE_KEY}@filestore:9000?s3_bucket=al-storage&amp;use_ssl=True&amp;verify=/etc/assemblyline/ssl/al_root-ca.crt\",\n        ]\n  ...\n\n# Equivalent changes for `datastore` would be made for `log-storage`\n  datastore:\n+   protocol: https\n+   esConfig:\n+     elasticsearch.yml: |\n+       ingest.geoip.downloader.enabled: false\n+       logger.level: WARN\n+       xpack.security.enabled: true\n+       xpack.security.transport.ssl.enabled: true\n+       xpack.security.transport.ssl.verification_mode: certificate\n+       xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12\n+       xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12\n+       xpack.security.http.ssl.enabled: ${DATASTORE_SSL_ENABLED}\n+       xpack.security.http.ssl.verification_mode: full\n+       xpack.security.http.ssl.key: ${DATASTORE_SSL_KEY}\n+       xpack.security.http.ssl.certificate: ${DATASTORE_SSL_CERTIFICIATE}\n    ...\n    extraVolumes: |\n      - name: elastic-certificates\n        emptyDir: {}\n+     - name: datastore-master-cert\n+       secret:\n+         secretName: datastore-master-cert\n    extraVolumeMounts: |\n      - name: elastic-certificates\n        mountPath: /usr/share/elasticsearch/config/certs\n+     - name: datastore-master-cert\n+       mountPath: /usr/share/elasticsearch/config/http_ssl\n+       readOnly: true\n  ...\n\n- kibanaHost: http://kibana/kibana\n+ kibanaHost: https://kibana/kibana\n  kibana:\n-   elasticsearchHosts: http://log-storage-master:9200\n+   elasticsearchHosts: https://log-storage-master:9200\n    ...\n+   kibanaConfig:\n+     kibana.yml: |\n+       server.ssl:\n+         enabled: true\n+         certificate: /etc/certs/kibana/tls.crt\n+         key: /etc/certs/kibana/tls.key\n+         certificateAuthorities: /etc/certs/ca.crt\n+       elasticsearch:\n+         hosts: ['${ELASTICSEARCH_HOSTS}']\n+         serviceAccountToken: '${ELASTICSEARCH_SERVICEACCOUNTTOKEN}'\n+         ssl.certificateAuthorities: '${ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES:[]}'\n+   extraVolumes:\n+     - name: init-script\n+       configMap:\n+         name: init-kibana-token\n+     - name: root-ca\n+       secret:\n+         secretName: \"&lt;release_name&gt;.internal-generated-ca\"\n+     - name: kibana-cert\n+       secret:\n+         secretName: kibana-cert\n+   extraVolumeMounts:\n+     - name: root-ca\n+       mountPath: /etc/certs/ca.crt\n+       subPath: tls.crt\n+     - name: kibana-cert\n+       mountPath: /etc/certs/kibana/\n+       readOnly: true\n  ...\n\n  filestore:\n    ...\n+   tls:\n+     enabled: true\n+     certSecret: filestore-cert\n+     publicCrt: tls.crt\n+     privateKey: tls.key\n</code></pre>"},{"location":"installation/configuration/kubernetes/upgrading_elastic/","title":"Upgrading to Elasticsearch 8","text":"<p>As of version 4.3.0 of the Assemblyline helm chart, we'll start using Elasticsearch 8 for the datastore and log-storage, if enabled.</p> <p>To migrate your system to the newest version of the chart and Elasticsearch, take note of the following before performing the upgrade:</p>"},{"location":"installation/configuration/kubernetes/upgrading_elastic/#elasticsearch-datastorelog-storage","title":"Elasticsearch (datastore/log-storage)","text":"<p>If you're using an exact copy-and-paste of the <code>values.yaml</code> with some modifications, note the following (<code>datastore</code> will be used as an example):</p> <p>Upgrading <code>log-storage</code> configuration for internal TLS/SSL involves different environment variables (<code>DATASTORE_*</code> \u2192 <code>LOGGING_*</code>)</p> <pre><code>datastore:\n  imageTag: \"8.10.2\" # Tag statically set to the latest version of ES at the time of writing\n  createCert: False # Prevent the chart from creating it's own certificates\n  roles: # Assign the specific roles required for database\n    - master\n    - data\n    - data_content\n    - data_hot\n    - data_warm\n    - data_cold\n    # Only if using Elasticsearch instance for logging\n    # - ingest\n  secret:\n    enabled: false  # Disable auto-password generation\n  esConfig:\n    elasticsearch.yml: |\n      ...\n      # Suppress warning logs from certain loggers\n      logger.org.elasticsearch.cluster.coordination.ClusterBootstrapService: ERROR\n      logger.com.amazonaws.auth.profile.internal.BasicProfileConfigFileLoader: ERROR\n      logger.org.elasticsearch.cluster.coordination.Coordinator: ERROR\n      logger.org.elasticsearch.deprecation: ERROR\n      logger.org.elasticsearch.xpack.ml.inference.loadingservice.ModelLoadingService: ERROR\n\n      # Don't create a deprecation index\n      cluster.deprecation_indexing.enabled: false\n\n      # Below configurations are used when `enableInternalEncryption: true`\n      # xpack.security.http.ssl.enabled: ${DATASTORE_SSL_ENABLED}\n      # xpack.security.http.ssl.verification_mode: full\n      # xpack.security.http.ssl.key: ${DATASTORE_SSL_KEY}\n      # xpack.security.http.ssl.certificate: ${DATASTORE_SSL_CERTIFICIATE}\n</code></pre>"},{"location":"installation/configuration/kubernetes/upgrading_elastic/#kibana","title":"Kibana","text":"<p>As of Elastic v8, Kibana isn't allowed to use the local superuser to authenticate with the cluster. As a result, we've created a job that will generate a service account token that Kibana will use instead.</p> <p>This will involve creating a new secret in the Assemblyline namespace:</p> <pre><code># Initalizes secret with a temporary value, will be replaced by job upon helm (install|upgrade)\napiVersion: v1\nkind: Secret\nmetadata:\n    name: kibana-service-token\nstringData:\n    token: \"\"\n</code></pre> <p>Using the following command:</p> <pre><code>kubectl create secret generic kibana-service-token --from-literal=token=\"\" -n &lt;al_ns&gt;\n</code></pre> <p>As well as the following changes to the Kibana chart values:</p> <pre><code>kibana:\n...\nextraEnvs:\n  - name: ELASTICSEARCH_SERVICEACCOUNTTOKEN\n    valueFrom:\n      secretKeyRef:\n        name: kibana-service-token\n        key: token\nkibanaConfig:\n  kibana.yml: |\n    elasticsearch:\n      hosts: ['${ELASTICSEARCH_HOSTS}']\n      serviceAccountToken: '${ELASTICSEARCH_SERVICEACCOUNTTOKEN}'\n      # ssl.certificate: '${ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES}'\n</code></pre> <p>No changes should be required for Beats, Logstash, and APM server</p>"},{"location":"installation/configuration/kubernetes/upgrading_elastic/#upgrade-checklist","title":"Upgrade Checklist","text":"<ul> <li> (Optional) Pause system processing using the toggles on the Ingestion &amp; Dispatch cards in the Dashboard view</li> <li> Update Elasticsearch configuration(s) in <code>values.yaml</code></li> <li> Create a new Secret for Kibana's service token with <code>\"\"</code> value</li> <li> Update Kibana configuration in <code>values.yaml</code></li> </ul> <p>If all these items are checked off, you can proceed with a <code>helm upgrade</code> of your deployment.</p>"},{"location":"integration/api_exercise_1/","title":"Collecting Network IoCs","text":""},{"location":"integration/api_exercise_1/#collecting-network-iocs","title":"Collecting Network IoCs","text":""},{"location":"integration/api_exercise_1/#scenario","title":"Scenario","text":"<p>\u201cI want to collect all the network-related IoCs that Assemblyline was able to extract and store them in a dictionary/mapping. For my use case, I would also want to sort them based on the type of network IoC (ie. Domain, IP, URL)\u201d</p>"},{"location":"integration/api_exercise_1/#expected-results","title":"Expected Results","text":"<pre><code>{\n    \u201cnetwork.static.ip\u201d: [\u201c172.0.0.1\u201d, ...]\n    \u201cnetwork.static.domain\u201d: [\u201cwww.google.com\u201d, ...]\n    ...\n}\n</code></pre>"},{"location":"integration/api_exercise_1/#apis-involved","title":"APIs Involved","text":"RESTPython <pre><code>GET /api/v4/submission/summary/&lt;sid&gt;/\nGET /api/v4/ontology/submission/&lt;sid&gt;/\n</code></pre> <pre><code>Client.submission.summary(&lt;sid&gt;)\nClient.ontology.submission(&lt;sid&gt;)\n</code></pre>"},{"location":"integration/api_exercise_1/#solutions","title":"Solutions","text":"Using Submission APIUsing Ontology API Python: Using <code>assemblyline_client</code>Python: Using <code>requests</code> <pre><code>import os\nfrom pprint import pprint\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n\n# Choose a submission ID that we will use to pull IOCs from\nSID = '&lt;sid&gt;'\n\n# The result of this exercise will be stored in this variable\nCOLLECTED_IOCS = dict()\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n# client.submission.summary(&lt;sid&gt;) --&gt; /api/v4/submission/summary/&lt;sid&gt;/\nfor tag_name, tag_values in client.submission.summary(SID)['tags']['ioc'].items():\n    for tag_value, tag_verdict, is_tag_safelisted, classification in tag_values:\n        if tag_name.startswith('network'):\n            # Create the tag category if does not exist\n            COLLECTED_IOCS.setdefault(tag_name, [])\n\n            # Add the IOC to our list of collected IOCs\n            COLLECTED_IOCS[tag_name].append(tag_value)\n\n\n# Now that we have gathered the IOCs, let's print them to the screen\npprint(COLLECTED_IOCS)\n</code></pre> <pre><code>import requests\nimport json\nimport os\nfrom pprint import pprint\n\nheaders = {\n    \"x-user\": os.getenv('AL_USER', '&lt;AL user&gt;'),\n    \"x-apikey\": os.getenv('AL_APIKEY', '&lt;AL API key&gt;'),\n    \"accept\": \"application/json\"\n}\n\n# Choose a submission ID that we will use to pull IOCs from\nSID = '&lt;sid&gt;'\n\n# The result of this exercise will be stored in this variable\nCOLLECTED_IOCS = dict()\n\n# This is the connection to the Assemblyline client that we will use\nhost = f\"https://{os.getenv('AL_HOST', '&lt;AL URL&gt;')}:443\"\n\n# client.submission.summary(&lt;sid&gt;) --&gt; /api/v4/submission/summary/&lt;sid&gt;/\ndata = requests.get(f\"{host}/api/v4/submission/summary/{SID}/\", headers=headers, verify=False).content\nsummary = json.loads(data)[\"api_response\"]\nfor tag_name, tag_values in summary[\"tags\"][\"ioc\"].items():\n    for tag_value, tag_verdict, is_tag_safelisted, classification in tag_values:\n        if tag_name.startswith('network'):\n            # Create the tag category if does not exist\n            COLLECTED_IOCS.setdefault(tag_name, [])\n\n            # Add the IOC to our list of collected IOCs\n            COLLECTED_IOCS[tag_name].append(tag_value)\n\n# Now that we have gathered the IOCs, let's print them to the screen\npprint(COLLECTED_IOCS)\n</code></pre> Python: Using <code>assemblyline_client</code>Python: Using <code>requests</code> <pre><code>import os\nfrom pprint import pprint\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n\n# Choose a submission ID that we will use to pull IOCs from\nSID = '&lt;sid&gt;'\n\n# The result of this exercise will be stored in this variable\nCOLLECTED_IOCS = dict()\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n# client.ontology.submission(&lt;sid&gt;) --&gt; /api/v4/ontology/submission/&lt;sid&gt;/\nfor record in client.ontology.submission(SID):\n    for tag_name, tag_values in record['results']['tags'].items():\n        if tag_name.startswith('network'):\n            # Create the tag category if does not exist\n            COLLECTED_IOCS.setdefault(tag_name, [])\n\n            # Add the IOC to our list of collected IOCs\n            COLLECTED_IOCS[tag_name].extend(tag_values)\n\n# Now that we have gathered the IOCs, let's print them to the screen\npprint(COLLECTED_IOCS)\n</code></pre> <pre><code>import requests\nimport json\nimport os\nfrom pprint import pprint\n\nheaders = {\n    \"x-user\": os.getenv('AL_USER', '&lt;AL user&gt;'),\n    \"x-apikey\": os.getenv('AL_APIKEY', '&lt;AL API key&gt;'),\n    \"accept\": \"application/json\"\n}\n\n# Choose a submission ID that we will use to pull IOCs from\nSID = '&lt;sid&gt;'\n\n# The result of this exercise will be stored in this variable\nCOLLECTED_IOCS = dict()\n\n# This is the connection to the Assemblyline client that we will use\nhost = f\"https://{os.getenv('AL_HOST', '&lt;AL URL&gt;')}:443\"\n\n# Option 2: Get IOCs from the ontology API\n# client.ontology.submission(&lt;sid&gt;) --&gt; /api/v4/ontology/submission/&lt;sid&gt;/\ndata = requests.get(f\"{host}/api/v4/ontology/submission/{SID}/\", headers=headers, verify=False).content\nontology = [json.loads(line) for line in data.splitlines()]\nfor record in ontology:\n    for tag_name, tag_values in record['results']['tags'].items():\n        if tag_name.startswith('network'):\n            # Create the tag category if does not exist\n            COLLECTED_IOCS.setdefault(tag_name, [])\n\n            # Add the IOC to our list of collected IOCs\n            COLLECTED_IOCS[tag_name].extend(tag_values)\n\n# Now that we have gathered the IOCs, let's print them to the screen\npprint(COLLECTED_IOCS)\n</code></pre>"},{"location":"integration/api_exercise_2/","title":"Performing Filtered File Collection","text":""},{"location":"integration/api_exercise_2/#performing-filtered-file-collection","title":"Performing Filtered File Collection","text":""},{"location":"integration/api_exercise_2/#scenario","title":"Scenario","text":"<p>\u201cI want to collect all files with a very high score in Assemblyline (score \u2265 7000). I would like to also store these files on my antivirus-protected host so that I can feed them into another process.\u201d</p> <p>This exercise has the potential to iterate through a lot of data.</p> <p>If we want to scan through all the contents in a Search API result without circling back on data we\u2019ve already seen, we need to use a pointer.</p> <p>You have to keep in mind the number of items the API can return per search request is bound to the number of documents Elasticsearch can return (10K) per request, so you won\u2019t be able to get everything in a single request if there\u2019s a lot of items to return.</p> <p>This is where the idea of paging comes in and the Search API already gives you a <code>paging_id</code> that you can use in a later request to pick up from where you left off. Similar to what a bookmark does when reading a book.</p> <p>Does this sound complex? It is, which is why we will be performing a \u201cstream search\u201d through the Assemblyline client rather than attempting to manage <code>paging_id</code> values with a native network request library:</p> <pre><code>Client.search.stream.&lt;index&gt;()\n</code></pre>"},{"location":"integration/api_exercise_2/#expected-results","title":"Expected Results","text":"<pre><code>A directory containing downloaded files that scored &gt;= 7000\n\nThe downloaded files should have cART-encoding so that they do not trigger AV\n</code></pre>"},{"location":"integration/api_exercise_2/#apis-involved","title":"APIs Involved","text":"RESTPython <pre><code>GET /api/v4/search/&lt;index&gt;/\nGET /api/v4/submission/full/&lt;sid&gt;/\nGET /api/v4/file/download/&lt;sha256&gt;/\n</code></pre> <pre><code>Client.search.stream.&lt;index&gt;()\nClient.submission.full(&lt;sid&gt;)\nClient.file.download(&lt;sha256&gt;)\n</code></pre>"},{"location":"integration/api_exercise_2/#solutions","title":"Solutions","text":"Python: Using <code>assemblyline_client</code>Python: Using <code>requests</code> <pre><code>import os\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n# Filter for files within a submission that exceed a certain score\nFILE_SCORE_THRESHOLD = 7000\n\n# Download files that meet the FILE_SCORE_THRESHOLD into this directory\nOUTPUT_DIRECTORY = '/tmp/ex2'\nos.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n# For all submissions that are over the file score threshold\n# client.search.stream.submission --&gt; /api/v4/search/submission/?deep_paging_id=*\nfor record in client.search.stream.submission(query=f\"max_score:&gt;={FILE_SCORE_THRESHOLD}\", fl='sid'):\n    sid = record['sid']\n\n    # Download the full submission result and compute the score for each file\n    # client.submission.full(&lt;sid&gt;) --&gt; /api/v4/submission/full/&lt;sid&gt;/\n    submission_results = client.submission.full(sid)\n\n    # Compute the score of each files in the submission\n    files_scores = dict()\n    for result in submission_results['results'].values():\n        # Initialize the default score for the file if the file is not in the list\n        files_scores.setdefault(result['sha256'], 0)\n\n        # Add the score of the result record to the file\n        files_scores[result['sha256']] += result['result']['score']\n\n    # For each files where the score is greater than threshold, download in cARTed format\n    # client.file.download --&gt; /api/v4/file/download/sha256?encoding=cart/\n    for sha256, score in files_scores.items():\n        if score &gt;= FILE_SCORE_THRESHOLD:\n            client.file.download(sha256, encoding=\"cart\", output=os.path.join(OUTPUT_DIRECTORY, f\"{sha256}.cart\"))\n\nprint(os.listdir(OUTPUT_DIRECTORY))\n</code></pre> <pre><code>import requests\nimport os\n\nheaders = {\n    \"x-user\": os.getenv('AL_USER', '&lt;AL user&gt;'),\n    \"x-apikey\": os.getenv('AL_APIKEY', '&lt;AL API key&gt;'),\n    \"accept\": \"application/json\"\n}\n\n# Filter for files within a submission that exceed a certain score\nFILE_SCORE_THRESHOLD = 7000\n\n# Download files that meet the FILE_SCORE_THRESHOLD into this directory\nOUTPUT_DIRECTORY = '/tmp/ex2'\nos.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n\n# This is the connection to the Assemblyline client that we will use\nhost = f\"https://{os.getenv('AL_HOST', '&lt;AL URL&gt;')}:443\"\n\n# For all submissions that are over the file score threshold\n# client.search.stream.submission --&gt; /api/v4/search/submission/\n# ** NOTE: this works for the purpose of this exercise but in real life you'd have to\n#          keep track of the deep_paging_id in a while loop.\ndata = requests.get(f\"{host}/api/v4/search/submission/?fl=sid&amp;query=max_score:&gt;={FILE_SCORE_THRESHOLD}\",\n                    headers=headers, verify=False).json()['api_response']['items']\n\nfor submission in data:\n    # Download the full submission result and compute the score for each file\n    # client.submission.full(&lt;sid&gt;) --&gt; /api/v4/submission/full/&lt;sid&gt;/\n    submission_results = requests.get(f\"{host}/api/v4/submission/full/{submission['sid']}/\",\n                                    headers=headers, verify=False).json()['api_response']\n\n    # Compute the score of each files in the submission\n    files_scores = dict()\n    for result in submission_results['results'].values():\n        # Initialize the default score for the file if the file is not in the list\n        files_scores.setdefault(result['sha256'], 0)\n\n        # Add the score of the result record to the file\n        files_scores[result['sha256']] += result['result']['score']\n\n    # For each files where the score is greater than threshold, download in cARTed format\n    # client.file.download --&gt; /api/v4/file/download/sha256?encoding=cart/\n    for sha256, score in files_scores.items():\n        if score &gt;= FILE_SCORE_THRESHOLD:\n            with open(os.path.join(OUTPUT_DIRECTORY, f\"{sha256}.cart\"), 'wb') as file:\n                raw_file = requests.get(f\"{host}/api/v4/file/download/{sha256}/?encoding=cart\",\n                                        headers=headers, verify=False).content\n                file.write(raw_file)\n\nprint(os.listdir(OUTPUT_DIRECTORY))\n</code></pre>"},{"location":"integration/api_exercise_3/","title":"Ingest Files With The Ingest API","text":""},{"location":"integration/api_exercise_3/#ingest-files-with-the-ingest-api","title":"Ingest Files With The Ingest API","text":""},{"location":"integration/api_exercise_3/#scenario","title":"Scenario","text":"<p>\u201cI want to be able to automate ingestion from a host-based sensor to submit files to Assemblyline and send the parsed results to a database for long-term use.\u201d</p> <p>Some notes on the Ingest API &amp; submission tracking:</p> <ul> <li>The Ingest API is an asynchronous API, unlike the Submit API, intended to be a \u201cfire-and-forget\u201d endpoint for submitting files.</li> <li>However, at some point, we\u2019d like to be able to get the results from our submissions. We could poll the Search API periodically looking for our submissions, but this puts unnecessary strain on Assemblyline (imagine if 100+ systems were doing this at the same time!).</li> <li>Fortunately, the API supports passing in an argument to specify a notification queue name, which generates a new notification queue. The queue can be used to check and retrieve the results from the files submitted under this queue.</li> </ul>"},{"location":"integration/api_exercise_3/#expected-results","title":"Expected Results","text":"<pre><code>A script for ingesting the files into Assemblyline and sets a notification queue\n\nA script for listening on a notification queue for new results to come in\n</code></pre>"},{"location":"integration/api_exercise_3/#apis-involved","title":"APIs Involved","text":"RESTPython <pre><code>POST /api/v4/ingest/\nGET /api/v4/ingest/get_message_list/&lt;notification_queue&gt;/\n</code></pre> <pre><code>Client.ingest()\nClient.ingest.get_message_list(&lt;notification_queue&gt;)\n</code></pre>"},{"location":"integration/api_exercise_3/#solution","title":"Solution","text":"<p>The sender made sure to include the <code>\u201cfile_path\u201d</code> in the metadata so that the receiver knows which files have finished being analyzed.</p> SenderReceiver <pre><code>import os\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n# Files within this directory will be ingested to Assemblyline\n#  ** Let's just reuse the files from exercise 2\nINGEST_DIR = '/tmp/ex2'\nfiles_to_scan = list()\nfor root, _, files in os.walk(INGEST_DIR):\n    for file in files:\n        files_to_scan.append(os.path.join(root, file))\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n# Submission parameters\n#  ** NOTE: This is only provided as an example you should not use it to get the user's default values.\n#           It's just here to show you that you can alter any submission parameters\nsubmission_params = {\n    'classification': 'TLP:C',                                      # classification\n    'description': \"I don't wanna use the user's default values!\",  # description\n    'deep_scan': False,                                             # activate deep scan mode\n    'priority': 1000,              # queue priority (the higher the number, the higher the priority)\n    'ignore_cache': False,         # ignore system cache\n    'services': {\n        'selected': [              # selected service list (override user profile)\n            'Extract',\n            'Safelist'\n        ]\n    },\n    'service_spec': {               # provide a service parameter\n        'Extract': {\n            'password': 'password'\n        }\n    }\n}\n\n# This is the queue that will be use to communicate between your sender and receiver process\n# ** please make it unique for you so you don't receive messages from others!\nNOTIFICATION_QUEUE_NAME = \"CHANGE_THIS\"\n\n# Ingest files through Ingest API with a notification queue\n# ** NOTE: you should add the path of the file you just scaned to\n#          the metadata so you can keep track of it\n# client.ingest --&gt; /api/v4/ingest/\n\n# Ingest all files to scan in Assemblyline\nfor file_path in files_to_scan:\n    # That's it, just need to send all files in... the receiver will pull the results\n    client.ingest(path=file_path, metadata={'file_path': file_path}, nq=NOTIFICATION_QUEUE_NAME)\n</code></pre> <pre><code>import os\nfrom time import sleep\n\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n# Files within this directory will be ingested to Assemblyline\n#  ** Let's just reuse the files from exercise 2\nINGEST_DIR = '/tmp/ex2'\nfiles_to_scan = list()\nfor root, _, files in os.walk(INGEST_DIR):\n    for file in files:\n        files_to_scan.append(os.path.join(root, file))\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n# This is the queue that will be use to communicate between your sender and receiver process\n# ** You should have changed it in your sender... make it the same here!\nNOTIFICATION_QUEUE_NAME = \"CHANGE_THIS\"\n\n# Receive completion messages from the notification queue\n# client.ingest.get_message_list --&gt; /api/v4/ingest/get_message_list/&lt;NOTIFICATION_QUEUE_NAME&gt;/\nwhile len(files_to_scan) != 0:\n    for result in client.ingest.get_message_list(NOTIFICATION_QUEUE_NAME):\n        # This is the file we are receiveing result for\n        current_file = result['submission']['metadata']['file_path']\n\n        # For each completetion message, pull the result record to get the score\n        submission = client.submission(result['submission']['sid'])\n\n        # Print file score to screen\n        print(current_file, \"=\", submission['max_score'])\n\n        # Stop waiting for the file\n        files_to_scan.remove(current_file)\n\n    # Otherwise wait for more messages until we're finished\n    sleep(1)\n</code></pre>"},{"location":"integration/api_exercise_4/","title":"Alert Monitoring And Identifying IoCs For Blocking","text":""},{"location":"integration/api_exercise_4/#alert-monitoring-and-identifying-iocs-for-blocking","title":"Alert Monitoring And Identifying IoCs For Blocking","text":""},{"location":"integration/api_exercise_4/#scenario","title":"Scenario","text":"<p>\u201cI want to be able to action on IoCs that Assemblyline has alerted on.\u201d</p> <p>A common use for Assemblyline is the ability to perform dynamic defence which can involve blocking certain domains/IPs in a firewall</p> <p>In this exercise, we\u2019re going to look for any alerts containing a network IoC and task that IoC to be blocked (or in this case, just print the IoC that you\u2019re blocking to the console).</p>"},{"location":"integration/api_exercise_4/#expected-results","title":"Expected Results","text":"<pre><code>IoCs to be blocked are printed to console\n</code></pre>"},{"location":"integration/api_exercise_4/#apis-involved","title":"APIs Involved","text":"RESTPython <pre><code>GET /api/v4/search/&lt;index&gt;/\n</code></pre> <pre><code>Client.search.&lt;index&gt;\n</code></pre>"},{"location":"integration/api_exercise_4/#solution","title":"Solution","text":"<pre><code># Exercise #4: Alert monitoring and identify IOC for blocking\n\nimport os\nfrom assemblyline_client import get_client\n\nAL_HOST = os.getenv('AL_HOST', '&lt;AL URL&gt;')\nAL_USER = os.getenv('AL_USER', '&lt;AL user&gt;')\nAL_APIKEY = os.getenv('AL_APIKEY', '&lt;AL API key&gt;')\n\n# This is the connection to the Assemblyline client that we will use\nclient = get_client(f\"https://{AL_HOST}:443\", apikey=(AL_USER, AL_APIKEY), verify=False)\n\n\ndef block_IOC(ioc: str, ioc_type: str, verdict: str):\n    # Fake block IOC function, this would obviously be tailored code for your systems\n    print(f\"Blocking {ioc_type.upper()}: {ioc} ({verdict})\")\n\n\n# Search through the alert index for alerts with IPs and Domains\n# client.search.alert --&gt; /api/v4/search/index/\nfor alert in client.search.alert('al.ip:* OR al.domain:* OR al.uri:*', fl=\"al.detailed.*\")['items']:\n    # Iterate over the IOCs in the alert\n    for ioc_type in ['ip', 'domain', 'uri']:\n        # Iterate through the different items to check if they should be blocked\n        for ioc in alert['al']['detailed'][ioc_type]:\n            # Make sure those IOCs are not safe or informational\u200b\n            if ioc['verdict'] in ['info', 'safe']:\n                continue\n\n            # Block suspicious and malicious IOCs (ie. add to FW rules)\u200b\n            block_IOC(ioc=ioc['value'], ioc_type=ioc_type, verdict=ioc['verdict'])\n</code></pre>"},{"location":"integration/api_exercise_5/","title":"What About Custom Tradecraft?","text":""},{"location":"integration/api_exercise_5/#what-about-custom-tradecraft","title":"What About Custom Tradecraft?","text":""},{"location":"integration/api_exercise_5/#scenario","title":"Scenario","text":"<p>\u201cI can\u2019t use Assemblyline\u2019s Python/Java client to integrate with my existing tradecraft. What can I do?\u201d</p> <p>In previous exercises, we\u2019ve shown that you can use the Assemblyline client or a language\u2019s network requests library to interact with the Assemblyline\u2019s API, but what about tools like cURL or Postman?</p>"},{"location":"integration/api_exercise_5/#expected-results","title":"Expected Results","text":"<pre><code>Send files to Assemblyline through Ingest/Submit API using cURL\n</code></pre>"},{"location":"integration/api_exercise_5/#rest-apis-involved","title":"REST APIs Involved","text":"<pre><code>POST /api/v4/submit/\nPOST /api/v4/ingest/\n</code></pre> cURL Cheatsheet <pre><code>to pass headers: -H 'key: value'\nto set the request type: -X GET\nto stop cert validation: -k\nto add a multipart form data:\n      -F 'name=data'\n  OR  -F 'name=@path_to_file'\n\n# Ingest / Submit API cheatsheet\n * JSON parameters to the submission are passed inside a multipart object named 'json'\n * The file binary is passed inside a multipart object named 'bin'\n\n# ** Tip: you can pipe the curl output to json_pp so you can actually read it\n</code></pre>"},{"location":"integration/api_exercise_5/#solution","title":"Solution","text":"Using Submit APIUsing Ingest API <pre><code># Send a file for live processing using CURL\n# ** API to use: /api/v4/submit/ (POST)\necho \"Send to submit API:\"\ncurl -s -k -X POST https://$AL_HOST/api/v4/submit/ \\\n    -H \"x-user: ${AL_USER}\" \\\n    -H \"x-apikey: ${AL_APIKEY}\" \\\n    -H 'accept: application/json' \\\n    -F 'json={\"params\": {\"description\": \"My CURL test\"}, \"metadata\": {\"any_key\": \"any_value\"}}' \\\n    -F 'bin=@myfile.txt' | json_pp\n</code></pre> <pre><code># Send a file for asynchronous processing using CURL\n# ** API to use: /api/v4/ingest/ (POST)\necho \"Send to ingest API:\"\ncurl -s -k -X POST https://$AL_HOST/api/v4/ingest/ \\\n    -H \"x-user: ${AL_USER}\" \\\n    -H \"x-apikey: ${AL_APIKEY}\" \\\n    -H 'accept: application/json' \\\n    -F 'json={\"params\": {\"description\": \"My CURL test\"}, \"metadata\": {\"any_key\": \"any_value\"}}' \\\n    -F 'bin=@myfile.txt' | json_pp\n</code></pre>"},{"location":"integration/ingestion_method/","title":"Choosing your ingestion method","text":""},{"location":"integration/ingestion_method/#choosing-your-ingestion-method","title":"Choosing your ingestion method","text":"<p>While integrating Assemblyline with other systems, the first thing you will need to do is to pick an ingestion method.</p> <p>Assemblyline gives you two options:</p> <ol> <li>Asynchronous (Using the Ingest API: /api/v4/ingest/)</li> <li>Synchronous (Using the Submit API: /api/v4/submit/)</li> </ol> <p>We will give you here a rundown of the different particularities of each method so you can pick the one that fits your needs the best.</p>"},{"location":"integration/ingestion_method/#asynchronous-ingestion","title":"Asynchronous ingestion","text":"<p>This is the preferred ingestion method for use with Assemblyline. In this mode, Assemblyline will queue your submission based on priority and will process them when the services have empty processing cycles. For each submission in this mode, you will get assigned an ingestion ID and you can be notified via a completion queue when your file has completed scanning. Alternatively, you can use the alerting page in the Assemblyline UI if you want to only view asynchronous submissions that Assemblyline deems highly suspicious.</p> <p>The asynchronous model was built to sustain a large sample set of files and to help analysts focus on what is important.</p>"},{"location":"integration/ingestion_method/#benefits-and-drawbacks","title":"Benefits and Drawbacks","text":"<p>Benefits</p> <ul> <li> Support large volume of files for processing</li> <li> Not subjected to quota limits</li> <li> Allows for alerting functionality to be used</li> <li> Will resort to data sampling if it gets overwhelmed with too many files</li> <li> Does submission-level caching if the same file is submitted twice with the same parameters, for performance optimization</li> </ul> <p>Drawbacks</p> <ul> <li> Submissions may sit in the queue a long time if the system is busy</li> <li> Submissions may be skipped if the system is overwhelmed</li> <li> Metadata is not searchable for all submissions since the system does not create a submission entry for cache submissions</li> </ul>"},{"location":"integration/ingestion_method/#typical-use-cases","title":"Typical use cases","text":"<p>Here are the typical use cases that users encounter while using the asynchronous submission mode in the system.</p> Using the Ingest API while reading a message from the notification queue <ol> <li>The user submits all its files and receives ingestion IDs for its files<ul> <li>API: /api/v4/ingest/</li> </ul> </li> <li>The user asks the notification for messages until it receives a confirmation message for all its files<ul> <li>API: /api/v4/ingest/get_message_list/</li> </ul> </li> </ol> <p>This is how this works in the backend: </p> Using the Ingest API ignoring the notification queue but using the alert perspective <ol> <li>The user submits all its files and ignores the returned ingestion IDs<ul> <li>API: /api/v4/ingest/</li> </ul> </li> <li>The user then monitors the UI alerting perspective for newly created alerts<ul> <li>UI: /alerts</li> </ul> </li> </ol> <p>This is how this works in the backend: </p>"},{"location":"integration/ingestion_method/#synchronous-ingestion","title":"Synchronous ingestion","text":"<p>In this mode, Assemblyline will start the scanning of your file right away and will return you the ID of your submission. You will be able to use this ID to ask the system if the submission is complete and to pull the results when all the services are done reporting results for that submission.</p> <p>This is more suited for a small volume of files and manual analysis. Files submitted via the User interface are using the synchronous mode.</p>"},{"location":"integration/ingestion_method/#benefits-and-drawbacks_1","title":"Benefits and Drawbacks","text":"<p>Benefits</p> <ul> <li> Instant scanning</li> <li> Higher priority than asynchronous</li> <li> Submission guaranteed to be processed (no data sampling)</li> <li> Metadata searchable for all submissions</li> </ul> <p>Drawbacks</p> <ul> <li> Subjected to quota (Default: 5 concurrent submissions)</li> <li> Not suited for large volume of files</li> <li> No submission-level caching</li> <li> Alerting not available</li> </ul>"},{"location":"integration/ingestion_method/#typical-use-cases_1","title":"Typical use cases","text":"<p>Here are the typical use cases that user's encounter while using the synchronous submission mode in the system.</p> Using the Submit API waiting for the submission to be done <ol> <li>The user sends its file for processing and receives an ID for its submission<ul> <li>API: /api/v4/submit/</li> </ul> </li> <li>The user queries the <code>is completed</code> API until the system says the submission is completed<ul> <li>API: /api/v4/submission/is_completed//</li> </ul> </li> <li>The user pulls the results for the submission<ul> <li>API: /api/v4/submission/full//</li> </ul> </li> </ol> <p>This is how this works in the backend: </p>"},{"location":"integration/ingestion_method/#mass-submission-toolkit","title":"Mass Submission Toolkit","text":"<p>The Assemblyline Incident Manager can assist you with submitting a large amount of files, such as every file on a hard drive for example. It utilizes the ingest API that we just talked about.</p> <p>The Cyber Centre uses this tool when we are in a pinch and need to ingest millions of files to Assemblyline without learning how to use the APIs.</p> <p>One key consideration for submitting a large volume of files in a burst is the <code>default sampling values</code> in the Ingester Configuration.</p> <p>You must keep your ingestion flow at a rate such that the size of the priority ingestion queue remains lower than the corresponding priority queue <code>sampling_at</code> values, otherwise, Assemblyline will skip files.</p>"},{"location":"integration/java/","title":"Java Client","text":""},{"location":"integration/java/#java-client","title":"Java Client","text":"<p>The Assemblyline Java client library provides methods to submit requests to Assemblyline.</p> <p> Get the Java client</p>"},{"location":"integration/java/#using-the-client","title":"Using the client","text":"<p>To instantiate the client bean set the application properties associated with the desired authentication method. The client can be accessed by auto-wiring the bean into the class using it.</p> <p>There are two authentication methods: username/apikey or username/password.</p>"},{"location":"integration/java/#api-key-authentication","title":"API Key Authentication","text":"<p>To instantiate an API key authenticated Assemblyline client, define the following properties:</p> <pre><code>assemblyline-java-client:\n    url: &lt;assemblyline-instance-url&gt;\n    api-auth:\n        apikey: &lt;api-key&gt;\n        username: &lt;username&gt;\n</code></pre>"},{"location":"integration/java/#password-authentication","title":"Password Authentication","text":"<p>To instantiate a password authenticated Assemblyline client, define the following properties:</p> <pre><code>assemblyline-java-client:\n    url: &lt;assemblyline-instance-url&gt;\n    password-auth:\n        password: &lt;password&gt;\n        username: &lt;username&gt;\n</code></pre>"},{"location":"integration/java/#proxy","title":"Proxy","text":"<p>To go through a proxy, add the following properties:</p> <pre><code>assemblyline-java-client:\n    proxy:\n        host: &lt;host&gt;\n        port: &lt;port&gt;\n</code></pre>"},{"location":"integration/key_generation/","title":"Generating an API key","text":""},{"location":"integration/key_generation/#generating-an-api-key","title":"Generating an API key","text":"<p>While integrating Assemblyline to another system, you should not save your username and password into another app. Instead, you should create an API Key with only the appropriate requirements for that specific integration.</p>"},{"location":"integration/key_generation/#role-based-access-controls","title":"Role-based Access Controls","text":"<p>All APIs have role-based access controls (RBAC) and require users to be authenticated to use those APIs through basic authentication like username and password, API keys, certificates, etc.</p> <p></p> <p>API keys can be defined to have specific restraints that are less than or equal to those imposed on the owner.</p> <p>The APIs also drive whether information should be made accessible to a user by comparing the classification of the requester against the data asked to retrieve.</p>"},{"location":"integration/key_generation/#create-an-api-key","title":"Create an API key","text":"<p>Here is how to do this:</p> <p></p> <ul> <li> Login to Assemblyline's user interface with the user that will perform API requests</li> <li> Click on your avatar in the top-right corner of the Assemblyline UI and select \"Manage Account\"</li> <li> Scroll down to the bottom to the \"Security\" section and select \"Manage API Keys\"</li> <li> Add the API Key name, select access privileges then click the \"Add\" button.</li> <li> The API KEY will only be displayed once and can't be recovered. Copy it somewhere safe so that you can use it later.</li> <li> Click the \"Done\" button.</li> </ul>"},{"location":"integration/ontology/","title":"Ontology","text":""},{"location":"integration/ontology/#ontology","title":"Ontology","text":"<p>The data returned for standard APIs is structured in a way that is tailored for the frontend of Assemblyline, because the frontend relies on these APIs for its data also.</p> <p>Using the Ontology APIs, it's easier to export analysis results from Assemblyline to feed into other systems without having to parse data intended to be rendered in the UI for human consumption.</p> <p>For machine-to-machine parsing, we recommend the use of the Ontology APIs.</p> <p></p> <p>At the Cyber Centre, we are currently using the Ontology APIs to export submission results to a Superset deployment for data visualization and big-data analytics.</p> <p>For more details regarding the Ontology output, see here.</p>"},{"location":"integration/python/","title":"Python Client","text":""},{"location":"integration/python/#python-client","title":"Python Client","text":"<p>The Assemblyline Python client facilitates issuing requests to Assemblyline.</p>"},{"location":"integration/python/#installing-the-client","title":"Installing the client","text":"<pre><code>pip install assemblyline_client\n</code></pre>"},{"location":"integration/python/#connecting-to-assemblyline","title":"Connecting to Assemblyline","text":"<p>You can instantiate the client by using the following snippet of Python code:</p> API KeyUser/PasswordCertificate <p>You will need an API key. </p><pre><code>from assemblyline_client import get_client\nal_client = get_client(\"https://yourdomain:443\", apikey=('user', 'key'))\n</code></pre><p></p> <pre><code>from assemblyline_client import get_client\nal_client = get_client(\"https://yourdomain:443\", auth=('user', 'password'))\n</code></pre> <pre><code>from assemblyline_client import get_client\n# If your Assemblyline server is using a self-signed certificate\nal_client = get_client(\"https://yourdomain:443\", cert='/path/to/cert/file.pem')\n</code></pre> When connecting to Assemblyline, you can provide a certificate for SSL or ignore the certificate error <pre><code>    al_client = get_client(..., verify='/path/to/server.crt')\n\n    al_client = get_client(..., verify=False)\n</code></pre> The client is fully documented in the docstrings so that you can use the 'help' feature of IPython or Jupyter Notebook <pre><code>    al_client.search.alert?\n\n    Signature: al_client.search.alert(\n        query,\n        filters=None,\n        fl=None,\n        offset=0,\n        rows=25,\n        sort=None,\n        timeout=None,\n    )\n    Docstring:\n    Search alerts with a Lucene query.\n\n    Required:\n    query   : Lucene query. (string)\n\n    Optional:\n    filters : Additional Lucene queries used to filter the data (list of strings)\n    fl      : List of fields to return (comma separated string of fields)\n    offset  : Offset at which the query items should start (integer)\n    rows    : Number of records to return (integer)\n    sort    : Field used for sorting with direction (string: ex. 'id desc')\n    timeout : Max number of milliseconds the query will run (integer)\n\n    Returns all results.\n    File:      /usr/local/lib/python3.7/site-packages/assemblyline_client/v4_client/module/search/__init__.py\n    Type:      method\n</code></pre>"},{"location":"integration/python/#examples","title":"Examples","text":""},{"location":"integration/python/#submit-a-file-url-or-sha256-for-analysis","title":"Submit a file, URL or SHA256 for analysis","text":"<p>There are two methods for sending a file/URL/SHA256 to Assemblyline for analysis: Ingest and Submit.</p> <p>In most cases, you want to use the Ingest API via the CLI</p> <p>The default limit for sample size is 100mb, this is also the case in the UI and the use of the API is the only workaround. In order to circumvent this, the option ignore_size=True must be used when submitting via the API. </p> <p>(Optional) Customizing your submission</p> <p>Note: Service names are case-sensitive</p> <pre><code># Submission parameters (works for both Ingest and Submit)\nsettings = {\n    'classification' : 'TLP:A',     # classification\n    'description' : 'Hello world',  # file description\n    'name' : 'filename',            # file name\n    'deep_scan' : False,            # activate deep scan mode\n    'priority' : 1000,              # queue priority (the higher the number, the higher the priority)\n    'ignore_cache' : False,         # ignore system cache\n    'services' : {\n        'selected' : [              # selected service list (override user profile)\n            'CAPE','Extract'\n        ],\n        'resubmit' : [],            # resubmit to these services if file initially scores &gt; 500\n        'excluded': [],             # exclude these services\n    },\n    'service_spec': {               # provide a service parameter\n        'Extract': {\n            'password': 'password'\n        }\n    }\n}\n\n# Adding metadata (such as the source of the files or anything you want!)\nmy_meta = {\n    'my_metadata' : 'value',     # any metadata of your liking\n    'my_metadata2' : 'value2'     # any metadata of your liking\n}\n</code></pre> <p>You can find all parameters and their default values in the SubmissionParams class.</p> <p>For submitting a URL instead of a file, use the <code>url</code> argument instead of <code>path</code></p> <p>For submitting a SHA256 instead of a file, use the <code>sha256</code> argument instead of <code>path</code>. You may use external sources with <code>params={\"default_external_sources\":[\"VirusTotal\",\"Malware Bazaar\"]}</code></p>"},{"location":"integration/python/#ingest","title":"Ingest","text":"<p>The Ingest API supports two additional functionalities over the Submit API:</p> <ul> <li>By passing the argument <code>alert=True</code>, the system will generate an alert if the score is over 500</li> <li>By passing the argument <code>nq='notification_queue_name'</code>, you can use the client to poll a notification queue for a message indicating if the analysis has been completed<ul> <li>If you don't need to know when the analysis completes, then you can omit the <code>nq</code> argument and ignore the subsequent code that interacts with the notification queue</li> </ul> </li> </ul> <pre><code>ingest_id = al_client.ingest(path='/pathto/file.txt', nq='my_queue_name', params=settings, metadata=my_meta)\n\n# If you use a notification queue you can get your asynchronous results with:\nfrom time import sleep\nmessage = None\nwhile True:\n    message = al_client.ingest.get_message(\"my_queue_name\")\n    if message is None:\n        sleep(1)    # Poll every second\n    else:\n        do something ...\n</code></pre>"},{"location":"integration/python/#submit","title":"Submit","text":"<pre><code>submit_results = al_client.submit(path='/pathto/file.txt', fname='fname', params=settings, metadata=my_meta)\n</code></pre>"},{"location":"integration/python/#submission-details","title":"Submission details","text":"<p>To get the details about a submission, you simply need to pass the client a submission ID (sid)</p> <pre><code>submission_details = al_client.submission(\"4nxrpBePQDLH427aA8m3TZ\")\n</code></pre>"},{"location":"integration/python/#using-search","title":"Using search","text":"<p>More details about Search</p> <p>You can use the search engine in the client by simply passing a Lucene query.</p> <p>In the following example, we want to retrieve the first page of submissions made by <code>user</code>:</p> <pre><code>search_result = al_client.search.submission(\"params.submitter:user\")\n</code></pre>"},{"location":"integration/python/#using-search-iterator","title":"Using search iterator","text":"<p>Instead of using <code>search</code> and getting a page of results, you can use the <code>search</code> iterator <code>stream</code> to go through all the results.</p> <p>Streamed results only return indexed fields. If you want the full result, you must go get it via the client</p> <pre><code>for submission in al_client.search.stream.submission(\"params.submitter:user\"):\n    submission_id = submission[\"sid\"]\n    full_submission = al_client.submission(submission_id)\n</code></pre>"},{"location":"integration/python/#using-search-parameters","title":"Using search parameters","text":"<p>In the following example, we want to retrieve the first page of submissions that were submitted in the last week, and we only want the submission IDs:</p> <pre><code>submission_results = al_client.search.submission('times.submitted:[now-7d TO now]', fl='sid')\n</code></pre> <p><code>fl</code> defaults to a list of predefined fields that we deemed important; you may use <code>fl=\"*\"</code> to get all fields. You can view the fields that we deem important under the 'Search Help' page on your Assemblyline instance. These fields have a <code>stored</code> attribute.</p>"},{"location":"integration/python/#using-facet-searching","title":"Using facet searching","text":"<p>In the following example, we want to retrieve the users who have made submissions in the last week, and the number of submissions that they have made:</p> <pre><code>submission_results = al_client.search.facet.submission('params.submitter', query='times.submitted:[now-7d TO now]')\n</code></pre>"},{"location":"integration/python/#using-the-command-line-tool","title":"Using the Command line Tool","text":"<p>By installing the <code>assemblyline_client</code> PIP package, a command line tool <code>al-submit</code> is installed. In case you don't want to use Python code to interface with the Assemblyline client, you can use this tool instead. You can view the user options via <code>al-submit --help</code>.</p> (Optional) Configuration file example <p>Rather than passing authentication and server details as parameters in a command line, you can use a configuration file. This configuration file should be placed at <code>~/.al/submit.cfg</code>. A template for this configuration file can be found below. NOTE: You can use <code>=</code> or <code>:</code> as the delimiter between key and value. </p><pre><code>[auth]\n#   Username for the Assemblyline account.\nuser =\n#   There are three methods to authenticate a user account. Choose one:\n#   - Password Provided via User Prompt\n#       Leave the `password' configuration value below empty.\n#   - Password Provided in Configuration File\n#       Enter the password for the Assemblyline account in plaintext.\npassword =\n#   - API Key in Configuration File\n#       Enter the API key to use in plaintext for the user to login.\n#       NOTE: The API key must have WRITE access for INGEST and WRITE+READ for SUBMIT.\napikey =\n# Skip server cert validation.\n# Value can be one of: true, false, yes, no\n# If not supplied, the default value is: false\ninsecure =\n[server]\n# Method of network transport.\n# If not supplied, the default value is: https\ntransport =\n# Domain of Assemblyline instance.\n# If not supplied, the default value is: localhost\nhost =\n# Port to which traffic will be sent.\n# If not supplied, the default value is: 443\nport =\n# Server cert used to connect to server.\ncert =\n</code></pre><p></p>"},{"location":"integration/rest/","title":"RESTful API","text":""},{"location":"integration/rest/#restful-api","title":"RESTful API","text":"<p>When it is impossible to integrate your application using the dedicated Python or Java clients, you can use Assemblyline's RESTful API to perform any task that you can think of.</p>"},{"location":"integration/rest/#api-documentation","title":"API documentation","text":"<p>Each instance of Assemblyline comes with its internal API documentation which can be viewed by browsing to https://yourdomain/help/api</p> <p></p>"},{"location":"integration/rest/#connecting-to-the-api","title":"Connecting to the API","text":"<p>For easy integration, it is recommended that you generate an API key for the user who will perform RESTful queries. Otherwise, you will have to build yourself a library that will handle session cookies and XSRF tokens and you probably want something simpler.</p>"},{"location":"integration/rest/#using-the-api-key","title":"Using the API key","text":"<p>To use your newly created API key you can simply add the <code>X-USER</code> and <code>X-APIKEY</code> headers to your request and the system will identify you with that key at each request instead of relying on a session cookie.</p> <p>Simple api call</p> <p>Let's use a hypothetical API key to ask the system who we are. (Using the <code>/api/v4/user/whoami/</code> API)</p> CURLJavascript (fetch)Python (requests) <pre><code>curl -X GET \"https://yourdomain/api/v4/user/whoami/\" \\\n    -H 'x-user: &lt;your_user_id&gt;' \\\n    -H 'x-apikey: &lt;key_name:randomly_generated_password&gt;' \\\n    -H 'accept: application/json'\n</code></pre> <pre><code>fetch(\n\"https://yourdomain/api/v4/user/whoami/\",\n{\n    \"headers\": {\n    \"accept\": \"application/json\",\n    \"x-apikey\": \"&lt;key_name:randomly_generated_password&gt;\",\n    \"x-user\": \"&lt;your_user_id&gt;\"\n    },\n    \"method\": \"GET\"\n}\n);\n</code></pre> <pre><code>import requests\nrequests.get(\n    \"https://yourdomain/api/v4/user/whoami/\",\n    headers={\n        \"x-user\": \"&lt;your_user_id&gt;\",\n        \"x-apikey\": \"&lt;key_name:randomly_generated_password&gt;\",\n        \"accept\": \"application/json\"\n    }\n)\n</code></pre> <p>Submit a file to the API</p> <p>Submit API was used here but you can use the ingest API with the same parameters.</p> <p>Now if we were to reuse that same API key to submit a file to the system, not only you need to pass the <code>X-USER</code>, <code>X-APIKEY</code> and <code>ACCEPT</code> headers, you also need to pass at least one of the two multipart sections:</p> <ul> <li><code>json</code> (optional): This is a JSON dictionary with 3 possible keys<ul> <li><code>name</code> (optional): Name of the file, otherwise <code>bin.filename</code> is used</li> <li><code>params</code> (optional): Changes to the default submission parameters for the user</li> <li><code>metadata</code> (optional): Metadata to be added to the submission</li> </ul> </li> <li><code>bin</code> (required): the actual file to be scanned</li> </ul> CURLPython (requests) <pre><code>curl -X POST https://yourdomain/api/v4/submit/ \\\n    -H 'x-user: &lt;your_user_id&gt;' \\\n    -H 'x-apikey: &lt;key_name:randomly_generated_password&gt;' \\\n    -H 'accept: application/json' \\\n    -F 'json={\"params\": {\"description\": \"My CURL test\"}, \"metadata\": {\"any_key\": \"any_value\"}}' \\\n    -F 'bin=@myfile.txt'\n</code></pre> <pre><code>import requests\nimport json\n\nrequests.post(\n    \"https://yourdomain/api/v4/submit/\",\n    headers={\n        \"x-user\": \"&lt;your_user_id&gt;\",\n        \"x-apikey\": \"&lt;key_name:randomly_generated_password&gt;\",\n        \"accept\": \"application/json\",\n        \"content-type\": None\n    },\n    files={'bin': open('myfile.txt', 'rb')},\n    data={'json': json.dumps({'params': {'description': 'My CURL test'}, 'metadata': {'any_key': 'any_value'}})}\n)\n</code></pre>"},{"location":"integration/rest/#api-gotcha","title":"API Gotcha","text":"<p>Here is a list of the most common issues users are facing while using the API</p>"},{"location":"integration/rest/#wrong-content-type","title":"Wrong content type","text":"<p>All Assemblyline APIs are built around receiving and returning JSON data. Do not forget to set your <code>Content-Type</code> and <code>Accept</code> headers to <code>\"application/json\"</code> or you might encounter some issues.</p>"},{"location":"integration/rest/#trailing-forward-slash","title":"Trailing forward slash","text":"<p>All Assemblyline APIs end with a trailing forward slash <code>\"/\"</code>. Make sure that the API URL has it at the end of the URL otherwise you may get a <code>\"Method not allowed\"</code> error and you'll have issues figuring out why.</p>"},{"location":"odm/getting_started/","title":"Getting Started","text":""},{"location":"odm/getting_started/#welcome-to-odm-documentation","title":"Welcome to ODM documentation \ud83d\udcdd","text":"Auto-Generated Documentation <p>This set of documentation is automatically generated from source and will help ensure any change to functionality will always be documented and available on release.</p> <p>This section of the site is reserved for advanced use cases where you're interested in how different pieces of data are stored and structured in Assemblyline.</p> <p>This gives insight into the current functionality of Assemblyline and what you can do.</p>"},{"location":"odm/getting_started/#basic-field-types","title":"Basic Field Types","text":"<p>Here is a table of the basic types of fields in our data models and what they're used for:</p> Name Description <code>Any</code> A field that can hold any value whatsoever but which is stored as a Keyword in the datastore index. <code>Boolean</code> A field storing a boolean value. <code>Classification</code> A field storing access control classification. <code>ClassificationString</code> A field storing the classification as a string only. <code>Compound</code> A field used to wrap Model classes and treat them as fields. <code>Date</code> A field storing a datetime value. <code>Domain</code> A field storing a network domain value. <code>Email</code> A field storing an email address value. <code>EmptyableKeyword</code> A keyword which allow to differentiate between empty and None values. <code>Enum</code> A field storing a short string that has predefined list of possible values. <code>FlattenedListObject</code> A field storing a flattened list object. <code>FlattenedObject</code> A field storing a flattened object. <code>Float</code> A field storing a floating point value. <code>IP</code> A field storing a network IP value. <code>IndexText</code> A special field with special processing rules to simplify searching. <code>Integer</code> A field storing an integer value. <code>Json</code> A field storing serializeable structure with their JSON encoded representations.An example of this is metadata <code>Keyword</code> A field storing a short string with a technical interpretation.Examples: file hashes, service names, document IDs <code>List</code> A field storing a sequence of typed elements. <code>MAC</code> A field storing a MAC address value. <code>MD5</code> A field storing an MD5 hash of a file. <code>Mapping</code> A field storing a sequence of typed elements. <code>Optional</code> A wrapper field that allows simple types (int, float, bool) to take None values. <code>PhoneNumber</code> A field storing a phone number. <code>Platform</code> A field storing an OS platform value. <code>Processor</code> A field storing a processor value. <code>SHA1</code> A field storing an SHA1 hash of a file. <code>SHA256</code> A field storing an SHA256 hash of a file. <code>SSDeepHash</code> A field storing an SSDeepHash value. <code>Text</code> A field storing human-readable text data. <code>URI</code> A field storing a network URI value. <code>URIPath</code> A field storing a network URI path value. <code>UUID</code> A field storing an auto-generated unique ID if None is provided. <code>UpperKeyword</code> A field storing a short uppercase string with a technical interpretation. <code>ValidatedKeyword</code> Keyword field whose value is validated by a regular expression."},{"location":"odm/getting_started/#field-states","title":"Field States","text":"<p>In each table, there will be a \"Required\" column with different states about the field's status:</p> State Description  Yes This field is required to be set in the model  Optional This field isn't required to be set in the model  Deprecated This field has been deprecated in the model. See field's description for more details. <p>Note: Fields that are \" Deprecated\" that are still shown in the docs will still work as expected but you're encouraged to update your configuration as soon as possible to avoid future deployment issues.</p>"},{"location":"odm/messages/alert/","title":"AlertMessage","text":""},{"location":"odm/messages/alert/#alertmessage","title":"AlertMessage","text":"<p>Model of Alert Message</p> Field Type Description Required Default msg Alert Message of alert  Yes <code>None</code> msg_loader Enum Loader class for messagesSupported values are:<code>\"assemblyline.odm.messages.alert.AlertMessage\"</code>  Yes <code>assemblyline.odm.messages.alert.AlertMessage</code> msg_type Enum Type of MessageSupported values are:<code>\"AlertCreated\", \"AlertUpdated\"</code>  Yes <code>AlertCreated</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/alerter_heartbeat/","title":"AlerterMessage","text":""},{"location":"odm/messages/alerter_heartbeat/#alertermessage","title":"AlerterMessage","text":"<p>Model of Alerter Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message from Alerter  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.alerter_heartbeat.AlerterMessage\"</code>  Yes <code>assemblyline.odm.messages.alerter_heartbeat.AlerterMessage</code> msg_type Enum Type of messageSupported values are:<code>\"AlerterHeartbeat\"</code>  Yes <code>AlerterHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/alerter_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model for Alerter</p> Field Type Description Required Default instances Integer Number of Alerter instances  Yes <code>None</code> metrics Metrics Alert metrics  Yes <code>None</code> queues Queues Alert queues  Yes <code>None</code>"},{"location":"odm/messages/alerter_heartbeat/#metrics","title":"Metrics","text":"<p>Alerter Metrics</p> Field Type Description Required Default created Integer Number of alerts created  Yes <code>None</code> error Integer Number of alerts with errors  Yes <code>None</code> received Integer Number of alerts received  Yes <code>None</code> updated Integer Number of alerts updated  Yes <code>None</code> wait Integer Number of alerts waiting for submission to complete  Yes <code>None</code>"},{"location":"odm/messages/alerter_heartbeat/#queues","title":"Queues","text":"<p>Alerter Queues</p> Field Type Description Required Default alert Integer Number of alerts in queue  Yes <code>None</code> alert_retry Integer Number of alerts in retry queue  Yes <code>None</code>"},{"location":"odm/messages/archive_heartbeat/","title":"ArchiveMessage","text":""},{"location":"odm/messages/archive_heartbeat/#archivemessage","title":"ArchiveMessage","text":"<p>Model for Archive Heartbeat Messages</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.archive_heartbeat.ArchiveMessage\"</code>  Yes <code>assemblyline.odm.messages.archive_heartbeat.ArchiveMessage</code> msg_type Enum Message typeSupported values are:<code>\"ArchiveHeartbeat\"</code>  Yes <code>ArchiveHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/archive_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Archive Heartbeat Model</p> Field Type Description Required Default instances Integer Number of instances  Yes <code>None</code> metrics Metrics Archive metrics  Yes <code>None</code> queued Integer Number of documents to be archived  Yes <code>None</code>"},{"location":"odm/messages/archive_heartbeat/#metrics","title":"Metrics","text":"<p>Archive Metrics</p> Field Type Description Required Default file Integer Number of files archived  Yes <code>None</code> result Integer Number of results archived  Yes <code>None</code> submission Integer Number of submissions archived  Yes <code>None</code> received Integer Number of received archive messages  Yes <code>None</code> exception Integer Number of exceptions during archiving  Yes <code>None</code> invalid Integer Number of invalid archive type errors during archiving  Yes <code>None</code> not_found Integer Number of submission not found failures during archiving  Yes <code>None</code>"},{"location":"odm/messages/dispatcher_heartbeat/","title":"DispatcherMessage","text":""},{"location":"odm/messages/dispatcher_heartbeat/#dispatchermessage","title":"DispatcherMessage","text":"<p>Model of Dispatcher Heartbeat Messages</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.dispatcher_heartbeat.DispatcherMessage\"</code>  Yes <code>assemblyline.odm.messages.dispatcher_heartbeat.DispatcherMessage</code> msg_type Enum Type of messageSupported values are:<code>\"DispatcherHeartbeat\"</code>  Yes <code>DispatcherHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/dispatcher_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default inflight Inflight Inflight submissions  Yes <code>None</code> instances Integer Number of instances  Yes <code>None</code> metrics Metrics Dispatcher metrics  Yes <code>None</code> queues Queues Dispatcher queues  Yes <code>None</code> component Keyword Component name  Yes <code>None</code>"},{"location":"odm/messages/dispatcher_heartbeat/#inflight","title":"Inflight","text":"<p>Inflight Model</p> Field Type Description Required Default max Integer Maximum number of submissions  Yes <code>None</code> outstanding Integer Number of outstanding submissions  Yes <code>None</code> per_instance List [Integer] Number of submissions per Dispatcher instance  Yes <code>None</code>"},{"location":"odm/messages/dispatcher_heartbeat/#metrics","title":"Metrics","text":"<p>Metrics Model</p> Field Type Description Required Default files_completed Integer Number of files completed  Yes <code>None</code> submissions_completed Integer Number of submissions completed  Yes <code>None</code> service_timeouts Integer Number of service timeouts  Yes <code>None</code> cpu_seconds PerformanceTimer CPU time  Yes <code>None</code> cpu_seconds_count Integer CPU count  Yes <code>None</code> busy_seconds PerformanceTimer Busy CPU time  Yes <code>None</code> busy_seconds_count Integer Busy CPU count  Yes <code>None</code> save_queue Integer Processed submissions waiting to be saved  Yes <code>None</code> error_queue Integer Errors waiting to be saved  Yes <code>None</code>"},{"location":"odm/messages/dispatcher_heartbeat/#queues","title":"Queues","text":"<p>Queue Model</p> Field Type Description Required Default ingest Integer Number of submissions in ingest queue  Yes <code>None</code> start List [Integer] Number of submissions that started  Yes <code>None</code> result List [Integer] Number of results in queue  Yes <code>None</code> command List [Integer] Number of commands in queue  Yes <code>None</code>"},{"location":"odm/messages/dispatching/","title":"DispatcherCommandMessage","text":""},{"location":"odm/messages/dispatching/#dispatchercommandmessage","title":"DispatcherCommandMessage","text":"<p>Model of Dispatcher Command Message</p> Field Type Description Required Default kind Enum Kind of messageSupported values are:<code>\"CREATE_WATCH\", \"LIST_OUTSTANDING\", \"UPDATE_BAD_SID\"</code>  Yes <code>None</code> payload_data Any Message payload  Yes <code>None</code>"},{"location":"odm/messages/dispatching/#watchqueuemessage","title":"WatchQueueMessage","text":"<p>These are messages sent by dispatcher on the watch queue</p> Field Type Description Required Default cache_key Keyword Cache key  Optional <code>None</code> status Enum Watch statusesSupported values are:<code>\"FAIL\", \"OK\", \"START\", \"STOP\"</code>  Yes <code>None</code>"},{"location":"odm/messages/dispatching/#createwatch","title":"CreateWatch","text":"<p>Create Watch Message</p> Field Type Description Required Default queue_name Keyword Name of queue  Yes <code>None</code> submission Keyword Submission ID  Yes <code>None</code>"},{"location":"odm/messages/dispatching/#listoutstanding","title":"ListOutstanding","text":"<p>List Outstanding Message</p> Field Type Description Required Default response_queue Keyword Response queue  Yes <code>None</code> submission Keyword Submission ID  Yes <code>None</code>"},{"location":"odm/messages/elastic_heartbeat/","title":"ElasticMessage","text":""},{"location":"odm/messages/elastic_heartbeat/#elasticmessage","title":"ElasticMessage","text":"<p>Model of Elasticsearch Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message for elasticsearch  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.elastic_heartbeat.ElasticMessage\"</code>  Yes <code>assemblyline.odm.messages.elastic_heartbeat.ElasticMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ElasticHeartbeat\"</code>  Yes <code>ElasticHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/elastic_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model for Elasticsearch</p> Field Type Description Required Default instances Integer Number of Elasticsearch instances with assigned shards  Yes <code>None</code> unassigned_shards Integer Number of unassigned shards in the cluster  Yes <code>None</code> request_time Float Time to load shard metrics  Yes <code>None</code> shard_sizes List [IndexData] Information about each index  Yes <code>None</code>"},{"location":"odm/messages/elastic_heartbeat/#indexdata","title":"IndexData","text":"<p>Information about an elasticsearch shard</p> Field Type Description Required Default name Keyword None  Yes <code>None</code> shard_size Integer None  Yes <code>None</code>"},{"location":"odm/messages/expiry_heartbeat/","title":"ExpiryMessage","text":""},{"location":"odm/messages/expiry_heartbeat/#expirymessage","title":"ExpiryMessage","text":"<p>Model of Expiry Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Hearbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.expiry_heartbeat.ExpiryMessage\"</code>  Yes <code>assemblyline.odm.messages.expiry_heartbeat.ExpiryMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ExpiryHeartbeat\"</code>  Yes <code>ExpiryHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/expiry_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default instances Integer Number of instances  Yes <code>None</code> metrics Metrics Expiry metrics  Yes <code>None</code> queues Metrics Expiry queues  Yes <code>None</code>"},{"location":"odm/messages/expiry_heartbeat/#metrics","title":"Metrics","text":"<p>Expiry Stats</p> Field Type Description Required Default alert Integer Number of alerts  Yes <code>None</code> badlist Integer Number of badlisted items  Yes <code>None</code> cached_file Integer Number of cached files  Yes <code>None</code> emptyresult Integer Number of empty results  Yes <code>None</code> error Integer Number of errors  Yes <code>None</code> file Integer Number of files  Yes <code>None</code> filescore Integer Number of filscores  Yes <code>None</code> result Integer Number of results  Yes <code>None</code> retrohunt_hit Integer Number of retrohunt hits  Yes <code>None</code> safelist Integer Number of safelisted items  Yes <code>None</code> submission Integer Number of submissions  Yes <code>None</code> submission_tree Integer Number of submission trees  Yes <code>None</code> submission_summary Integer Number of submission summaries  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/","title":"IngestMessage","text":""},{"location":"odm/messages/ingest_heartbeat/#ingestmessage","title":"IngestMessage","text":"<p>Model of Ingester Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.ingest_heartbeat.IngestMessage\"</code>  Yes <code>assemblyline.odm.messages.ingest_heartbeat.IngestMessage</code> msg_type Enum Type of messageSupported values are:<code>\"IngestHeartbeat\"</code>  Yes <code>IngestHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default instances Integer Number of ingest processes  Yes <code>None</code> metrics Metrics Metrics  Yes <code>None</code> processing Processing Inflight queue sizes  Yes <code>None</code> processing_chance ProcessingChance Chance of processing items  Yes <code>None</code> queues Queues Queue lengths block  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/#metrics","title":"Metrics","text":"<p>Metrics</p> Field Type Description Required Default cache_miss Integer Number of cache misses  Yes <code>None</code> cache_expired Integer Number of cache expires  Yes <code>None</code> cache_stale Integer Number of cache stales  Yes <code>None</code> cache_hit_local Integer Number of cache local hits  Yes <code>None</code> cache_hit Integer Number of cache hits  Yes <code>None</code> bytes_completed Integer Number of bytes completed  Yes <code>None</code> bytes_ingested Integer Number of bytes ingested  Yes <code>None</code> duplicates Integer Number of duplicate submissions  Yes <code>None</code> error Integer Number of errors  Yes <code>None</code> files_completed Integer Number of completed files  Yes <code>None</code> skipped Integer Number of skipped files  Yes <code>None</code> submissions_completed Integer Number of completed submissions  Yes <code>None</code> submissions_ingested Integer Number of ingested submissions  Yes <code>None</code> timed_out Integer Number of timed_out submissions  Yes <code>None</code> whitelisted Integer Number of safelisted submissions  Yes <code>None</code> cpu_seconds PerformanceTimer None  Yes <code>None</code> cpu_seconds_count Integer None  Yes <code>None</code> busy_seconds PerformanceTimer None  Yes <code>None</code> busy_seconds_count Integer None  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/#processing","title":"Processing","text":"<p>Processing</p> Field Type Description Required Default inflight Integer Number of inflight submissions  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/#processingchance","title":"ProcessingChance","text":"<p>Chance of Processing</p> Field Type Description Required Default critical Float Chance of processing critical items  Yes <code>None</code> high Float Chance of processing high items  Yes <code>None</code> low Float Chance of processing low items  Yes <code>None</code> medium Float Chance of processing medium items  Yes <code>None</code>"},{"location":"odm/messages/ingest_heartbeat/#queues","title":"Queues","text":"<p>Queues</p> Field Type Description Required Default critical Integer Size of the critical priority queue  Yes <code>None</code> high Integer Size of the high priority queue  Yes <code>None</code> ingest Integer Size of the ingest queue  Yes <code>None</code> complete Integer Size of the complete queue  Yes <code>None</code> low Integer Size of the low priority queue  Yes <code>None</code> medium Integer Size of the medium priority queue  Yes <code>None</code>"},{"location":"odm/messages/metrics/","title":"MetricsMessage","text":""},{"location":"odm/messages/metrics/#metricsmessage","title":"MetricsMessage","text":"<p>Model of Metric Message</p> Field Type Description Required Default msg Metrics Metrics message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.metrics.MetricsMessage\"</code>  Yes <code>assemblyline.odm.messages.metrics.MetricsMessage</code> msg_type Enum Type of messageSupported values are:<code>\"MetricsCounter\"</code>  Yes <code>MetricsCounter</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/metrics/#metrics","title":"Metrics","text":"<p>Metrics Model</p> Field Type Description Required Default host Keyword Host that generated metric  Yes <code>None</code> type Keyword Type of metric  Yes <code>None</code> name Keyword Metric name  Yes <code>None</code> metrics Mapping [String, Integer] Metric value  Yes <code>None</code>"},{"location":"odm/messages/retrohunt_heartbeat/","title":"RetrohuntMessage","text":""},{"location":"odm/messages/retrohunt_heartbeat/#retrohuntmessage","title":"RetrohuntMessage","text":"<p>Model of retrohunt heartbeat message</p> Field Type Description Required Default msg Heartbeat Heartbeat message for retrohunt  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.retrohunt_heartbeat.RetrohuntMessage\"</code>  Yes <code>assemblyline.odm.messages.retrohunt_heartbeat.RetrohuntMessage</code> msg_type Enum Type of messageSupported values are:<code>\"RetrohuntHeartbeat\"</code>  Yes <code>RetrohuntHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/retrohunt_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model for retrohunt</p> Field Type Description Required Default instances Integer Number of retrohunt workers  Yes <code>None</code> request_time Float None  Optional <code>None</code> pending_files Integer Files not yet available for searching  Yes <code>None</code> ingested_last_minute Integer Files ingested in last minute  Yes <code>None</code> worker_storage_available Integer Free storage for most depleted worker  Yes <code>None</code> total_storage_available Integer Free storage across workers  Yes <code>None</code> active_searches Integer Number of currently running searches  Yes <code>None</code> last_minute_cpu Float Last minute cpu load across all workers  Yes <code>None</code> total_memory_used Float Estimated current memory use across all workers  Yes <code>None</code>"},{"location":"odm/messages/scaler_heartbeat/","title":"ScalerMessage","text":""},{"location":"odm/messages/scaler_heartbeat/#scalermessage","title":"ScalerMessage","text":"<p>Model of Scaler Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class of messageSupported values are:<code>\"assemblyline.odm.messages.scaler_heartbeat.ScalerMessage\"</code>  Yes <code>assemblyline.odm.messages.scaler_heartbeat.ScalerMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ScalerHeartbeat\"</code>  Yes <code>ScalerHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/scaler_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default instances Integer Number of instances  Yes <code>None</code> metrics Metrics Metrics  Yes <code>None</code>"},{"location":"odm/messages/scaler_heartbeat/#metrics","title":"Metrics","text":"<p>Metrics</p> Field Type Description Required Default memory_free Float Amount of free memory  Yes <code>None</code> cpu_free Float Amount of free CPU  Yes <code>None</code> memory_total Float Amount of total memory  Yes <code>None</code> cpu_total Float Amount of total CPU  Yes <code>None</code>"},{"location":"odm/messages/scaler_status_heartbeat/","title":"ScalerStatusMessage","text":""},{"location":"odm/messages/scaler_status_heartbeat/#scalerstatusmessage","title":"ScalerStatusMessage","text":"<p>Model of Scaler's Status Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.scaler_status_heartbeat.ScalerStatusMessage\"</code>  Yes <code>assemblyline.odm.messages.scaler_status_heartbeat.ScalerStatusMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ScalerStatusHeartbeat\"</code>  Yes <code>ScalerStatusHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/scaler_status_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Hearbeat Model</p> Field Type Description Required Default service_name Keyword Name of service  Yes <code>None</code> metrics Status Status of service  Yes <code>None</code>"},{"location":"odm/messages/scaler_status_heartbeat/#status","title":"Status","text":"<p>Service Status Model</p> Field Type Description Required Default running Integer Number of instances running  Yes <code>None</code> target Integer Target scaling for service  Yes <code>None</code> minimum Integer Minimum number of instances  Yes <code>None</code> maximum Integer Maximum number of instances  Yes <code>None</code> dynamic_maximum Integer Dynamic maximum number of instances  Yes <code>None</code> queue Integer Service queue  Yes <code>None</code> pressure Float Service pressure  Yes <code>None</code> duty_cycle Float Duty Cycle  Yes <code>None</code>"},{"location":"odm/messages/service_heartbeat/","title":"ServiceMessage","text":""},{"location":"odm/messages/service_heartbeat/#servicemessage","title":"ServiceMessage","text":"<p>Model of Service Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.service_heartbeat.ServiceMessage\"</code>  Yes <code>assemblyline.odm.messages.service_heartbeat.ServiceMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ServiceHeartbeat\"</code>  Yes <code>ServiceHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/service_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default activity Activity Service activity  Yes <code>None</code> instances Integer Service instances  Yes <code>None</code> metrics Metrics Service metrics  Yes <code>None</code> queue Integer Service queue  Yes <code>None</code> service_name Keyword Service name  Yes <code>None</code>"},{"location":"odm/messages/service_heartbeat/#activity","title":"Activity","text":"<p>Service Activity</p> Field Type Description Required Default busy Integer Number of busy instances  Yes <code>None</code> idle Integer Number of idle instances  Yes <code>None</code>"},{"location":"odm/messages/service_heartbeat/#metrics","title":"Metrics","text":"<p>Service Metrics</p> Field Type Description Required Default cache_hit Integer Number of cache hits  Yes <code>None</code> cache_miss Integer Number of cache misses  Yes <code>None</code> cache_skipped Integer Number of cache skips  Yes <code>None</code> execute Integer Number of service executes  Yes <code>None</code> fail_recoverable Integer Number of recoverable fails  Yes <code>None</code> fail_nonrecoverable Integer Number of non-recoverable fails  Yes <code>None</code> scored Integer Number of tasks scored  Yes <code>None</code> not_scored Integer Number of tasks not scored  Yes <code>None</code>"},{"location":"odm/messages/service_timing_heartbeat/","title":"ServiceTimingMessage","text":""},{"location":"odm/messages/service_timing_heartbeat/#servicetimingmessage","title":"ServiceTimingMessage","text":"<p>Model of Service Timing Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Heartbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.service_heartbeat.ServiceTimingMessage\"</code>  Yes <code>assemblyline.odm.messages.service_heartbeat.ServiceTimingMessage</code> msg_type Enum Type of messageSupported values are:<code>\"ServiceTimingHeartbeat\"</code>  Yes <code>ServiceTimingHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/service_timing_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Hearbeat Model</p> Field Type Description Required Default instances Integer Number of instances  Yes <code>None</code> metrics Metrics Metrics  Yes <code>None</code> queue Integer Queue size  Yes <code>None</code> service_name Keyword Name of service  Yes <code>None</code>"},{"location":"odm/messages/service_timing_heartbeat/#metrics","title":"Metrics","text":"<p>Timing Metrics</p> Field Type Description Required Default execution PerformanceTimer Excution time  Yes <code>None</code> execution_count Integer Number of executes  Yes <code>None</code> idle PerformanceTimer Idle time  Yes <code>None</code> idle_count Integer Number of idles  Yes <code>None</code>"},{"location":"odm/messages/submission/","title":"SubmissionMessage","text":""},{"location":"odm/messages/submission/#submissionmessage","title":"SubmissionMessage","text":"<p>Model of Submission Message</p> Field Type Description Required Default msg Submission Body of the message  Yes <code>None</code> msg_loader Enum Class to use to load the message as an objectSupported values are:<code>\"assemblyline.odm.messages.submission.SubmissionMessage\"</code>  Yes <code>assemblyline.odm.messages.submission.SubmissionMessage</code> msg_type Enum Type of messageSupported values are:<code>\"SubmissionCompleted\", \"SubmissionIngested\", \"SubmissionReceived\", \"SubmissionStarted\"</code>  Yes <code>None</code> sender Keyword Sender of the message  Yes <code>None</code>"},{"location":"odm/messages/submission/#submission","title":"Submission","text":"<p>Submission Model</p> Field Type Description Required Default sid UUID Submission ID to use  Yes <code>None</code> time Date Message time  Yes <code>NOW</code> files List [File] File block  Yes <code>[]</code> metadata FlatMapping Metadata submitted with the file  Yes <code>{}</code> notification Notification Notification queue parameters  Yes See Notification for more details. params SubmissionParams Parameters of the submission  Yes <code>None</code> scan_key Keyword None  Optional <code>None</code> file_tree Any File tree of the files in this submission  Yes <code>{}</code> file_infos Mapping [String, Any] SHA256 and file information in the file.  Yes <code>{}</code> errors List [Keyword] List of error keys  Yes <code>[]</code> results Mapping [String, Any] Result key value mapping  Yes <code>{}</code>"},{"location":"odm/messages/submission/#notification","title":"Notification","text":"<p>Notification Model</p> Field Type Description Required Default queue Keyword Queue to publish the completion message  Optional <code>None</code> threshold Integer Notify only if this score threshold is met  Optional <code>None</code>"},{"location":"odm/messages/task/","title":"TaskMessage","text":""},{"location":"odm/messages/task/#taskmessage","title":"TaskMessage","text":"<p>Model for Service Task Message</p> Field Type Description Required Default msg Task Body of the message  Yes <code>None</code> msg_loader Enum Class to use to load the message as an objectSupported values are:<code>\"assemblyline.odm.messages.task.TaskMessage\"</code>  Yes <code>assemblyline.odm.messages.task.TaskMessage</code> msg_type Enum Type of messageSupported values are:<code>\"Task\"</code>  Yes <code>Task</code> sender Keyword Sender of the message  Yes <code>None</code>"},{"location":"odm/messages/task/#task","title":"Task","text":"<p>Service Task Model</p> Field Type Description Required Default sid UUID Submission ID  Yes <code>None</code> metadata FlatMapping Metadata associated to the submission  Yes <code>{}</code> min_classification Classification Minimum classification of the file being scanned  Yes <code>None</code> fileinfo FileInfo File info block  Yes <code>None</code> filename Keyword File name  Yes <code>None</code> service_name Keyword Service name  Yes <code>None</code> service_config Mapping [String, Any] Service specific parameters  Yes <code>{}</code> depth Integer File depth relative to initital submitted file  Yes <code>0</code> max_files Integer Maximum number of files that submission can have  Yes <code>None</code> ttl Integer Task TTL  Yes <code>0</code> tags List [TagItem] List of tags  Yes <code>[]</code> temporary_submission_data List [DataItem] Temporary submission data  Yes <code>[]</code> deep_scan Boolean Perform deep scanning  Yes <code>False</code> ignore_cache Boolean Whether the service cache should be ignored during the processing of this task  Yes <code>False</code> ignore_recursion_prevention Boolean Whether the service should ignore recursion prevention or not  Yes <code>False</code> ignore_filtering Boolean Should the service filter it's output?  Yes <code>False</code> priority Integer Priority for processing order  Yes <code>1</code> safelist_config ServiceSafelist Safelisting configuration (as defined in global configuration)  Yes See ServiceSafelist for more details."},{"location":"odm/messages/task/#dataitem","title":"DataItem","text":"<p>Data Item</p> Field Type Description Required Default name Keyword None  Yes <code>None</code> value Any None  Yes <code>None</code>"},{"location":"odm/messages/task/#fileinfo","title":"FileInfo","text":"<p>File Information</p> Field Type Description Required Default magic Keyword The output from libmagic which was used to determine the tag  Yes <code>None</code> md5 MD5 MD5 of the file  Yes <code>None</code> mime Keyword The libmagic mime type  Optional <code>None</code> sha1 SHA1 SHA1 hash of the file  Yes <code>None</code> sha256 SHA256 SHA256 hash of the file  Yes <code>None</code> size Integer Size of the file in bytes  Yes <code>None</code> ssdeep SSDeepHash None  Optional <code>None</code> tlsh Keyword None  Optional <code>None</code> type Keyword Type of file as identified by Assemblyline  Yes <code>None</code> uri_info URIInfo URI structure to speed up specialty file searching  Optional <code>None</code>"},{"location":"odm/messages/task/#tagitem","title":"TagItem","text":"<p>Tag Item</p> Field Type Description Required Default type Keyword Type of tag item  Yes <code>None</code> value Keyword Value of tag item  Yes <code>None</code> short_type Keyword Short version of tag type  Yes <code>None</code> score Integer Score of tag item  Optional <code>None</code>"},{"location":"odm/messages/vacuum_heartbeat/","title":"VacuumMessage","text":""},{"location":"odm/messages/vacuum_heartbeat/#vacuummessage","title":"VacuumMessage","text":"<p>Model of Vacuum Heartbeat Message</p> Field Type Description Required Default msg Heartbeat Hearbeat message  Yes <code>None</code> msg_loader Enum Loader class for messageSupported values are:<code>\"assemblyline.odm.messages.vacuum_heartbeat.VacuumMessage\"</code>  Yes <code>assemblyline.odm.messages.vacuum_heartbeat.VacuumMessage</code> msg_type Enum Type of messageSupported values are:<code>\"VacuumHeartbeat\"</code>  Yes <code>VacuumHeartbeat</code> sender Keyword Sender of message  Yes <code>None</code>"},{"location":"odm/messages/vacuum_heartbeat/#heartbeat","title":"Heartbeat","text":"<p>Heartbeat Model</p> Field Type Description Required Default metrics Metrics Vacuum metrics  Yes <code>None</code>"},{"location":"odm/messages/vacuum_heartbeat/#metrics","title":"Metrics","text":"<p>Vacuum Stats</p> Field Type Description Required Default ingested Integer Files ingested  Yes <code>None</code> safelist Integer Files safelisted  Yes <code>None</code> errors Integer None  Yes <code>None</code> skipped Integer None  Yes <code>None</code>"},{"location":"odm/models/actions/","title":"PostprocessAction","text":""},{"location":"odm/models/actions/#postprocessaction","title":"PostprocessAction","text":"<p>Postprocessing Action</p> Field Type Description Required Default enabled Boolean Is this action active  Yes <code>False</code> run_on_cache Boolean Should this action run on cache hits  Yes <code>False</code> run_on_completed Boolean Should this action run on newly completed submissions  Yes <code>False</code> filter Keyword Query string to select submissions  Yes <code>None</code> webhook Webhook Webhook action configuration  Optional <code>None</code> raise_alert Boolean Raise an alert when this action is triggered  Yes <code>False</code> resubmit ResubmitOptions Resubmission configuration  Optional <code>None</code> archive_submission Boolean Archive the submission when this action is triggered  Yes <code>False</code> use_archive_alternate_dtl Boolean Should we use the alternate dtl while archiving?  Yes <code>False</code>"},{"location":"odm/models/actions/#resubmitoptions","title":"ResubmitOptions","text":"<p>Resubmission Options</p> Field Type Description Required Default additional_services List [Keyword] None  Yes <code>None</code> random_below Integer None  Optional <code>None</code>"},{"location":"odm/models/actions/#webhook","title":"Webhook","text":"<p>Webhook Configuration</p> Field Type Description Required Default password Keyword Password used to authenticate with source  Optional `` ca_cert Keyword CA cert for source  Optional `` ssl_ignore_errors Boolean Ignore SSL errors when reaching out to source?  Yes <code>False</code> proxy Keyword Proxy server for source  Optional `` method Keyword HTTP method used to access webhook  Yes <code>POST</code> uri Keyword URI to source  Yes <code>None</code> username Keyword Username used to authenticate with source  Optional `` headers List [NamedValue] Headers  Yes <code>[]</code> retries Integer None  Yes <code>3</code>"},{"location":"odm/models/actions/#namedvalue","title":"NamedValue","text":"<p>Named Value</p> Field Type Description Required Default name Keyword Name  Yes <code>None</code> value Keyword Value  Yes <code>None</code>"},{"location":"odm/models/alert/","title":"Alert","text":""},{"location":"odm/models/alert/#alert","title":"Alert","text":"<p>The Alert object model, as defined in this documentation, specifies the structured representation of alert data within the Assemblyline application's Alert index. Each field delineated in the schema is an attribute of the Alert document, characterized by its data type, semantic definition, mandatory status, and default instantiation value.</p> <p>Comprehension of this schema is pivotal for the construction of targeted Lucene search queries, which are instrumental in the interrogation and retrieval of alert-specific data from Assemblyline's analytical output. The schema's fields provide the analytical lexicon necessary to query and dissect alert data, facilitating the isolation of alerts based on defined parameters such as threat identifiers, heuristic evaluations, and temporal metadata.</p> <p>This schema serves as a technical blueprint for cybersecurity professionals to navigate Assemblyline's alerting system, enabling refined query strategies and data extraction methodologies that align with operational cybersecurity imperatives and threat intelligence workflows.</p> Field Type Description Required Default alert_id Keyword Unique identifier for the alert.  Yes <code>None</code> al ALResults Contains the results of the Assemblyline analysis for the alert.  Yes <code>None</code> archive_ts Date Timestamp indicating when the alert was archived in the system.  Optional <code>None</code> attack Attack Structured data representing MITRE ATT&amp;CK information associated with the alert.  Yes <code>None</code> classification Classification Security classification assigned to the alert based on its contents and context.  Yes <code>None</code> expiry_ts Date Timestamp indicating when the alert is scheduled to expire from the system.  Optional <code>None</code> extended_scan Enum Indicates the status of an extended scan, if applicable. Extended scans are additional analyses performed after the initial analysis.Supported values are:<code>\"completed\", \"incomplete\", \"skipped\", \"submitted\"</code>  Yes <code>None</code> file File Information about the file associated with the alert.  Yes <code>None</code> filtered Boolean Indicates whether portions of the submission's analysis results have been omitted due to the user's classification level not meeting the required threshold for viewing certain data.  Yes <code>False</code> heuristic Heuristic Data regarding the heuristics that triggered the alert.  Yes <code>None</code> label List [Keyword] Labels assigned to the alert for categorization and filtering.  Yes <code>[]</code> metadata FlatMapping Additional metadata provided with the file at the time of submission.  Yes <code>{}</code> owner Keyword Specifies the user or system component that has taken ownership of the alert. If no user has claimed the alert, it remains under system ownership with no specific user associated, indicated by a value of <code>None</code>.  Optional <code>None</code> priority Enum Indicates the importance level assigned to the alert.Supported values are:<code>\"CRITICAL\", \"HIGH\", \"LOW\", \"MEDIUM\", None</code>  Optional <code>None</code> reporting_ts Date Timestamp when the alert was created.  Yes <code>None</code> submission_relations List [Relationship] Describes the hierarchical relationships between submissions that contributed to this alert.  Yes <code>None</code> sid UUID Identifier for the submission associated with this alert.  Yes <code>None</code> status Enum Reflects the current state of the alert throughout its lifecycle. This status is subject to change as a result of user actions, automated processes, or the execution of workflows within Assemblyline. The status provides insight into the current phase of analysis or response.Supported values are:<code>\"ASSESS\", \"MALICIOUS\", \"NON-MALICIOUS\", \"TRIAGE\", None</code>  Optional <code>None</code> ts Date Timestamp of when the file submission occurred that led to the generation of this alert.  Yes <code>None</code> type Keyword The type or category of the alert as specified at submission time by the user.  Yes <code>None</code> verdict Verdict Consolidates user assessments of the submission's nature. It records the user identifiers of those who have evaluated the submission, categorizing it as either malicious or non-malicious.  Yes See Verdict for more details. events List [Event] An audit trail of events and actions taken on the alert.  Yes <code>[]</code> workflows_completed Boolean Flag indicating whether all configured workflows have been executed for this alert.  Yes <code>False</code>"},{"location":"odm/models/alert/#alresults","title":"ALResults","text":"<p>Contains the aggregated results of the analysis performed by Assemblyline. It includes information such as attribution, behaviors observed, domains and IPs related to the threat, and the overall score indicating the severity of the findings.</p> Field Type Description Required Default attrib List [Keyword] A list of attribution tags that provide context by suggesting associations with known malware families, suspected threat actors, or ongoing campaigns.  Yes <code>[]</code> av List [Keyword] List of antivirus signatures that matched the file associated with the alert.  Yes <code>[]</code> behavior List [Keyword] Descriptions of behaviors exhibited by the analyzed file or artifact that led to the alert.  Yes <code>[]</code> detailed DetailedResults Provides a more detailed breakdown of the analysis results.  Yes <code>None</code> domain List [Domain] Aggregate list of domains related to the alert, derived from both static and dynamic analysis.  Yes <code>[]</code> domain_dynamic List [Domain] List of domains observed during dynamic analysis of the artifact.  Yes <code>[]</code> domain_static List [Domain] List of domains extracted from static analysis of the artifact.  Yes <code>[]</code> ip List [IP] Aggregate list of IP addresses related to the alert, derived from both static and dynamic analysis.  Yes <code>[]</code> ip_dynamic List [IP] List of IP addresses observed during dynamic analysis of the artifact.  Yes <code>[]</code> ip_static List [IP] List of IP addresses extracted from static analysis of the artifact.  Yes <code>[]</code> request_end_time Date The timestamp indicating when the processing of the submission completed.  Yes <code>None</code> score Integer The highest score assigned to any part of the submission based on the analysis results.  Yes <code>None</code> uri List [URI] Aggregate list of URIs related to the alert, derived from both static and dynamic analysis.  Yes <code>[]</code> uri_dynamic List [URI] List of URIs observed during dynamic analysis of the artifact.  Yes <code>[]</code> uri_static List [URI] List of URIs extracted from static analysis of the artifact.  Yes <code>[]</code> yara List [Keyword] List of YARA rule matches that contributed to the alert.  Yes <code>[]</code>"},{"location":"odm/models/alert/#detailedresults","title":"DetailedResults","text":"<p>Provides a comprehensive breakdown of specific attributes and their associated analysis results.</p> Field Type Description Required Default attack_pattern List [DetailedItem] MITRE ATT&amp;CK\u00ae framework patterns identified in the analysis.  Yes <code>[]</code> attack_category List [DetailedItem] MITRE ATT&amp;CK\u00ae framework categories associated with the alert.  Yes <code>[]</code> attrib List [DetailedItem] Attribution information that provides context by suggesting associations with known malware families, suspected threat actors, or ongoing campaigns.  Yes <code>[]</code> av List [DetailedItem] Information on antivirus signature matches.  Yes <code>[]</code> behavior List [DetailedItem] Descriptions of the behaviors exhibited by the analyzed file or artifact that led to the alert.  Yes <code>[]</code> domain List [DetailedItem] Domain information related to the alert.  Yes <code>[]</code> heuristic List [DetailedItem] Heuristic information that triggered the alert.  Yes <code>[]</code> ip List [DetailedItem] IP address information related to the alert.  Yes <code>[]</code> uri List [DetailedItem] URI information related to the alert.  Yes <code>[]</code> yara List [DetailedItem] Information on YARA rule matches that contributed to the alert.  Yes <code>[]</code>"},{"location":"odm/models/alert/#detaileditem","title":"DetailedItem","text":"<p>Represents a granular element within the detailed analysis results, providing specific insights into the analysis findings.</p> Field Type Description Required Default type Keyword Defines the specific attribute or aspect of the analysis that this detailed item pertains to.  Yes <code>None</code> value Keyword The specific value or identifier for the detail item.  Yes <code>None</code> verdict Enum Security assessment of the detailed item.Supported values are:<code>\"highly suspicious\", \"info\", \"malicious\", \"safe\", \"suspicious\"</code>  Yes <code>None</code> subtype Enum Specifies the item's subtype (e.g., CFG, EXP, IMP, OB, TA).Supported values are:<code>\"CFG\", \"EXP\", \"IMP\", \"OB\", \"TA\"</code>  Optional <code>None</code>"},{"location":"odm/models/alert/#attack","title":"Attack","text":"<p>The Attack submodel is a component of the Alert model that records information aligned with the MITRE ATT&amp;CK framework. It lists the ATT&amp;CK patterns and categories that have been identified in the analysis, helping to map the threat to known adversary tactics and techniques.</p> Field Type Description Required Default pattern List [Keyword] List of MITRE ATT&amp;CK\u00ae framework patterns that are relevant to the alert.  Yes <code>[]</code> category List [Keyword] List of MITRE ATT&amp;CK\u00ae framework categories that are relevant to the alert.  Yes <code>[]</code>"},{"location":"odm/models/alert/#event","title":"Event","text":"<p>Describes an event or action that has occurred during the lifecycle of the alert, capturing changes in status, priority, or labels.</p> Field Type Description Required Default entity_type Enum The type of entity associated with the event.Supported values are:<code>\"user\", \"workflow\"</code>  Yes <code>None</code> entity_id Keyword The unique identifier of the entity associated with the event.  Yes <code>None</code> entity_name Keyword The name of the entity associated with the event.  Yes <code>None</code> ts Date The timestamp when the event occurred.  Yes <code>NOW</code> labels List [Keyword] Labels that were added to the alert during the event.  Yes <code>[]</code> labels_removed List [Keyword] Labels that were removed from the alert during the event.  Yes <code>[]</code> status Enum The status of the alert after the event took place.Supported values are:<code>\"ASSESS\", \"MALICIOUS\", \"NON-MALICIOUS\", \"TRIAGE\"</code>  Optional <code>None</code> priority Enum The priority level assigned to the alert during the event.Supported values are:<code>\"CRITICAL\", \"HIGH\", \"LOW\", \"MEDIUM\"</code>  Optional <code>None</code>"},{"location":"odm/models/alert/#file","title":"File","text":"<p>Captures comprehensive metadata and unique identifiers for the original file submitted for analysis, which is central to the generation of the alert.</p> Field Type Description Required Default md5 MD5 The MD5 hash of the file.  Yes <code>None</code> name Keyword The original name of the file as submitted.  Yes <code>None</code> sha1 SHA1 The SHA1 hash of the file.  Yes <code>None</code> sha256 SHA256 The SHA256 hash of the file.  Yes <code>None</code> size Long The size of the file in bytes.  Yes <code>None</code> type Keyword The file type as identified by Assemblyline's analysis.  Yes <code>None</code> screenshots List [Screenshot] Screenshots taken of the file during analysis, if applicable.  Yes <code>[]</code>"},{"location":"odm/models/alert/#screenshot","title":"Screenshot","text":"<p>Stores information about screenshots taken during the analysis of the file. Each screenshot has a name, description, and the hashes of the image and its thumbnail, offering a visual reference that can aid in manual review processes.</p> Field Type Description Required Default name Keyword The name or title of the screenshot.  Yes <code>None</code> description Keyword A brief description of the screenshot's content.  Yes <code>None</code> img SHA256 The SHA256 hash of the full-size screenshot image.  Yes <code>None</code> thumb SHA256 The SHA256 hash of the thumbnail version of the screenshot.  Yes <code>None</code>"},{"location":"odm/models/alert/#heuristic","title":"Heuristic","text":"<p>Summarizes the heuristics that were triggered during the analysis. These heuristics are part of the detection logic used by Assemblyline to identify suspicious or malicious behavior in the analyzed file.</p> Field Type Description Required Default name List [Keyword] Names of the heuristics that have been matched in the analysis.  Yes <code>[]</code>"},{"location":"odm/models/alert/#relationship","title":"Relationship","text":"<p>Describes the relationship between different submissions that are linked to the formation of the alert, highlighting parent-child connections.</p> Field Type Description Required Default child UUID The identifier of the child submission in the relationship.  Yes <code>None</code> parent UUID The identifier of the parent submission, if applicable.  Optional <code>None</code>"},{"location":"odm/models/alert/#verdict","title":"Verdict","text":"<p>The Verdict submodel captures the conclusions drawn by users regarding the nature of a submission. It lists user identifiers for those who have deemed the submission as either malicious or non-malicious, representing a collective assessment of the threat.</p> Field Type Description Required Default malicious List [Keyword] User IDs of those who have marked the alert as malicious.  Yes <code>[]</code> non_malicious List [Keyword] User IDs of those who have marked the alert as non-malicious.  Yes <code>[]</code>"},{"location":"odm/models/apikey/","title":"Apikey","text":""},{"location":"odm/models/apikey/#apikey","title":"Apikey","text":"<p>Model of Apikey</p> Field Type Description Required Default acl List [Enum] Access Control List for the API key  Yes <code>None</code> password Keyword BCrypt hash of the password for the apikey  Yes <code>None</code> roles List [Enum] List of roles tied to the API key  Yes <code>[]</code> uname Keyword Username  Yes <code>None</code> key_name Keyword Name of the key  Yes <code>None</code> creation_date Date The date this API key is created.  Yes <code>NOW</code> expiry_ts Date Expiry timestamp.  Optional <code>None</code> last_used Date The last time this API key was used.  Optional <code>None</code>"},{"location":"odm/models/badlist/","title":"Badlist","text":""},{"location":"odm/models/badlist/#badlist","title":"Badlist","text":"<p>Badlist Model</p> Field Type Description Required Default added Date Date when the badlisted hash was added  Yes <code>NOW</code> attribution Attribution Attribution related to the bad hash  Optional <code>None</code> classification Classification Computed max classification for the bad hash  Yes <code>None</code> enabled Boolean Is bad hash enabled or not?  Yes <code>True</code> expiry_ts Date When does this item expire from the list?  Optional <code>None</code> hashes Hashes List of hashes related to the bad hash  Yes See Hashes for more details. file File Information about the file  Optional <code>None</code> sources List [Source] List of reasons why hash is badlisted  Yes <code>None</code> tag Tag Information about the tag  Optional <code>None</code> type Enum Type of bad hashSupported values are:<code>\"file\", \"tag\"</code>  Yes <code>None</code> updated Date Last date when sources were added to the bad hash  Yes <code>NOW</code>"},{"location":"odm/models/badlist/#attribution","title":"Attribution","text":"<p>Attribution Tag Model</p> Field Type Description Required Default actor List [UpperKeyword] Attribution Actor  Optional <code>None</code> campaign List [UpperKeyword] Attribution Campaign  Optional <code>None</code> category List [UpperKeyword] Attribution Category  Optional <code>None</code> exploit List [UpperKeyword] Attribution Exploit  Optional <code>None</code> implant List [UpperKeyword] Attribution Implant  Optional <code>None</code> family List [UpperKeyword] Attribution Family  Optional <code>None</code> network List [UpperKeyword] Attribution Network  Optional <code>None</code>"},{"location":"odm/models/badlist/#file","title":"File","text":"<p>File Details</p> Field Type Description Required Default name List [Keyword] List of names seen for that file  Yes <code>[]</code> size Long Size of the file in bytes  Optional <code>None</code> type Keyword Type of file as identified by Assemblyline  Optional <code>None</code>"},{"location":"odm/models/badlist/#hashes","title":"Hashes","text":"<p>Hashes of a badlisted file</p> Field Type Description Required Default md5 MD5 MD5  Optional <code>None</code> sha1 SHA1 SHA1  Optional <code>None</code> sha256 SHA256 SHA256  Optional <code>None</code> ssdeep SSDeepHash SSDEEP  Optional <code>None</code> tlsh Keyword None  Optional <code>None</code>"},{"location":"odm/models/badlist/#source","title":"Source","text":"<p>Badlist source</p> Field Type Description Required Default classification Classification Classification of the source  Yes <code>TLP:C</code> name Keyword Name of the source  Yes <code>None</code> reason List [Keyword] Reason for why file was badlisted  Yes <code>None</code> type Enum Type of badlisting sourceSupported values are:<code>\"external\", \"user\"</code>  Yes <code>None</code>"},{"location":"odm/models/badlist/#tag","title":"Tag","text":"<p>Tag associated to file</p> Field Type Description Required Default type Keyword Tag type  Yes <code>None</code> value Keyword Tag value  Yes <code>None</code>"},{"location":"odm/models/cached_file/","title":"CachedFile","text":""},{"location":"odm/models/cached_file/#cachedfile","title":"CachedFile","text":"<p>CachedFile Model</p> Field Type Description Required Default component Keyword Name of component which created the file  Yes <code>None</code> expiry_ts Date Expiry timestamp  Yes <code>None</code>"},{"location":"odm/models/config/","title":"Config","text":""},{"location":"odm/models/config/#config","title":"Config","text":"<p>Assemblyline Deployment Configuration</p> Field Type Description Required Default auth Auth Authentication module configuration  Yes See Auth for more details. core Core Core component configuration  Yes See Core for more details. datastore Datastore Datastore configuration  Yes See Datastore for more details. datasources Mapping [String, Datasource] Datasources configuration  Yes See Datasource for more details. filestore Filestore Filestore configuration  Yes See Filestore for more details. logging Logging Logging configuration  Yes See Logging for more details. retrohunt Retrohunt Retrohunt configuration for the frontend and server.  Yes See Retrohunt for more details. services Services Service configuration  Yes See Services for more details. submission Submission Options for how submissions will be processed  Yes See Submission for more details. system System System configuration  Yes See System for more details. ui UI UI configuration parameters  Yes See UI for more details."},{"location":"odm/models/config/#auth","title":"Auth","text":"<p>Authentication Methods</p> Field Type Description Required Default allow_2fa Boolean Allow 2FA?  Yes <code>True</code> allow_apikeys Boolean Allow API keys?  Yes <code>True</code> apikey_max_dtl Integer None  Optional <code>None</code> allow_extended_apikeys Boolean Allow extended API keys?  Yes <code>True</code> allow_security_tokens Boolean Allow security tokens?  Yes <code>True</code> internal Internal Internal authentication settings  Yes See Internal for more details. ldap LDAP LDAP settings  Yes See LDAP for more details. oauth OAuth OAuth settings  Yes See OAuth for more details. saml SAML SAML settings  Yes See SAML for more details."},{"location":"odm/models/config/#internal","title":"Internal","text":"<p>Internal Authentication Configuration</p> Field Type Description Required Default enabled Boolean Internal authentication allowed?  Yes <code>True</code> failure_ttl Integer How long to wait after <code>max_failures</code> before re-attempting login?  Yes <code>60</code> max_failures Integer Maximum number of fails allowed before timeout  Yes <code>5</code> password_requirements PasswordRequirement Password requirements  Yes See PasswordRequirement for more details. signup Signup Signup method  Yes See Signup for more details. ip_filter List [ValidatedKeyword] List of CIDRs allowed to access internal authentication  Optional <code>None</code>"},{"location":"odm/models/config/#passwordrequirement","title":"PasswordRequirement","text":"<p>Password Requirement</p> Field Type Description Required Default lower Boolean Password must contain lowercase letters  Yes <code>False</code> number Boolean Password must contain numbers  Yes <code>False</code> special Boolean Password must contain special characters  Yes <code>False</code> upper Boolean Password must contain uppercase letters  Yes <code>False</code> min_length Integer Minimum password length  Yes <code>12</code>"},{"location":"odm/models/config/#signup","title":"Signup","text":"<p>Signup Configuration</p> Field Type Description Required Default enabled Boolean Can a user automatically signup for the system  Yes <code>False</code> smtp SMTP Signup via SMTP  Yes See SMTP for more details. notify Notify Signup via GC Notify  Yes See Notify for more details. valid_email_patterns List [Keyword] Email patterns that will be allowed to automatically signup for an account  Yes <code>['.*', '.*@localhost']</code>"},{"location":"odm/models/config/#notify","title":"Notify","text":"<p>Configuration block for GC Notify signup and password reset</p> Field Type Description Required Default base_url Keyword Base URL  Optional <code>None</code> api_key Keyword API key  Optional <code>None</code> registration_template Keyword Registration template  Optional <code>None</code> password_reset_template Keyword Password reset template  Optional <code>None</code> authorization_template Keyword Authorization template  Optional <code>None</code> activated_template Keyword Activated Template  Optional <code>None</code>"},{"location":"odm/models/config/#smtp","title":"SMTP","text":"<p>Configuration block for SMTP signup and password reset</p> Field Type Description Required Default from_adr Keyword Email address used for sender  Optional <code>None</code> host Keyword SMTP host  Optional <code>None</code> password Keyword Password for SMTP server  Optional <code>None</code> port Integer Port of SMTP server  Yes <code>587</code> tls Boolean Should we communicate with SMTP server via TLS?  Yes <code>True</code> user Keyword User to authenticate to the SMTP server  Optional <code>None</code>"},{"location":"odm/models/config/#ldap","title":"LDAP","text":"<p>LDAP Configuration</p> Field Type Description Required Default enabled Boolean Should LDAP be enabled or not?  Yes <code>False</code> ip_filter List [ValidatedKeyword] List of CIDRs allowed to access internal authentication  Optional <code>None</code> admin_dn Keyword DN of the group or the user who will get admin privileges  Optional <code>None</code> bind_user Keyword User use to query the LDAP server  Optional <code>None</code> bind_pass Keyword Password used to query the LDAP server  Optional <code>None</code> auto_create Boolean Auto-create users if they are missing  Yes <code>True</code> auto_sync Boolean Should we automatically sync with LDAP server on each login?  Yes <code>True</code> auto_properties List [AutoProperty] Automatic role and classification assignments  Yes <code>[]</code> base Keyword Base DN for the users  Yes <code>ou=people,dc=assemblyline,dc=local</code> email_field Keyword Name of the field containing the email address  Yes <code>mail</code> group_lookup_query Keyword How the group lookup is queried  Yes <code>(&amp;(objectClass=Group)(member=%s))</code> group_lookup_with_uid Boolean Use username/uid instead of dn for group lookup  Yes <code>False</code> image_field Keyword Name of the field containing the user's avatar  Yes <code>jpegPhoto</code> image_format Keyword Type of image used to store the avatar  Yes <code>jpeg</code> name_field Keyword Name of the field containing the user's name  Yes <code>cn</code> uid_field Keyword Field name for the UID  Yes <code>uid</code> uri Keyword URI to the LDAP server  Yes <code>ldap://localhost:389</code>"},{"location":"odm/models/config/#autoproperty","title":"AutoProperty","text":"<p>None</p> Field Type Description Required Default field Keyword Field to apply <code>pattern</code> to  Yes <code>None</code> pattern Keyword Regex pattern for auto-prop assignment  Yes <code>None</code> type Enum Type of property assignment on pattern matchSupported values are:<code>\"access\", \"api_daily_quota\", \"api_quota\", \"classification\", \"default_metadata\", \"group\", \"multi_group\", \"organization\", \"remove_role\", \"role\", \"submission_async_quota\", \"submission_daily_quota\", \"submission_quota\", \"type\"</code>  Yes <code>None</code> value List [Keyword] Assigned property value  Yes <code>[]</code>"},{"location":"odm/models/config/#oauth","title":"OAuth","text":"<p>OAuth Configuration</p> Field Type Description Required Default enabled Boolean Enable use of OAuth?  Yes <code>False</code> gravatar_enabled Boolean Enable gravatar?  Yes <code>True</code> providers Mapping [String, OAuthProvider] OAuth provider configuration  Yes See OAuthProvider for more details."},{"location":"odm/models/config/#oauthprovider","title":"OAuthProvider","text":"<p>OAuth Provider Configuration</p> Field Type Description Required Default auto_create Boolean Auto-create users if they are missing  Yes <code>True</code> auto_sync Boolean Should we automatically sync with OAuth provider?  Yes <code>False</code> auto_properties List [AutoProperty] Automatic role and classification assignments  Yes <code>[]</code> app_provider AppProvider None  Optional <code>None</code> ip_filter List [ValidatedKeyword] List of CIDRs allowed to access internal authentication  Optional <code>None</code> uid_randomize Boolean Should we generate a random username for the authenticated user?  Yes <code>False</code> uid_randomize_digits Integer How many digits should we add at the end of the username?  Yes <code>0</code> uid_randomize_delimiter Keyword What is the delimiter used by the random name generator?  Yes <code>-</code> uid_regex Keyword Regex used to parse an email address and capture parts to create a user ID out of it  Optional <code>None</code> uid_format Keyword Format of the user ID based on the captured parts from the regex  Optional <code>None</code> client_id Keyword ID of your application to authenticate to the OAuth provider  Optional <code>None</code> client_secret Keyword Password to your application to authenticate to the OAuth provider  Optional <code>None</code> redirect_uri Keyword URI to redirect to after authentication with OAuth provider  Optional <code>None</code> request_token_url Keyword URL to request token  Optional <code>None</code> request_token_params Mapping [String, Keyword] Parameters to request token  Optional <code>None</code> access_token_url Keyword URL to get access token  Optional <code>None</code> access_token_params Mapping [String, Keyword] Parameters to get access token  Optional <code>None</code> authorize_url Keyword URL used to authorize access to a resource  Optional <code>None</code> authorize_params Mapping [String, Keyword] Parameters used to authorize access to a resource  Optional <code>None</code> api_base_url Keyword Base URL for downloading the user's and groups info  Optional <code>None</code> client_kwargs Mapping [String, Keyword] Keyword arguments passed to the different URLs  Optional <code>None</code> jwks_uri Keyword URL used to verify if a returned JWKS token is valid  Optional <code>None</code> jwt_token_alg Keyword Algorythm use the validate JWT OBO tokens  Yes <code>RS256</code> uid_field Keyword Name of the field that will contain the user ID  Optional <code>None</code> user_get Keyword Path from the base_url to fetch the user info  Optional <code>None</code> user_groups Keyword Path from the base_url to fetch the group info  Optional <code>None</code> user_groups_data_field Keyword Field return by the group info API call that contains the list of groups  Optional <code>None</code> user_groups_name_field Keyword Name of the field in the list of groups that contains the name of the group  Optional <code>None</code> use_new_callback_format Boolean Should we use the new callback method?  Yes <code>False</code> allow_external_tokens Boolean Should token provided to the login API directly be use for authentication?  Yes <code>False</code> external_token_alternate_audiences List [Keyword] List of valid alternate audiences for the external token.  Yes <code>[]</code> email_fields List [Keyword] List of fields in the claim to get the email from  Yes <code>['email', 'emails', 'extension_selectedEmailAddress', 'otherMails', 'preferred_username', 'upn']</code> username_field Keyword Name of the field that will contain the username  Yes <code>uname</code> validate_token_with_secret Boolean Should we send the client secret while validating the access token?  Yes <code>False</code> identity_id_field Keyword Field to fetch the managed identity ID from.  Yes <code>oid</code> openid_connect_discovery_url Keyword URL for connecting to the OpenID configuration JSON.  Optional <code>None</code> groups_id_token_field Keyword Name of the field in the id token that contains the list of groups.  Yes <code>groups</code>"},{"location":"odm/models/config/#appprovider","title":"AppProvider","text":"<p>App provider</p> Field Type Description Required Default access_token_url Keyword URL used to get the access token  Yes <code>None</code> user_get Keyword Path from the base_url to fetch the user info  Optional <code>None</code> group_get Keyword Path from the base_url to fetch the group info  Optional <code>None</code> scope Keyword None  Yes <code>None</code> client_id Keyword ID of your application to authenticate to the OAuth  Optional <code>None</code> client_secret Keyword Password to your application to authenticate to the OAuth provider  Optional <code>None</code>"},{"location":"odm/models/config/#autoproperty_1","title":"AutoProperty","text":"<p>None</p> Field Type Description Required Default field Keyword Field to apply <code>pattern</code> to  Yes <code>None</code> pattern Keyword Regex pattern for auto-prop assignment  Yes <code>None</code> type Enum Type of property assignment on pattern matchSupported values are:<code>\"access\", \"api_daily_quota\", \"api_quota\", \"classification\", \"default_metadata\", \"group\", \"multi_group\", \"organization\", \"remove_role\", \"role\", \"submission_async_quota\", \"submission_daily_quota\", \"submission_quota\", \"type\"</code>  Yes <code>None</code> value List [Keyword] Assigned property value  Yes <code>[]</code>"},{"location":"odm/models/config/#saml","title":"SAML","text":"<p>SAML Configuration</p> Field Type Description Required Default enabled Boolean Enable use of SAML?  Yes <code>False</code> ip_filter List [ValidatedKeyword] List of CIDRs allowed to access internal authentication  Optional <code>None</code> auto_create Boolean Auto-create users if they are missing  Yes <code>True</code> auto_sync Boolean Should we automatically sync with SAML server on each login?  Yes <code>True</code> lowercase_urlencoding Boolean Enable lowercase encoding if using ADFS as IdP  Yes <code>False</code> attributes SAMLAttributes SAML attributes  Yes See SAMLAttributes for more details. settings SAMLSettings SAML settings method  Yes See SAMLSettings for more details."},{"location":"odm/models/config/#samlattributes","title":"SAMLAttributes","text":"<p>SAML Attributes</p> Field Type Description Required Default username_attribute Keyword SAML attribute name for AL username  Optional <code>uid</code> email_attribute Keyword SAML attribute name for a user's email address  Yes <code>email</code> fullname_attribute Keyword SAML attribute name for a user's first name  Yes <code>name</code> groups_attribute Keyword SAML attribute name for the groups  Yes <code>groups</code> classification_attribute Keyword SAML attribute name for cliassification  Yes <code>classification</code> dn_attribute Keyword SAML attribute name for user's LDAP DN  Yes <code>dn</code> roles_attribute Keyword SAML attribute name for the roles  Yes <code>roles</code> group_type_mapping Mapping [String, Keyword] SAML group to role mapping  Yes <code>{}</code>"},{"location":"odm/models/config/#samlsettings","title":"SAMLSettings","text":"<p>SAML Settings</p> Field Type Description Required Default strict Boolean Should we be strict in our SAML checks?  Yes <code>False</code> debug Boolean Should we be in debug mode?  Yes <code>False</code> sp SAMLServiceProvider SP settings  Yes <code>None</code> idp SAMLIdentityProvider IDP settings  Yes <code>None</code> security SAMLSecurity Security settings  Optional <code>None</code> contact_person SAMLContacts Contact settings  Optional <code>None</code> organization Mapping [String, SAMLOrganization] Organization settings  Optional <code>None</code>"},{"location":"odm/models/config/#samlcontacts","title":"SAMLContacts","text":"<p>SAML Contacts</p> Field Type Description Required Default technical SAMLContactPerson Technical Contact  Yes <code>None</code> support SAMLContactPerson Support Contact  Yes <code>None</code>"},{"location":"odm/models/config/#samlcontactperson","title":"SAMLContactPerson","text":"<p>SAML Contact Entry</p> Field Type Description Required Default given_name Keyword Given Name  Yes <code>None</code> email_address Keyword Email Address  Yes <code>None</code>"},{"location":"odm/models/config/#samlidentityprovider","title":"SAMLIdentityProvider","text":"<p>SAML Identity Provider</p> Field Type Description Required Default entity_id Keyword Entity ID  Yes <code>None</code> single_sign_on_service SAMLSingleSignOnService Single Sign On Service  Yes <code>None</code> x509cert Keyword X509 Certificate  Optional <code>None</code>"},{"location":"odm/models/config/#samlsinglesignonservice","title":"SAMLSingleSignOnService","text":"<p>SAML Single Sign On Service</p> Field Type Description Required Default url Keyword URL  Yes <code>None</code> binding Keyword Binding  Yes <code>urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect</code>"},{"location":"odm/models/config/#samlorganization","title":"SAMLOrganization","text":"<p>SAML Organization</p> Field Type Description Required Default name Keyword Name  Yes <code>None</code> display_name Keyword Display Name  Yes <code>None</code> url Keyword URL  Yes <code>None</code>"},{"location":"odm/models/config/#samlsecurity","title":"SAMLSecurity","text":"<p>SAML Security</p> Field Type Description Required Default name_id_encrypted Boolean Name ID Encrypted  Optional <code>None</code> authn_requests_signed Boolean Authn Requests Signed  Optional <code>None</code> logout_request_signed Boolean Logout Request Signed  Optional <code>None</code> logout_response_signed Boolean Logout Response Signed  Optional <code>None</code> sign_metadata Boolean Sign Metadata  Optional <code>None</code> want_messages_signed Boolean Want Messages Signed  Optional <code>None</code> want_assertions_signed Boolean Want Assertions Signed  Optional <code>None</code> want_assertions_encrypted Boolean Want Assertions Encrypted  Optional <code>None</code> want_name_id Boolean Want Name ID  Optional <code>None</code> want_name_id_encrypted Boolean Want Name ID Encrypted  Optional <code>None</code> want_attribute_statement Boolean Want Attribute Statement  Optional <code>None</code> requested_authn_context Boolean Requested Authn Context  Optional <code>None</code> requested_authn_context_comparison Keyword Requested Authn Context Comparison  Optional <code>None</code> fail_on_authn_context_mismatch Boolean Fail On Authn Context Mismatch  Optional <code>None</code> metadata_valid_until Keyword Metadata Valid Until  Optional <code>None</code> metadata_cache_duration Keyword Metadata Cache Duration  Optional <code>None</code> allow_single_label_domains Boolean Allow Single Label Domains  Optional <code>None</code> signature_algorithm Keyword Signature Algorithm  Optional <code>None</code> digest_algorithm Keyword Digest Algorithm  Optional <code>None</code> allow_repeat_attribute_name Boolean Allow Repeat Attribute Name  Optional <code>None</code> reject_deprecated_algorithm Boolean Reject Deprecated Algorithm  Optional <code>None</code>"},{"location":"odm/models/config/#samlserviceprovider","title":"SAMLServiceProvider","text":"<p>SAML Service Provider</p> Field Type Description Required Default entity_id Keyword Entity ID  Yes <code>None</code> assertion_consumer_service SAMLAssertionConsumerService Assertion Consumer Service  Yes <code>None</code> attribute_consuming_service SAMLAttributeConsumingService Attribute Consuming Service  Optional <code>None</code> name_id_format Keyword Name ID Format  Yes <code>urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified</code> x509cert Keyword X509 Certificate  Optional <code>None</code> private_key Keyword Private Key  Optional <code>None</code>"},{"location":"odm/models/config/#samlassertionconsumerservice","title":"SAMLAssertionConsumerService","text":"<p>SAML Assertion Consumer Service</p> Field Type Description Required Default url Keyword URL  Yes <code>None</code> binding Keyword Binding  Yes <code>urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST</code>"},{"location":"odm/models/config/#samlattributeconsumingservice","title":"SAMLAttributeConsumingService","text":"<p>SAML Attribute Consuming Service</p> Field Type Description Required Default service_name Keyword Service Name  Yes <code>None</code> service_description Keyword Service Description  Yes <code>None</code> requested_attributes List [SAMLRequestedAttribute] Requested Attributes  Yes <code>[]</code>"},{"location":"odm/models/config/#samlrequestedattribute","title":"# SAMLRequestedAttribute","text":"<p>SAML Attribute</p> Field Type Description Required Default name Keyword Name  Yes <code>None</code> is_required Boolean Is required?  Yes <code>False</code> name_format Keyword Name Format  Yes <code>urn:oasis:names:tc:SAML:2.0:attrname-format:unspecified</code> friendly_name Keyword Friendly Name  Yes `` attribute_value List [Keyword] Attribute Value  Yes <code>[]</code>"},{"location":"odm/models/config/#core","title":"Core","text":"<p>Core Component Configuration</p> Field Type Description Required Default alerter Alerter Configuration for Alerter  Yes See Alerter for more details. archiver Archiver Configuration for the permanent submission archive  Yes See Archiver for more details. dispatcher Dispatcher Configuration for Dispatcher  Yes See Dispatcher for more details. expiry Expiry Configuration for Expiry  Yes See Expiry for more details. ingester Ingester Configuration for Ingester  Yes See Ingester for more details. metrics Metrics Configuration for Metrics Collection  Yes See Metrics for more details. plumber Plumber Configuration for system cleanup  Yes See Plumber for more details. redis Redis Configuration for Redis instances  Yes See Redis for more details. scaler Scaler Configuration for Scaler  Yes See Scaler for more details. updater Updater Configuration for Updater  Yes See Updater for more details. vacuum Vacuum Configuration for Vacuum  Yes See Vacuum for more details."},{"location":"odm/models/config/#alerter","title":"Alerter","text":"<p>Alerter Configuration</p> Field Type Description Required Default alert_ttl Integer Time to live (days) for an alert in the system  Yes <code>90</code> default_group_field Keyword Default field used for alert grouping view  Yes <code>file.sha256</code> delay Integer Time in seconds that we give extended scans and workflow to complete their work before we start showing alerts in the alert viewer.  Yes <code>300</code> filtering_group_fields List [Keyword] List of group fields that when selected will ignore certain alerts where this field is missing.  Yes <code>['file.name', 'status', 'priority']</code> non_filtering_group_fields List [Keyword] List of group fields that are sure to be present in all alerts.  Yes <code>['file.md5', 'file.sha1', 'file.sha256']</code> process_alert_message Keyword Python path to the function that will process an alert message.  Yes <code>assemblyline_core.alerter.processing.process_alert_message</code> threshold Integer Minimum score to reach for a submission to be considered an alert.  Yes <code>500</code>"},{"location":"odm/models/config/#archiver","title":"Archiver","text":"<p>Malware Archive Configuration</p> Field Type Description Required Default alternate_dtl Integer Alternate number of days to keep the data in the malware archive. (0: Disabled, will keep data forever)  Yes <code>0</code> minimum_required_services List [Keyword] List of minimum required service before archiving takes place  Yes <code>[]</code> webhook Webhook Webhook to call before triggering the archiving process  Optional <code>None</code> use_webhook Boolean None  Optional <code>False</code>"},{"location":"odm/models/config/#webhook","title":"Webhook","text":"<p>Webhook Configuration</p> Field Type Description Required Default password Keyword Password used to authenticate with source  Optional `` ca_cert Keyword CA cert for source  Optional `` ssl_ignore_errors Boolean Ignore SSL errors when reaching out to source?  Yes <code>False</code> proxy Keyword Proxy server for source  Optional `` method Keyword HTTP method used to access webhook  Yes <code>POST</code> uri Keyword URI to source  Yes <code>None</code> username Keyword Username used to authenticate with source  Optional `` headers List [NamedValue] Headers  Yes <code>[]</code> retries Integer None  Yes <code>3</code>"},{"location":"odm/models/config/#namedvalue","title":"NamedValue","text":"<p>Named Value</p> Field Type Description Required Default name Keyword Name  Yes <code>None</code> value Keyword Value  Yes <code>None</code>"},{"location":"odm/models/config/#dispatcher","title":"Dispatcher","text":"<p>Dispatcher Configuration</p> Field Type Description Required Default timeout Integer Time between re-dispatching attempts, as long as some action (submission or any task completion) happens before this timeout ends, the timeout resets.  Yes <code>900</code> max_inflight Integer Maximum submissions allowed to be in-flight  Yes <code>1000</code>"},{"location":"odm/models/config/#expiry","title":"Expiry","text":"<p>None</p> Field Type Description Required Default batch_delete Boolean Perform expiry in batches?Delete queries are rounded by day therefore all delete operation happen at the same time at midnight  Yes <code>False</code> delay Integer Delay, in hours, that will be applied to the expiry query so we can keepdata longer then previously set or we can offset deletion during non busy hours  Yes <code>0</code> delete_storage Boolean Should we also cleanup the file storage?  Yes <code>True</code> sleep_time Integer Time, in seconds, to sleep in between each expiry run  Yes <code>15</code> workers Integer Number of concurrent workers  Yes <code>20</code> delete_workers Integer Worker processes for file storage deletes.  Yes <code>2</code> iteration_max_tasks Integer How many query chunks get run per iteration.  Yes <code>50</code> delete_batch_size Integer How large a batch get deleted per iteration.  Yes <code>2000</code> safelisted_tag_dtl Integer The default period, in days, before tags expire from Safelist  Yes <code>0</code> badlisted_tag_dtl Integer The default period, in days, before tags expire from Badlist  Yes <code>0</code>"},{"location":"odm/models/config/#ingester","title":"Ingester","text":"<p>Ingester Configuration</p> Field Type Description Required Default always_create_submission Boolean Always create submissions even on cache hit?  Yes <code>False</code> default_user Keyword Default user for bulk ingestion and unattended submissions  Yes <code>internal</code> default_services List [Keyword] Default service selection  Yes <code>[]</code> default_resubmit_services List [Keyword] Default service selection for resubmits  Yes <code>[]</code> description_prefix Keyword A prefix for descriptions. When a description is automatically generated, it will be the hash prefixed by this string  Yes <code>Bulk</code> is_low_priority Keyword Path to a callback function filtering ingestion tasks that should have their priority forcefully reset to low  Yes <code>assemblyline.common.null.always_false</code> get_whitelist_verdict Keyword None  Yes <code>assemblyline.common.signaturing.drop</code> whitelist Keyword None  Yes <code>assemblyline.common.null.whitelist</code> default_max_extracted Integer How many extracted files may be added to a Submission. Overrideable via submission parameters.  Yes <code>100</code> default_max_supplementary Integer How many supplementary files may be added to a Submission. Overrideable via submission parameters  Yes <code>100</code> expire_after Integer Period, in seconds, in which a task should be expired  Yes <code>1296000</code> stale_after_seconds Integer Drop a task altogether after this many seconds  Yes <code>86400</code> incomplete_expire_after_seconds Integer How long should scores be kept before expiry  Yes <code>3600</code> incomplete_stale_after_seconds Integer How long should scores be cached in the ingester  Yes <code>1800</code> sampling_at Mapping [String, Integer] Thresholds at certain buckets before sampling  Yes <code>None</code> max_inflight Integer How long can a queue get before we start dropping files  Yes <code>500</code> cache_dtl Integer How long are files results cached  Yes <code>2</code>"},{"location":"odm/models/config/#metrics","title":"Metrics","text":"<p>Metrics Configuration</p> Field Type Description Required Default apm_server APMServer APM server configuration  Yes See APMServer for more details. elasticsearch ESMetrics Where to export metrics?  Yes See ESMetrics for more details. export_interval Integer How often should we be exporting metrics?  Yes <code>5</code> redis RedisServer Redis for Dashboard metrics  Yes See RedisServer for more details."},{"location":"odm/models/config/#apmserver","title":"APMServer","text":"<p>None</p> Field Type Description Required Default server_url Keyword URL to API server  Optional <code>None</code> token Keyword Authentication token for server  Optional <code>None</code>"},{"location":"odm/models/config/#esmetrics","title":"ESMetrics","text":"<p>None</p> Field Type Description Required Default hosts List [Keyword] Elasticsearch hosts  Optional <code>None</code> host_certificates Keyword Host certificates  Optional <code>None</code> warm Integer How long, per unit of time, should a document remain in the 'warm' tier?  Yes <code>2</code> cold Integer How long, per unit of time, should a document remain in the 'cold' tier?  Yes <code>30</code> delete Integer How long, per unit of time, should a document remain before being deleted?  Yes <code>90</code> unit Enum Unit of time used by <code>warm</code>, <code>cold</code>, <code>delete</code> phasesSupported values are:<code>\"d\", \"h\", \"m\"</code>  Yes <code>d</code>"},{"location":"odm/models/config/#redisserver","title":"RedisServer","text":"<p>Redis Service configuration</p> Field Type Description Required Default host Keyword Hostname of Redis instance  Yes <code>127.0.0.1</code> port Integer Port of Redis instance  Yes <code>6379</code>"},{"location":"odm/models/config/#plumber","title":"Plumber","text":"<p>Plumber Configuration</p> Field Type Description Required Default notification_queue_interval Integer Interval at which the notification queue cleanup should run  Yes <code>1800</code> notification_queue_max_age Integer Max age in seconds notification queue messages can be  Yes <code>86400</code>"},{"location":"odm/models/config/#redis","title":"Redis","text":"<p>Redis Configuration</p> Field Type Description Required Default nonpersistent RedisServer A volatile Redis instance  Yes See RedisServer for more details. persistent RedisServer A persistent Redis instance  Yes See RedisServer for more details."},{"location":"odm/models/config/#redisserver_1","title":"RedisServer","text":"<p>Redis Service configuration</p> Field Type Description Required Default host Keyword Hostname of Redis instance  Yes <code>127.0.0.1</code> port Integer Port of Redis instance  Yes <code>6379</code>"},{"location":"odm/models/config/#scaler","title":"Scaler","text":"<p>None</p> Field Type Description Required Default service_defaults ScalerServiceDefaults Defaults Scaler will assign to a service.  Yes <code>None</code> cpu_overallocation Float Percentage of CPU overallocation  Yes <code>1</code> memory_overallocation Float Percentage of RAM overallocation  Yes <code>1</code> overallocation_node_limit Integer None  Optional <code>None</code> additional_labels List [Text] Additional labels to be applied to services('=' delimited)  Optional <code>None</code> privileged_services_additional_labels List [Text] Additional labels to be applied to privileged services only('=' delimited)  Optional <code>None</code> linux_node_selector Selector Selector for linux nodes under kubernetes  Yes <code>None</code> cluster_pod_list Boolean Sets if scaler list pods for all namespaces. Disabling this lets you use stricter cluster roles but will make cluster resource usage less accurate, setting a namespace resource quota might be needed.  Yes <code>True</code> enable_pod_security Boolean Launch all containers in compliance with the 'Restricted' pod security standard.  Yes <code>False</code>"},{"location":"odm/models/config/#scalerservicedefaults","title":"ScalerServiceDefaults","text":"<p>A set of default values to be used running a service when no other value is set</p> Field Type Description Required Default growth Integer Period, in seconds, to wait before scaling up a service deployment  Yes <code>None</code> shrink Integer Period, in seconds, to wait before scaling down a service deployment  Yes <code>None</code> backlog Integer Backlog threshold that dictates scaling adjustments  Yes <code>None</code> min_instances Integer The minimum number of service instances to be running  Yes <code>None</code> environment List [EnvironmentVariable] Environment variables to pass onto services  Yes <code>[]</code> mounts List [Mount] A list of volume mounts for every service  Yes <code>[]</code> tolerations List [Toleration] Toleration to apply to service pods. Reference: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/  Yes <code>[]</code>"},{"location":"odm/models/config/#mount","title":"Mount","text":"<p>A configuration for mounting existing volumes to a container</p> Field Type Description Required Default name Keyword Name of volume mount  Yes <code>None</code> path Text Target mount path  Yes <code>None</code> read_only Boolean Should this be mounted as read-only?  Yes <code>True</code> privileged_only Boolean Should this mount only be available for privileged services?  Yes <code>False</code> resource_type Enum Type of mountable Kubernetes resourceSupported values are:<code>\"configmap\", \"secret\", \"volume\"</code>  Yes <code>volume</code> resource_name Keyword Name of resource (Kubernetes only)  Optional <code>None</code> resource_key Keyword Key of ConfigMap/Secret (Kubernetes only)  Optional <code>None</code>"},{"location":"odm/models/config/#toleration","title":"Toleration","text":"<p>Limit a set of kubernetes objects based on a label query.</p> Field Type Description Required Default key Keyword The taint key that the toleration applies to  Optional <code>None</code> operator Enum Relationship between taint key and valueSupported values are:<code>\"Equal\", \"Exists\"</code>  Yes <code>Equal</code> value Keyword Taint value the toleration matches to  Optional <code>None</code> effect Enum The taint effect to match.Supported values are:<code>\"NoExecute\", \"NoSchedule\", \"PreferNoSchedule\"</code>  Optional <code>None</code> toleration_seconds Integer The period of time the toleration tolerates the taint  Optional <code>None</code>"},{"location":"odm/models/config/#selector","title":"Selector","text":"<p>None</p> Field Type Description Required Default field List [FieldSelector] Field selector for resource under kubernetes  Yes <code>[]</code> label List [LabelSelector] Label selector for resource under kubernetes  Yes <code>[]</code>"},{"location":"odm/models/config/#fieldselector","title":"FieldSelector","text":"<p>Limit a set of kubernetes objects based on a field query.</p> Field Type Description Required Default key Keyword Name of a field to select on.  Yes <code>None</code> equal Boolean When true key must equal value, when false it must not  Yes <code>True</code> value Keyword Value to compare field to.  Yes <code>None</code>"},{"location":"odm/models/config/#labelselector","title":"LabelSelector","text":"<p>Limit a set of kubernetes objects based on a label query.</p> Field Type Description Required Default key Keyword Name of label to select on.  Yes <code>None</code> operator Enum Operation to select label with.Supported values are:<code>\"DoesNotExist\", \"Exists\", \"In\", \"NotIn\"</code>  Yes <code>None</code> values List [Keyword] Value list to compare label to.  Yes <code>None</code>"},{"location":"odm/models/config/#updater","title":"Updater","text":"<p>None</p> Field Type Description Required Default job_dockerconfig DockerConfigDelta Container configuration used for service registration/updates  Yes <code>None</code> registry_configs List [RegistryConfiguration] Configurations to be used with container registries  Yes <code>[{'name': 'registry.hub.docker.com', 'proxies': {}}]</code>"},{"location":"odm/models/config/#registryconfiguration","title":"RegistryConfiguration","text":"<p>None</p> Field Type Description Required Default name Text Name of container registry  Yes <code>None</code> proxies Mapping [String, Text] Proxy configuration that is passed to Python Requests  Optional <code>None</code> token_server Text Token server name to facilitate anonymous pull access  Optional <code>None</code>"},{"location":"odm/models/config/#vacuum","title":"Vacuum","text":"<p>None</p> Field Type Description Required Default list_cache_directory Keyword None  Yes <code>/cache/</code> worker_cache_directory Keyword None  Yes <code>/memory/</code> data_directories List [Keyword] None  Yes <code>[]</code> file_directories List [Keyword] None  Yes <code>[]</code> assemblyline_user Keyword None  Yes <code>vacuum-service-account</code> department_map_url Keyword None  Optional <code>None</code> department_map_init Keyword None  Optional <code>None</code> stream_map_url Keyword None  Optional <code>None</code> stream_map_init Keyword None  Optional <code>None</code> safelist List [VacuumSafelistItem] None  Yes <code>[]</code> worker_threads Integer None  Yes <code>50</code> worker_rollover Integer None  Yes <code>1000</code> minimum_classification Keyword None  Yes <code>U</code> ingest_type Keyword None  Yes <code>VACUUM</code>"},{"location":"odm/models/config/#vacuumsafelistitem","title":"VacuumSafelistItem","text":"<p>None</p> Field Type Description Required Default name Keyword None  Yes <code>None</code> conditions Mapping [String, Keyword] None  Yes <code>None</code>"},{"location":"odm/models/config/#datasource","title":"Datasource","text":"<p>Datasource Configuration</p> Field Type Description Required Default classpath Keyword None  Yes <code>None</code> config Mapping [String, Keyword] None  Yes <code>None</code>"},{"location":"odm/models/config/#datastore","title":"Datastore","text":"<p>Datastore Configuration</p> Field Type Description Required Default hosts List [Keyword] List of hosts used for the datastore  Yes <code>['http://elastic:devpass@localhost:9200']</code> archive Archive Datastore Archive feature configuration  Yes See Archive for more details. cache_dtl Integer Default cache lenght for computed indices (submission_tree, submission_summary...  Yes <code>5</code> type Enum Type of application used for the datastoreSupported values are:<code>\"elasticsearch\"</code>  Yes <code>elasticsearch</code>"},{"location":"odm/models/config/#archive","title":"Archive","text":"<p>Datastore Archive feature configuration</p> Field Type Description Required Default enabled Boolean Are we enabling Achiving features across indices?  Yes <code>False</code> indices List [Keyword] List of indices the ILM Applies to  Yes <code>['file', 'submission', 'result']</code>"},{"location":"odm/models/config/#filestore","title":"Filestore","text":"<p>Filestore Configuration</p> Field Type Description Required Default archive List [Keyword] List of filestores used for malware archive  Yes <code>['s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-archive&amp;use_ssl=False']</code> cache List [Keyword] List of filestores used for caching  Yes <code>['s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-cache&amp;use_ssl=False']</code> storage List [Keyword] List of filestores used for storage  Yes <code>['s3://al_storage_key:Ch@ngeTh!sPa33w0rd@localhost:9000?s3_bucket=al-storage&amp;use_ssl=False']</code>"},{"location":"odm/models/config/#logging","title":"Logging","text":"<p>Model Definition for the Logging Configuration</p> Field Type Description Required Default log_level Enum What level of logging should we have?Supported values are:<code>\"CRITICAL\", \"DEBUG\", \"DISABLED\", \"ERROR\", \"INFO\", \"WARNING\"</code>  Yes <code>INFO</code> log_to_console Boolean Should we log to console?  Yes <code>True</code> log_to_file Boolean Should we log to files on the server?  Yes <code>False</code> log_directory Keyword If <code>log_to_file: true</code>, what is the directory to store logs?  Yes <code>/var/log/assemblyline/</code> log_to_syslog Boolean Should logs be sent to a syslog server?  Yes <code>False</code> syslog_host Keyword If <code>log_to_syslog: true</code>, provide hostname/IP of the syslog server?  Yes <code>localhost</code> syslog_port Integer If <code>log_to_syslog: true</code>, provide port of the syslog server?  Yes <code>514</code> export_interval Integer How often, in seconds, should counters log their values?  Yes <code>5</code> log_as_json Boolean Log in JSON format?  Yes <code>True</code> heartbeat_file Keyword Add a health check to core components.If <code>true</code>, core components will touch this path regularly to tell the container environment it is healthy  Optional <code>/tmp/heartbeat</code>"},{"location":"odm/models/config/#retrohunt","title":"Retrohunt","text":"<p>Configuration for connecting to a retrohunt service.</p> Field Type Description Required Default enabled Boolean Is the Retrohunt functionnality enabled on the frontend  Yes <code>False</code> dtl Integer Number of days retrohunt jobs will remain in the system by default  Yes <code>30</code> max_dtl Integer Maximum number of days retrohunt jobs will remain in the system  Yes <code>0</code> url Keyword Base URL for service API  Yes <code>https://hauntedhouse:4443</code> api_key Keyword Service API Key  Yes <code>ChangeThisDefaultRetroHuntAPIKey!</code> tls_verify Boolean Should tls certificates be verified  Yes <code>True</code>"},{"location":"odm/models/config/#services","title":"Services","text":"<p>Services Configuration</p> Field Type Description Required Default categories List [Keyword] List of categories a service can be assigned to  Yes <code>['Antivirus', 'Dynamic Analysis', 'External', 'Extraction', 'Filtering', 'Internet Connected', 'Networking', 'Static Analysis']</code> default_auto_update Boolean Should services be auto-updated?  Yes <code>False</code> default_timeout Integer Default service timeout time in seconds  Yes <code>60</code> stages List [Keyword] List of execution stages a service can be assigned to  Yes <code>['FILTER', 'EXTRACT', 'CORE', 'SECONDARY', 'POST', 'REVIEW']</code> image_variables Mapping [String, Keyword] Substitution variables for image paths (for custom registry support)  Yes <code>None</code> update_image_variables Mapping [String, Keyword] Similar to <code>image_variables</code> but only applied to the updater. Intended for use with local registries.  Yes <code>None</code> preferred_update_channel Keyword Default update channel to be used for new services  Yes <code>stable</code> allow_insecure_registry Boolean Allow fetching container images from insecure registries  Yes <code>False</code> preferred_registry_type Enum Global registry type to be used for fetching updates for a service (overridable by a service)Supported values are:<code>\"docker\", \"harbor\"</code>  Yes <code>docker</code> prefer_service_privileged Boolean Global preference that controls if services should be privileged to communicate with core infrastucture  Yes <code>False</code> cpu_reservation Float How much CPU do we want to reserve relative to the service's request?At <code>1</code>, a service's full CPU request will be reserved for them.At <code>0</code> (only for very small appliances/dev boxes), the service's CPU will be limited but no CPU will be reserved allowing for more flexible scheduling of containers.  Yes <code>0.25</code> safelist ServiceSafelist None  Yes <code>None</code> registries List [ServiceRegistry] Global set of registries for services  Optional <code>[]</code>"},{"location":"odm/models/config/#serviceregistry","title":"ServiceRegistry","text":"<p>Pre-Configured Registry Details for Services</p> Field Type Description Required Default name Keyword Name of container registry  Yes <code>None</code> type Enum Type of container registrySupported values are:<code>\"docker\", \"harbor\"</code>  Yes <code>docker</code> username Keyword None  Optional <code>None</code> password Keyword None  Optional <code>None</code> use_fic Boolean Use federated identity credential token instead of user/passwords combinaison (ACR Only)  Yes <code>False</code>"},{"location":"odm/models/config/#servicesafelist","title":"ServiceSafelist","text":"<p>Service's Safelisting Configuration</p> Field Type Description Required Default enabled Boolean Should services be allowed to check extracted files against safelist?  Yes <code>True</code> hash_types List [Enum] Types of file hashes used for safelist checks  Yes <code>['sha1', 'sha256']</code> enforce_safelist_service Boolean Should the Safelist service always run on extracted files?  Yes <code>False</code>"},{"location":"odm/models/config/#submission","title":"Submission","text":"<p>Default values for parameters for submissions that may be overridden on a per submission basis</p> Field Type Description Required Default default_max_extracted Integer How many extracted files may be added to a submission?  Yes <code>500</code> default_max_supplementary Integer How many supplementary files may be added to a submission?  Yes <code>500</code> dtl Integer Number of days submissions will remain in the system by default  Yes <code>30</code> emptyresult_dtl Integer Number of days emptyresult will remain in the system  Yes <code>5</code> max_dtl Integer Maximum number of days submissions will remain in the system  Yes <code>0</code> max_extraction_depth Integer Maximum files extraction depth  Yes <code>6</code> max_file_size Long Maximum size for files submitted in the system  Yes <code>104857600</code> max_metadata_length Integer Maximum length for each metadata values  Yes <code>4096</code> max_temp_data_length Integer Maximum length for each temporary data values  Yes <code>4096</code> metadata MetadataConfig Metadata compliance rules  Yes See MetadataConfig for more details. file_sources List [FileSource] List of external source to fetch file  Yes <code>[]</code> tag_types TagTypes Tag types that show up in the submission summary  Yes See TagTypes for more details. verdicts Verdicts Minimum score value to get the specified verdict.  Yes See Verdicts for more details. default_temporary_keys Mapping [String, Enum] temporary_keys values for well known services.  Yes <code>None</code> temporary_keys Mapping [String, Enum] Set the operation that will be used to update values using this key in the temporary submission data.  Yes <code>None</code> profiles List [SubmissionProfile] Submission profiles with preset submission parameters  Yes <code>[{'name': 'static', 'display_name': 'Static Analysis [OFFLINE]', 'summary': 'Quick scan; keep it local', 'description': '\\n**Summary**\\n\\nQuick, local-only scan with no execution.\\n\\n**What it does**\\n\\nAnalyzes files using internal and open-source tools (e.g., YARA, CAPA) to inspect their structure, metadata, and embedded indicators without running any code.\\n\\n**When to use it**\\n- Rapid triage\\n- Checking sensitive or proprietary files that must never leave the local network\\n\\n**Limitations**\\n- Low detection rate for packed or heavily obfuscated malware\\n- Cannot observe runtime behavior or command-and-control (C2) logic\\n', 'params': {'services': {'selected': ['Filtering', 'Antivirus', 'Static Analysis', 'Extraction', 'Networking']}}}, {'name': 'static_with_dynamic', 'display_name': 'Static + Dynamic Analysis [OFFLINE]', 'summary': 'See behavior; keep it local', 'description': '\\n**Summary**\\n\\nLocal sandbox detonation with behavioral visibility.\\n\\n**What it does**\\n\\nCombines static analysis with full dynamic execution in a local sandbox to observe process creation, file system changes, registry activity, and system interactions.\\n\\n**When to use it**\\n- Standard malware investigation\\n- Understanding what a file does at runtime without risking data leakage to third-party APIs\\n\\n**Limitations**\\n- Malware may evade or delay execution if it detects the sandbox environment\\n- Limited visibility into network-based indicators without internet access\\n', 'params': {'services': {'selected': ['Filtering', 'Antivirus', 'Static Analysis', 'Extraction', 'Networking', 'Dynamic Analysis']}}}, {'name': 'static_with_internet', 'display_name': 'Static Analysis [ONLINE]', 'summary': 'Is this a known threat? (Quick check)', 'description': '\\n**Summary**\\n\\nQuick reputation check using global intelligence sources.\\n\\n**What it does**\\n\\nPerforms metadata and hash lookups against external services (e.g., VirusTotal, Google Threat Intelligence) without executing the file.\\n\\n**When to use it**\\n- Quickly determining whether a file is already known malicious\\n- Prioritizing triage based on global reputation\\n\\n**Limitations**\\n- Potential data leakage via hash or metadata queries\\n- Unique samples may alert adversaries that analysis is occurring\\n', 'params': {'services': {'selected': ['Filtering', 'Antivirus', 'Static Analysis', 'Extraction', 'Networking', 'Internet Connected']}}}, {'name': 'static_and_dynamic_with_internet', 'display_name': 'Static + Dynamic Analysis [ONLINE]', 'summary': 'Full deep-dive; allow network traffic', 'description': '\\n**Summary**\\n\\nComplete analysis with execution and internet access.\\n\\n**What it does**\\n\\nExecutes files in a sandbox with live internet connectivity to capture command-and-control traffic, network indicators, and runtime behavior, while also leveraging external reputation services.\\n\\n**When to use it**\\n- Deep investigation of unknown or high-risk samples\\n- Identifying network IOCs and full malware lifecycle behavior\\n\\n**Limitations**\\n- Privacy and data exposure risk\\n- Sample or metadata may be shared with third-party services\\n', 'params': {'services': {'selected': ['Filtering', 'Antivirus', 'Static Analysis', 'Extraction', 'Networking', 'Internet Connected', 'Dynamic Analysis']}, 'service_spec': {'CAPE': {'routing': 'internet'}, 'URLDownloader': {'proxy': 'localhost_proxy'}}}}]</code>"},{"location":"odm/models/config/#filesource","title":"FileSource","text":"<p>A file source entry for remote fetching via string</p> Field Type Description Required Default name Keyword Name of the sha256 source  Yes <code>None</code> auto_select Boolean Should we force the source to be auto-selected for the user ?  Yes <code>False</code> download_from_url Boolean Should we download from the resulting URL or create an Assemblyline URI file for it ?  Yes <code>True</code> hash_types List [Keyword] Method(s) of fetching file from source by string input(ie. ['sha256', 'sha1', 'md5', 'tlsh', 'ssdeep']). This also supports custom types.  Yes <code>['sha256']</code> hash_patterns Mapping [String, Text] Custom types to regex pattern definition for input detection/validation  Optional <code>None</code> classification ClassificationString None  Optional <code>None</code> data Keyword None  Optional <code>None</code> failure_pattern Keyword None  Optional <code>None</code> method Enum Method used to call the URLSupported values are:<code>\"GET\", \"POST\"</code>  Yes <code>GET</code> url Keyword Url to fetch the file via SHA256 from (Uses replace pattern)  Yes <code>None</code> replace_pattern Keyword Pattern to replace in the URL with the SHA256  Yes <code>None</code> headers Mapping [String, Keyword] Headers used to connect to the URL  Yes <code>{}</code> proxies Mapping [String, Keyword] Proxy used to connect to the URL  Yes <code>{}</code> select_services List [Keyword] List of services that will be auto-selected when using this source.  Yes <code>[]</code> verify Boolean Should the download function Verify SSL connections?  Yes <code>True</code> password Text None  Optional <code>None</code> metadata Mapping [String, Text] Metadata to append to submission based on source  Optional <code>None</code>"},{"location":"odm/models/config/#metadataconfig","title":"MetadataConfig","text":"<p>Configuration for metadata compliance with APIs</p> Field Type Description Required Default archive Mapping [String, Metadata] Metadata specification for archiving  Yes <code>None</code> submit Mapping [String, Metadata] Metadata specification for submission  Yes <code>None</code> ingest Mapping [String, Mapping [String, Metadata]] Metadata specification for certain ingestion based on ingest_type  Yes <code>None</code> strict_schemes List [Keyword] A list of metadata schemes with strict rules (ie. no extra/unknown metadata). Values can be: <code>archive</code>, <code>submit</code>, or one of the schemes under <code>ingest</code>.  Yes <code>[]</code>"},{"location":"odm/models/config/#metadata","title":"Metadata","text":"<p>Metadata configuration</p> Field Type Description Required Default validator_type Enum Type of validation to apply to metadata valueSupported values are:<code>\"boolean\", \"date\", \"domain\", \"email\", \"enum\", \"float\", \"integer\", \"ip\", \"keyword\", \"list\", \"regex\", \"text\", \"uri\"</code>  Yes <code>str</code> validator_params Mapping [String, Any] Configuration parameters to apply to validator  Yes <code>{}</code> suggestions List [Keyword] List of suggestions for this field  Yes <code>[]</code> suggestion_key Keyword Key in redis where to get the suggestions from  Optional <code>None</code> default Keyword None  Optional <code>None</code> required Boolean Is this field required?  Yes <code>False</code> aliases List [Keyword] Field name aliases that map over to the field.  Yes <code>[]</code>"},{"location":"odm/models/config/#submissionprofile","title":"SubmissionProfile","text":"<p>Configuration for defining submission profiles for basic users</p> Field Type Description Required Default name Text Submission profile name  Yes <code>None</code> display_name Text Submission profile display name  Yes <code>None</code> classification ClassificationString Submission profile classification  Yes <code>TLP:C</code> params SubmissionProfileParams Default submission parameters for profile  Yes <code>None</code> restricted_params Mapping [String, List [Text]] A list of parameters that can be configured for this profile. The keys are the service names or \"submission\" and the values are the parameters that cannot be configured by limited users.  Yes <code>{}</code> summary Text Short summary of the submission profile  Optional <code>None</code> description Text Detailed description of the submission profile (Markdown supported)  Optional <code>None</code>"},{"location":"odm/models/config/#submissionprofileparams","title":"SubmissionProfileParams","text":"<p>Submission Parameters for profile</p> Field Type Description Required Default classification Classification Original classification of the submission  Optional <code>None</code> deep_scan Boolean Should a deep scan be performed?  Optional <code>None</code> generate_alert Boolean Should this submission generate an alert?  Optional <code>None</code> ignore_cache Boolean Ignore the cached service results?  Optional <code>None</code> ignore_recursion_prevention Boolean Should we ignore recursion prevention?  Optional <code>None</code> ignore_filtering Boolean Should we ignore filtering services?  Optional <code>None</code> ignore_size Boolean Ignore the file size limits?  Optional <code>None</code> max_extracted Integer Max number of extracted files  Optional <code>None</code> max_supplementary Integer Max number of supplementary files  Optional <code>None</code> priority Integer Priority of the scan  Optional <code>None</code> services ServiceSelection Service selection  Optional <code>None</code> service_spec Mapping [String, Mapping [String, Any]] Service-specific parameters  Optional <code>None</code> auto_archive Boolean Does the submission automatically goes into the archive when completed?  Optional <code>None</code> delete_after_archive Boolean When the submission is archived, should we delete it from hot storage right away?  Optional <code>None</code> ttl Integer Time, in days, to live for this submission  Optional <code>None</code> type Keyword Type of submission  Optional <code>None</code> use_archive_alternate_dtl Boolean Should we use the alternate dtl while archiving?  Optional <code>None</code>"},{"location":"odm/models/config/#tagtypes","title":"TagTypes","text":"<p>None</p> Field Type Description Required Default attribution List [Keyword] Attibution tags  Yes <code>['attribution.actor', 'attribution.campaign', 'attribution.exploit', 'attribution.implant', 'attribution.family', 'attribution.network', 'av.virus_name', 'file.config', 'technique.obfuscation']</code> behavior List [Keyword] Behaviour tags  Yes <code>['file.behavior']</code> ioc List [Keyword] IOC tags  Yes <code>['network.email.address', 'network.static.ip', 'network.static.domain', 'network.static.uri', 'network.dynamic.ip', 'network.dynamic.domain', 'network.dynamic.uri']</code>"},{"location":"odm/models/config/#verdicts","title":"Verdicts","text":"<p>Minimum score value to get the specified verdict, otherwise the file is considered safe.</p> Field Type Description Required Default info Integer Minimum score for the verdict to be Informational.  Yes <code>0</code> suspicious Integer Minimum score for the verdict to be Suspicious.  Yes <code>300</code> highly_suspicious Integer Minimum score for the verdict to be Highly Suspicious.  Yes <code>700</code> malicious Integer Minimum score for the verdict to be Malicious.  Yes <code>1000</code>"},{"location":"odm/models/config/#system","title":"System","text":"<p>System Configuration</p> Field Type Description Required Default constants Keyword Module path to the assemblyline constants  Yes <code>assemblyline.common.constants</code> organisation Text Organisation acronym used for signatures  Yes <code>ACME</code> type Enum Type of systemSupported values are:<code>\"development\", \"production\", \"staging\"</code>  Yes <code>production</code> support SystemSupport Support configuration for the system  Yes <code>None</code>"},{"location":"odm/models/config/#systemsupport","title":"SystemSupport","text":"<p>System Support Configuration</p> Field Type Description Required Default documentation URI Documentation link for the system  Optional <code>https://cybercentrecanada.github.io/assemblyline4_docs/</code> email ValidatedKeyword Support email for the system (mailto: URI format)  Optional <code>None</code>"},{"location":"odm/models/config/#ui","title":"UI","text":"<p>UI Configuration</p> Field Type Description Required Default ai_backends AIBackends AI Multi-backends support for the UI  Yes See AIBackends for more details. alerting_meta AlertingMeta Alerting metadata fields  Yes See AlertingMeta for more details. allow_malicious_hinting Boolean Allow user to tell in advance the system that a file is malicious?  Yes <code>False</code> allow_raw_downloads Boolean Allow user to download raw files?  Yes <code>True</code> allow_zip_downloads Boolean Allow user to download files as password protected ZIPs?  Yes <code>True</code> allow_replay Boolean Allow users to request replay on another server?  Yes <code>False</code> allow_url_submissions Boolean Allow file submissions via url?  Yes <code>True</code> api_proxies Mapping [String, APIProxies] Proxy requests to the configured API target and add headers  Yes See APIProxies for more details. audit Boolean Should API calls be audited and saved to a separate log file?  Yes <code>True</code> audit_login Boolean Should login successes and failures be part of the audit log as well?  Yes <code>False</code> banner Mapping [String, Keyword] Banner message display on the main page (format: {: message})  Optional <code>None</code> banner_level Enum Banner message levelSupported values are:<code>\"error\", \"info\", \"success\", \"warning\"</code>  Yes <code>info</code> debug Boolean Enable debugging?  Yes <code>False</code> default_quotas Quotas Default API quotas values  Yes See Quotas for more details. discover_url Keyword Discover URL  Optional <code>None</code> download_encoding Enum Which encoding will be used for downloads?Supported values are:<code>\"cart\", \"raw\", \"zip\"</code>  Yes <code>cart</code> default_zip_password Text Default user-defined password for creating password protected ZIPs when downloading files  Optional <code>infected</code> email Email Assemblyline admins email address  Optional <code>None</code> enforce_quota Boolean Enforce the user's quotas?  Yes <code>True</code> external_links List [ExternalLinks] List of external pivot links  Yes <code>[]</code> external_sources List [ExternalSource] List of external sources to query  Yes <code>[]</code> fqdn Text Fully qualified domain name to use for the 2-factor authentication validation  Yes <code>localhost</code> ingest_max_priority Integer Maximum priority for ingest API  Yes <code>250</code> read_only Boolean Turn on read only mode in the UI  Yes <code>False</code> read_only_offset Keyword Offset of the read only mode for all paging and searches  Yes `` rss_feeds List [Keyword] List of RSS feeds to display on the UI  Yes <code>['https://alpytest.blob.core.windows.net/pytest/stable.json', 'https://alpytest.blob.core.windows.net/pytest/services.json', 'https://alpytest.blob.core.windows.net/pytest/community.json', 'https://alpytest.blob.core.windows.net/pytest/blog.json']</code> services_feed Keyword Feed of all the services built by the Assemblyline Team  Yes <code>https://alpytest.blob.core.windows.net/pytest/services.json</code> community_feed Keyword Feed of all the services built by the Assemblyline community.  Yes <code>https://alpytest.blob.core.windows.net/pytest/community.json</code> secret_key Keyword Flask secret key to store cookies, etc.  Yes <code>This is the default flask secret key... you should change this!</code> session_duration Integer Duration of the user session before the user has to login again  Yes <code>3600</code> statistics Statistics Statistics configuration  Yes See Statistics for more details. tos Text Terms of service  Optional <code>None</code> tos_lockout Boolean Lock out user after accepting the terms of service?  Yes <code>False</code> tos_lockout_notify List [Keyword] List of admins to notify when a user gets locked out  Optional <code>None</code> url_submission_auto_service_selection List [Keyword] List of services auto-selected by the UI when submitting URLs  Yes <code>['URLDownloader']</code> url_submission_headers Mapping [String, Keyword] Headers used by the url_download method  Optional <code>None</code> url_submission_proxies Mapping [String, Keyword] Proxy used by the url_download method  Optional <code>None</code> url_submission_timeout Integer Request timeout for fetching URLs  Yes <code>15</code> validate_session_ip Boolean Validate if the session IP matches the IP the session was created from  Yes <code>True</code> validate_session_useragent Boolean Validate if the session useragent matches the useragent the session was created with  Yes <code>True</code>"},{"location":"odm/models/config/#aibackends","title":"AIBackends","text":"<p>AI Multi-Backend support configuration block</p> Field Type Description Required Default enabled Boolean Is AI support enabled?  Yes <code>False</code> api_connections List [AIConnection] List of API definitions use in the API Pool  Yes <code>[{'chat_url': 'https://api.openai.com/v1/chat/completions', 'api_type': 'openai', 'headers': {'Content-Type': 'application/json'}, 'model_name': 'gpt-3.5-turbo', 'proxies': None, 'verify': True}, {'chat_url': 'https://api.openai.com/v1/chat/completions', 'api_type': 'openai', 'headers': {'Content-Type': 'application/json'}, 'model_name': 'gpt-4', 'proxies': None, 'verify': True}]</code> function_params AIFunctionParameters Definition of each parameters used in the different AI functions  Yes <code>None</code>"},{"location":"odm/models/config/#aiconnection","title":"AIConnection","text":"<p>Connection information to an AI backend</p> Field Type Description Required Default api_type Enum Type of chat API we are communicating withSupported values are:<code>\"cohere\", \"openai\"</code>  Yes <code>None</code> chat_url Keyword URL to the AI API  Yes <code>None</code> headers Mapping [String, Keyword] Headers used by the _call_ai_backend method  Optional <code>{}</code> model_name Keyword Name of the model to be used for the AI analysis.  Yes <code>None</code> proxies Mapping [String, Keyword] Proxies used by the _call_ai_backend method  Optional <code>None</code> use_fic Boolean Use Federated Identity Credentials to login  Yes <code>False</code> verify Boolean Should the SSL connection to the AI API be verified.  Yes <code>True</code>"},{"location":"odm/models/config/#aifunctionparameters","title":"AIFunctionParameters","text":"<p>Definition of each parameters used in the different AI functions</p> Field Type Description Required Default assistant AIQueryParams Parameters used for Assamblyline Assistant  Yes <code>None</code> code AIQueryParams Parameters used for code analysis  Yes <code>None</code> detailed_report AIQueryParams Parameters used for detailed reports  Yes <code>None</code> executive_summary AIQueryParams Parameters used for executive summaries  Yes <code>None</code>"},{"location":"odm/models/config/#aiqueryparams","title":"AIQueryParams","text":"<p>Parameters used during a AI query</p> Field Type Description Required Default system_message Keyword System message used for the query.  Yes <code>None</code> task Keyword Task description sent to the AI  Yes `` max_tokens Integer Maximum ammount of token used for the response.  Yes <code>None</code> options Mapping [String, Any] Other kwargs options directly passed to the API.  Optional <code>None</code>"},{"location":"odm/models/config/#apiproxies","title":"APIProxies","text":"<p>Configuration for connecting to a retrohunt service.</p> Field Type Description Required Default url Keyword URL to redirect to  Yes <code>None</code> verify Boolean Should we verify the cert or not  Yes <code>True</code> headers List [HeaderValue] Headers to add to the request  Yes <code>[]</code> public Mapping [String, Any] Parameters to be sent to the Frontend.  Optional <code>None</code>"},{"location":"odm/models/config/#headervalue","title":"HeaderValue","text":"<p>Header value</p> Field Type Description Required Default name Keyword Name of the header  Yes <code>None</code> value Keyword None  Optional <code>None</code> key Keyword None  Optional <code>None</code>"},{"location":"odm/models/config/#alertingmeta","title":"AlertingMeta","text":"<p>Alerting Metadata</p> Field Type Description Required Default important List [Keyword] Metadata keys that are considered important  Yes <code>['original_source', 'protocol', 'subject', 'submitted_url', 'source_url', 'url', 'web_url', 'from', 'to', 'cc', 'bcc', 'ip_src', 'ip_dst', 'source']</code> subject List [Keyword] Metadata keys that refer to an email's subject  Yes <code>['subject']</code> url List [Keyword] Metadata keys that refer to a URL  Yes <code>['submitted_url', 'source_url', 'url', 'web_url']</code>"},{"location":"odm/models/config/#externallinks","title":"ExternalLinks","text":"<p>External links that specific metadata and tags can pivot to</p> Field Type Description Required Default allow_bypass Boolean If the classification of the item is higher than the max_classificaiton, can we let the user bypass the check and still query the external link?  Yes <code>False</code> name Keyword Name of the link  Yes <code>None</code> encoding Enum How should the target value be encoded (used when <code>double_encode: true</code>)Supported values are:<code>\"sha256\", \"url\"</code>  Yes <code>url</code> double_encode Boolean Should the replaced value be encoded before url encoding?  Yes <code>False</code> classification ClassificationString None  Optional <code>None</code> max_classification ClassificationString None  Optional <code>None</code> replace_pattern Keyword Pattern that will be replaced in the URL with the metadata or tag value  Yes <code>None</code> targets List [ExternalLinksTargets] List of external sources to query  Yes <code>[]</code> url Keyword URL to redirect to  Yes <code>None</code>"},{"location":"odm/models/config/#externallinkstargets","title":"ExternalLinksTargets","text":"<p>Target definition of an external link</p> Field Type Description Required Default type Enum Type of external link targetSupported values are:<code>\"hash\", \"metadata\", \"tag\"</code>  Yes <code>None</code> key Keyword Key that it can be used against  Yes <code>None</code>"},{"location":"odm/models/config/#externalsource","title":"ExternalSource","text":"<p>Connection details for external systems/data sources.</p> Field Type Description Required Default name Keyword Name of the source.  Yes <code>None</code> classification ClassificationString None  Optional <code>None</code> max_classification ClassificationString None  Optional <code>None</code> url Keyword URL of the upstream source's lookup service.  Yes <code>None</code>"},{"location":"odm/models/config/#quotas","title":"Quotas","text":"<p>Default API and submission quota values for the system</p> Field Type Description Required Default concurrent_api_calls Integer Maximum concurrent API Calls that can be running for a user.  Yes <code>10</code> concurrent_submissions Integer Maximum concurrent Submission that can be running for a user.  Yes <code>5</code> concurrent_async_submissions Integer Maximum concurrent asynchroneous Submission that can be running for a user.  Yes <code>0</code> daily_api_calls Integer Maximum daily API calls a user can issue.  Yes <code>0</code> daily_submissions Integer Maximum daily submission a user can do.  Yes <code>0</code>"},{"location":"odm/models/config/#statistics","title":"Statistics","text":"<p>Statistics</p> Field Type Description Required Default alert List [Keyword] Fields used to generate statistics in the Alerts page  Yes <code>['al.attrib', 'al.av', 'al.behavior', 'al.domain', 'al.ip', 'al.yara', 'file.name', 'file.md5', 'owner']</code> submission List [Keyword] Fields used to generate statistics in the Submissions page  Yes <code>['params.submitter']</code>"},{"location":"odm/models/config/#archivermetadata","title":"ArchiverMetadata","text":"<p>Malware Archive Configuration</p> Field Type Description Required Default default Keyword None  Optional <code>None</code> editable Boolean Can the user provide a custom value  Yes <code>False</code> values List [Keyword] List of possible values to pick from  Yes <code>[]</code>"},{"location":"odm/models/config/#ai","title":"AI","text":"<p>AI support configuration block</p> Field Type Description Required Default chat_url Keyword URL to the AI API  Yes <code>None</code> api_type Enum Type of chat API we are communicating withSupported values are:<code>\"cohere\", \"openai\"</code>  Yes <code>None</code> assistant AIQueryParams Parameters used for Assamblyline Assistant  Yes <code>None</code> code AIQueryParams Parameters used for code analysis  Yes <code>None</code> detailed_report AIQueryParams Parameters used for detailed reports  Yes <code>None</code> executive_summary AIQueryParams Parameters used for executive summaries  Yes <code>None</code> enabled Boolean Is AI support enabled?  Yes <code>None</code> headers Mapping [String, Keyword] Headers used by the _call_ai_backend method  Optional <code>None</code> model_name Keyword Name of the model to be used for the AI analysis.  Yes <code>None</code> verify Boolean Should the SSL connection to the AI API be verified.  Yes <code>None</code> proxies Mapping [String, Keyword] Proxies used by the _call_ai_backend method  Optional <code>None</code>"},{"location":"odm/models/config/#aiqueryparams_1","title":"AIQueryParams","text":"<p>Parameters used during a AI query</p> Field Type Description Required Default system_message Keyword System message used for the query.  Yes <code>None</code> task Keyword Task description sent to the AI  Yes `` max_tokens Integer Maximum ammount of token used for the response.  Yes <code>None</code> options Mapping [String, Any] Other kwargs options directly passed to the API.  Optional <code>None</code>"},{"location":"odm/models/config/#sha256source","title":"Sha256Source","text":"<p>A source entry for the sha256 downloader</p> Field Type Description Required Default name Keyword Name of the sha256 source  Yes <code>None</code> classification ClassificationString None  Optional <code>None</code> data Keyword None  Optional <code>None</code> failure_pattern Keyword None  Optional <code>None</code> method Enum Method used to call the URLSupported values are:<code>\"GET\", \"POST\"</code>  Yes <code>GET</code> url Keyword Url to fetch the file via SHA256 from (Uses replace pattern)  Yes <code>None</code> replace_pattern Keyword Pattern to replace in the URL with the SHA256  Yes <code>None</code> headers Mapping [String, Keyword] Headers used to connect to the URL  Yes <code>{}</code> proxies Mapping [String, Keyword] Proxy used to connect to the URL  Yes <code>{}</code> verify Boolean Should the download function Verify SSL connections?  Yes <code>True</code>"},{"location":"odm/models/emptyresult/","title":"EmptyResult","text":""},{"location":"odm/models/emptyresult/#emptyresult","title":"EmptyResult","text":"<p>Model for Empty Results</p> Field Type Description Required Default expiry_ts Date Expiry timestamp  Yes <code>None</code>"},{"location":"odm/models/error/","title":"Error","text":""},{"location":"odm/models/error/#error","title":"Error","text":"<p>Error Model used by Error Viewer</p> Field Type Description Required Default archive_ts Date None  Optional <code>None</code> created Date Error creation timestamp  Yes <code>NOW</code> expiry_ts Date Expiry timestamp  Optional <code>None</code> response Response Response from the service  Yes <code>None</code> sha256 SHA256 SHA256 of file related to service error  Yes <code>None</code> type Enum Type of errorSupported values are:<code>\"EXCEPTION\", \"MAX DEPTH REACHED\", \"MAX FILES REACHED\", \"MAX RETRY REACHED\", \"SERVICE BUSY\", \"SERVICE DOWN\", \"TASK PRE-EMPTED\", \"UNKNOWN\"</code>  Yes <code>EXCEPTION</code> severity Enum Severity of the error.Supported values are:<code>\"error\", \"warning\"</code>  Yes <code>error</code>"},{"location":"odm/models/error/#response","title":"Response","text":"<p>Error Response from a Service</p> Field Type Description Required Default message Text Error message  Yes <code>None</code> service_debug_info Keyword Information about where the service was processed  Optional <code>None</code> service_name Keyword Service Name  Yes <code>None</code> service_tool_version Keyword Service Tool Version  Optional <code>None</code> service_version Keyword Service Version  Yes <code>None</code> status Enum Status of error produced by serviceSupported values are:<code>\"FAIL_NONRECOVERABLE\", \"FAIL_RECOVERABLE\"</code>  Yes <code>None</code>"},{"location":"odm/models/file/","title":"File","text":""},{"location":"odm/models/file/#file","title":"File","text":"<p>This section presents the detailed schema of the File object model within the Assemblyline application. Each entry in the schema represents a field that constitutes a File document within the File index. The information provided for each field includes the data type, a concise description, its requirement status, and the default value if any.</p> <p>Understanding this schema is crucial for constructing effective and precise Lucene search queries. By leveraging the fields outlined in the table below, you can craft queries to retrieve specific information about files analyzed by Assemblyline. These fields are integral for in-depth data analysis, enabling you to filter and locate files based on various attributes such as type, hash values, classification, and many others.</p> <p>Utilize this schema as a reference to enhance your search capabilities within the Assemblyline system, allowing for more targeted and refined data retrieval that aligns with your cybersecurity analysis needs.</p> Field Type Description Required Default archive_ts Date Timestamp indicating when the file was archived.  Optional <code>None</code> ascii Keyword Provides a dotted ASCII representation of the first 64 bytes of the file.  Yes <code>None</code> classification Classification Security classification assigned to the file based on its contents and context.  Yes <code>None</code> comments List [Comment] User comments linked to the file.  Yes <code>[]</code> entropy Float Entropy value indicating randomness or potential obfuscation.  Yes <code>None</code> expiry_ts Date Timestamp indicating when the file is scheduled to expire from the system.  Optional <code>None</code> is_section_image Boolean Indicates if the file is an image safe for web browser display, often part of analysis results.  Yes <code>False</code> is_supplementary Boolean Indicates if the file was created by an AssemblyLine service as supplementary data.  Yes <code>False</code> hex Keyword Hexadecimal representation of the first 64 bytes of the file.  Yes <code>None</code> labels List [Keyword] Array of descriptive labels applied to the file for categorization and analysis.  Yes <code>[]</code> label_categories LabelCategories Structured categories for the labels applied to the file.  Yes See LabelCategories for more details. md5 MD5 The MD5 hash of the file, used for identifying duplicates and verifying integrity.  Yes <code>None</code> magic Keyword File type info derived from libmagic.  Yes <code>None</code> mime Keyword MIME type of the file from libmagic.  Optional <code>None</code> seen Seen Records the frequency and timestamps of when the file was encountered.  Yes See Seen for more details. sha1 SHA1 The SHA1 hash of the file, providing a more secure alternative to MD5 for integrity checks.  Yes <code>None</code> sha256 SHA256 The SHA256 hash of the file, offering a high level of security for integrity verification.  Yes <code>None</code> size Long Size of the file in bytes.  Yes <code>None</code> ssdeep SSDeepHash The fuzzy hash of the file using SSDEEP, which is useful for identifying similar files.  Yes <code>None</code> type Keyword The file type as determined by the AssemblyLine file type identification service.  Yes <code>None</code> tlsh Keyword A locality-sensitive hash (TLSH) of the file's content, useful for similarity comparisons.  Optional <code>None</code> from_archive Boolean Indicates whether the file was retrieved from Assemblyline's archive during processing.  Yes <code>False</code> uri_info URIInfo Detailed components of the file's URI for advanced search functionality.  Optional <code>None</code>"},{"location":"odm/models/file/#comment","title":"Comment","text":"<p>Model that represents user annotations attached to a file.</p> <p>A Comment is a user-generated note or observation that can be added to a file within Assemblyline. This feature enables analysts to record insights, share findings, and collaborate on the analysis of a file. Each comment is timestamped and associated with the username of the individual who authored it, creating an audit trail of analytical discourse.</p> Field Type Description Required Default cid UUID Unique identifier for the comment.  Yes <code>None</code> uname Keyword The username of the individual who authored the comment.  Yes <code>None</code> date Date The date and time when the comment was posted.  Yes <code>NOW</code> text Text The content of the comment as written by the user.  Yes <code>None</code> reactions List [Reaction] An array of user reactions to the comment, such as likes or dislikes.  Yes <code>[]</code>"},{"location":"odm/models/file/#reaction","title":"Reaction","text":"<p>Model that encapsulates user interactions with a comment.</p> <p>The Reaction model captures the responses of users to comments made on a file. Reactions are simple expressions of agreement, disagreement, or sentiment, represented by a set of predefined icons. These reactions facilitate a quick, non-verbal form of feedback from users, enhancing collaborative analysis and engagement within the Assemblyline platform.</p> Field Type Description Required Default icon Enum Icon name representing the type of reaction given to a comment.Supported values are:<code>\"love\", \"party\", \"smile\", \"surprised\", \"thumbs_down\", \"thumbs_up\"</code>  Yes <code>None</code> uname Keyword The username of the individual who reacted to the comment.  Yes <code>None</code>"},{"location":"odm/models/file/#labelcategories","title":"LabelCategories","text":"<p>Structured categorization model for labels applied to a file.</p> <p>LabelCategories provide a systematic approach to classifying the characteristics and threat indicators of a file. This model organizes labels into distinct categories such as informational tags, technical techniques, and attribution links. By categorizing labels, analysts can efficiently navigate and assess the nature and potential threats associated with a file, streamlining the malware analysis process.</p> Field Type Description Required Default info List [Keyword] Informational labels providing context.  Yes <code>[]</code> technique List [Keyword] Labels identifying techniques or triggered detections (e.g., MITRE ATT&amp;CK\u00ae TTPs).  Yes <code>[]</code> attribution List [Keyword] Labels related to threat actors or campaigns.  Yes <code>[]</code>"},{"location":"odm/models/file/#seen","title":"Seen","text":"<p>Tracking model for the occurrence and frequency of a file within the system.</p> <p>The Seen model is designed to record and quantify the instances in which a file is encountered by Assemblyline. It keeps a count of the file's occurrences and logs the timestamps of the first and most recent sightings. This temporal information is crucial for understanding the prevalence and distribution of a file over time, aiding in threat trend analysis and situational awareness.</p> Field Type Description Required Default count Integer Number of times the file has been observed.  Yes <code>1</code> first Date The timestamp of the file's first sighting.  Yes <code>NOW</code> last Date The timestamp of the file's most recent sighting.  Yes <code>NOW</code>"},{"location":"odm/models/file/#uriinfo","title":"URIInfo","text":"<p>Detailed breakdown model of a file's Uniform Resource Identifier (URI).</p> <p>URIInfo dissects a file's URI into its fundamental components, providing granular data for advanced search and identification. This includes the scheme, network location, path, and other elements such as query parameters and fragments. By parsing these components, Assemblyline allows for a more nuanced analysis of the source and context of a file, which is essential for forensic investigations and threat intelligence gathering.</p> <p>Each of these descriptions aims to provide a clearer understanding of the purpose and utility of the respective models within Assemblyline, highlighting their roles in the broader context of malware analysis and cyber security operations.</p> Field Type Description Required Default uri Keyword Full URI of the file.  Yes <code>None</code> scheme Keyword URI scheme (e.g., http, ftp).  Yes <code>None</code> netloc Keyword Network location including domain and port.  Yes <code>None</code> path Keyword Path within the host.  Optional <code>None</code> params Keyword The parameters component of the URI, often used for session management.  Optional <code>None</code> query Keyword The query string of the URI, containing data for server-side processing.  Optional <code>None</code> fragment Keyword The fragment identifier of the URI, used to navigate to a specific part of the resource.  Optional <code>None</code> username Keyword Username in the URI, if present.  Optional <code>None</code> password Keyword Password in the URI, if present.  Optional <code>None</code> hostname Keyword Hostname extracted from the URI.  Yes <code>None</code> port Integer Port number in the URI.  Optional <code>None</code>"},{"location":"odm/models/filescore/","title":"FileScore","text":""},{"location":"odm/models/filescore/#filescore","title":"FileScore","text":"<p>Model of Scoring related to a File</p> Field Type Description Required Default psid UUID Parent submission ID of the associated submission  Optional <code>None</code> expiry_ts Date Expiry timestamp, used for garbage collection  Yes <code>None</code> score Integer Maximum score for the associated submission  Yes <code>None</code> errors Integer Number of errors that occurred during the previous analysis  Yes <code>None</code> sid UUID ID of the associated submission  Yes <code>None</code> time Float Epoch time at which the FileScore entry was created  Yes <code>None</code>"},{"location":"odm/models/heuristic/","title":"Heuristic","text":""},{"location":"odm/models/heuristic/#heuristic","title":"Heuristic","text":"<p>Model of Service Heuristics</p> Field Type Description Required Default attack_id List [Keyword] List of all associated ATT&amp;CK IDs  Yes <code>[]</code> classification Classification Classification of the heuristic  Yes <code>TLP:C</code> description Text Description of the heuristic  Yes <code>None</code> filetype Keyword What type of files does this heuristic target?  Yes <code>None</code> heur_id Keyword ID of the Heuristic  Yes <code>None</code> name Keyword Name of the heuristic  Yes <code>None</code> score Integer Default score of the heuristic  Yes <code>None</code> signature_score_map Mapping [String, Integer] Score of signatures for this heuristic  Yes <code>{}</code> stats Statistics Statistics related to the Heuristic  Yes See Statistics for more details. max_score Integer Maximum score for heuristic  Optional <code>None</code>"},{"location":"odm/models/replay/","title":"ReplayConfig","text":""},{"location":"odm/models/replay/#replayconfig","title":"ReplayConfig","text":"<p>None</p> Field Type Description Required Default creator Creator Replay creator options  Yes See Creator for more details. loader Loader Replay loader options  Yes See Loader for more details."},{"location":"odm/models/replay/#creator","title":"Creator","text":"<p>Replay creator configuration model</p> Field Type Description Required Default client Client Client to use for Replay operations  Yes See Client for more details. alert_input InputModule Input module for alerts  Yes See InputModule for more details. badlist_input InputModule Input module for badlist items  Yes See InputModule for more details. safelist_input InputModule Input module for safelist items  Yes See InputModule for more details. signature_input InputModule Input module for signatures  Yes See InputModule for more details. submission_input InputModule Input module for submissions  Yes See InputModule for more details. workflow_input InputModule Input module for workflows  Yes See InputModule for more details. lookback_time Keyword Lookback time for the Replay creator, e.g., '1d' for one day  Yes <code>*</code> output_filestore Keyword Output filestore URI for the Replay creator, e.g., 'file:///tmp/replay/output'  Yes <code>file:///tmp/replay/output</code> working_directory Keyword Working directory for the Replay creator, e.g., '/tmp/replay/work'  Yes <code>/tmp/replay/work</code>"},{"location":"odm/models/replay/#client","title":"Client","text":"<p>None</p> Field Type Description Required Default type Enum Type of client to use for Replay operationsSupported values are:<code>\"api\", \"direct\"</code>  Yes <code>direct</code> options ClientOptions Options for the client  Optional See ClientOptions for more details."},{"location":"odm/models/replay/#clientoptions","title":"ClientOptions","text":"<p>None</p> Field Type Description Required Default host Keyword None  Yes <code>https://localhost:443</code> user Keyword None  Yes <code>admin</code> apikey Keyword None  Yes <code>devkey:devpass</code> verify Boolean None  Yes <code>True</code>"},{"location":"odm/models/replay/#inputmodule","title":"InputModule","text":"<p>Input module configuration model for Replay creator operations</p> Field Type Description Required Default enabled Boolean Is this input module enabled?  Yes <code>True</code> threads Integer Number of threads to use for this input module  Yes <code>6</code> filter_queries List [Keyword] List of filter queries to apply to this input module  Yes <code>['NOT extended_scan:submitted', 'workflows_completed:true']</code>"},{"location":"odm/models/replay/#loader","title":"Loader","text":"<p>Replay loader configuration model</p> Field Type Description Required Default client Client Client to use for Replay loader operations  Yes See Client for more details. failed_directory Keyword Directory to store failed Replay bundles  Yes <code>/tmp/replay/failed</code> input_threads Integer Number of threads to use for loading input bundles  Yes <code>6</code> input_directory Keyword Directory to load input Replay bundles from  Yes <code>/tmp/replay/input</code> min_classification ClassificationString Minimum classification level for Replay bundles to be processed  Optional <code>None</code> reclassification ClassificationString Classification level to reclassify Replay bundles to after being imported  Optional <code>None</code> rescan List [Keyword] List of services to rescan after importing Replay bundles  Yes <code>[]</code> working_directory Keyword Working directory for the Replay loader, e.g., '/tmp/replay/work'  Yes <code>/tmp/replay/work</code> sync_check_interval Integer How often to check on imported Replay bundles (in seconds)?  Yes <code>3600</code>"},{"location":"odm/models/replay/#client_1","title":"Client","text":"<p>None</p> Field Type Description Required Default type Enum Type of client to use for Replay operationsSupported values are:<code>\"api\", \"direct\"</code>  Yes <code>direct</code> options ClientOptions Options for the client  Optional See ClientOptions for more details."},{"location":"odm/models/replay/#clientoptions_1","title":"ClientOptions","text":"<p>None</p> Field Type Description Required Default host Keyword None  Yes <code>https://localhost:443</code> user Keyword None  Yes <code>admin</code> apikey Keyword None  Yes <code>devkey:devpass</code> verify Boolean None  Yes <code>True</code>"},{"location":"odm/models/result/","title":"Result","text":""},{"location":"odm/models/result/#result","title":"Result","text":"<p>Result Model.</p> Field Type Description Required Default archive_ts Date None  Optional <code>None</code> classification Classification Aggregate security classification for the result.  Yes <code>None</code> created Date Date at which the result object got created.  Yes <code>NOW</code> expiry_ts Date Timestamp for when the result record expires.  Optional <code>None</code> response ResponseBody The body of the response from the service.  Yes <code>None</code> result ResultBody The result body.  Yes See ResultBody for more details. sha256 SHA256 SHA256 of the file the result object relates to.  Yes <code>None</code> type Keyword None  Optional <code>None</code> size Integer None  Optional <code>None</code> drop_file Boolean Use to not pass to other stages after this run.  Yes <code>False</code> partial Boolean Invalidate the current result cache creation.  Yes <code>False</code> from_archive Boolean Was loaded from the archive.  Yes <code>False</code>"},{"location":"odm/models/result/#responsebody","title":"ResponseBody","text":"<p>Response Body of Result.</p> Field Type Description Required Default milestones Milestone Milestone block.  Yes See Milestone for more details. service_version Keyword Version of the service.  Yes <code>None</code> service_name Keyword Name of the service that scanned the file.  Yes <code>None</code> service_tool_version Keyword Tool version of the service.  Optional <code>None</code> supplementary List [File] List of supplementary files.  Yes <code>[]</code> extracted List [File] List of extracted files.  Yes <code>[]</code> service_context Keyword Context about the service.  Optional <code>None</code> service_debug_info Keyword Debug info about the service.  Optional <code>None</code>"},{"location":"odm/models/result/#file","title":"File","text":"<p>File related to the Response.</p> Field Type Description Required Default name Keyword Name of the file.  Yes <code>None</code> sha256 SHA256 SHA256 of the file.  Yes <code>None</code> description Text Description of the file.  Yes <code>None</code> classification Classification Classification of the file.  Yes <code>None</code> is_section_image Boolean Is this an image used in an Image Result Section.  Yes <code>False</code> parent_relation Text File relation to parent, if any.            Values: <code>\"ROOT\", \"EXTRACTED\", \"INFORMATION\", \"DYNAMIC\", \"MEMDUMP\", \"DOWNLOADED\"</code>.  Yes <code>EXTRACTED</code> allow_dynamic_recursion Boolean Allow file to be analysed during Dynamic Analysis even if Dynamic Recursion Prevention is enabled.  Yes <code>False</code>"},{"location":"odm/models/result/#milestone","title":"Milestone","text":"<p>Service Milestones.</p> Field Type Description Required Default service_started Date Date the service started scanning.  Yes <code>NOW</code> service_completed Date Date the service finished scanning.  Yes <code>NOW</code>"},{"location":"odm/models/result/#resultbody","title":"ResultBody","text":"<p>Result Body.</p> Field Type Description Required Default score Integer Aggregate of the score for all heuristics.  Yes <code>0</code> sections List [Section] List of sections.  Yes <code>[]</code>"},{"location":"odm/models/result/#section","title":"Section","text":"<p>Result Section.</p> Field Type Description Required Default auto_collapse Boolean Auto-collapse result sections upon loading.  Yes <code>False</code> body Text Text body of the result section.  Optional <code>None</code> classification Classification Security classification of the individual result section.  Yes <code>None</code> body_format Enum Type of body in this section.Supported values are:<code>\"GRAPH_DATA\", \"IMAGE\", \"JSON\", \"KEY_VALUE\", \"MEMORY_DUMP\", \"MULTI\", \"ORDERED_KEY_VALUE\", \"PROCESS_TREE\", \"SANDBOX\", \"TABLE\", \"TEXT\", \"TIMELINE\", \"URL\"</code>  Yes <code>None</code> body_config Mapping [String, Any] None  Optional <code>None</code> depth Integer Depth of the section.  Yes <code>None</code> heuristic Heuristic Heuristic triggered in a result section.  Optional <code>None</code> tags Tagging List of tags associated with this section.  Yes See Tagging for more details. safelisted_tags FlatMapping List of safelisted tags.  Yes <code>{}</code> title_text Text Title of the section.  Yes <code>None</code> promote_to Enum NoneSupported values are:<code>\"ENTROPY\", \"SCREENSHOT\", \"URI_PARAMS\"</code>  Optional <code>None</code>"},{"location":"odm/models/result/#heuristic","title":"Heuristic","text":"<p>Heuristic associated with the result section.</p> Field Type Description Required Default heur_id Keyword Heuristic ID.  Yes <code>None</code> name Keyword Name of the heuristic.  Yes <code>None</code> attack List [Attack] List of the Mitre Att&amp;ck IDs related to this heuristic.  Yes <code>[]</code> signature List [Signature] List of signatures that triggered the heuristic.  Yes <code>[]</code> score Integer Calculated heuristic score.  Yes <code>None</code>"},{"location":"odm/models/result/#attack","title":"Attack","text":"<p>None</p> Field Type Description Required Default attack_id Keyword Mitre ATT&amp;CK ID.  Yes <code>None</code> pattern Keyword MITRE ATT&amp;CK\u00ae framework patterns identified in the analysis.  Yes <code>None</code> categories List [Keyword] MITRE ATT&amp;CK\u00ae framework categories associated with the alert.  Yes <code>None</code>"},{"location":"odm/models/result/#signature","title":"Signature","text":"<p>Heuristic Signatures.</p> Field Type Description Required Default name Keyword Name of the signature that triggered a heuristic.  Yes <code>None</code> frequency Integer Number of times this signature triggered a heuristic.  Yes <code>1</code> safe Boolean Is the signature safelisted or not.  Yes <code>False</code>"},{"location":"odm/models/retrohunt/","title":"RetrohuntHit","text":""},{"location":"odm/models/retrohunt/#retrohunthit","title":"RetrohuntHit","text":"<p>A hit encountered during a retrohunt search.</p> Field Type Description Required Default key Keyword Unique code indentifying this hit  Yes <code>None</code> classification Classification Classification string for the retrohunt job and results list  Yes <code>None</code> sha256 SHA256 None  Yes <code>None</code> expiry_ts Date Expiry for this entry.  Optional <code>None</code> search Keyword None  Yes <code>None</code>"},{"location":"odm/models/retrohunt/#retrohunt","title":"Retrohunt","text":"<p>A search run on stored files.</p> Field Type Description Required Default indices Enum Defines the indices used for this retrohunt jobSupported values are:<code>\"archive\", \"hot\", \"hot_and_archive\"</code>  Yes <code>hot_and_archive</code> classification Classification Classification for this retrohunt job  Yes <code>None</code> search_classification ClassificationString Maximum classification of results in the search  Yes <code>None</code> creator Keyword User who created this retrohunt job  Yes <code>None</code> description Text Human readable description of this retrohunt job  Yes <code>None</code> expiry_ts Date Expiry timestamp of this retrohunt job  Optional <code>None</code> start_group Long Earliest expiry group this search will include  Yes <code>None</code> end_group Long Latest expiry group this search will include  Yes <code>None</code> created_time Date Start time for the search.  Yes <code>None</code> started_time Date Start time for the search.  Yes <code>None</code> completed_time Date Time that the search ended  Optional <code>None</code> key Keyword Unique code identifying this retrohunt job  Yes <code>None</code> raw_query Keyword Text of filter query derived from yara signature  Yes <code>None</code> yara_signature Keyword Text of original yara signature run  Yes <code>None</code> errors List [Keyword] List of error messages that occured during the search  Yes <code>None</code> warnings List [Keyword] List of warning messages that occured during the search  Yes <code>None</code> finished Boolean Boolean that indicates if this retrohunt job is finished  Yes <code>False</code> truncated Boolean Indicates if the list of hits been truncated at some limit  Yes <code>False</code>"},{"location":"odm/models/safelist/","title":"Safelist","text":""},{"location":"odm/models/safelist/#safelist","title":"Safelist","text":"<p>Safelist Model</p> Field Type Description Required Default added Date Date when the safelisted hash was added  Yes <code>NOW</code> classification Classification Computed max classification for the safe hash  Yes <code>None</code> enabled Boolean Is safe hash enabled or not?  Yes <code>True</code> expiry_ts Date When does this item expire from the list?  Optional <code>None</code> hashes Hashes List of hashes related to the safe hash  Yes See Hashes for more details. file File Information about the file  Optional <code>None</code> sources List [Source] List of reasons why hash is safelisted  Yes <code>None</code> tag Tag Information about the tag  Optional <code>None</code> signature Signature Information about the signature  Optional <code>None</code> type Enum Type of safe hashSupported values are:<code>\"file\", \"signature\", \"tag\"</code>  Yes <code>None</code> updated Date Last date when sources were added to the safe hash  Yes <code>NOW</code>"},{"location":"odm/models/safelist/#file","title":"File","text":"<p>File Details</p> Field Type Description Required Default name List [Keyword] List of names seen for that file  Yes <code>[]</code> size Long Size of the file in bytes  Optional <code>None</code> type Keyword Type of file as identified by Assemblyline  Optional <code>None</code>"},{"location":"odm/models/safelist/#hashes","title":"Hashes","text":"<p>Hashes of a safelisted file</p> Field Type Description Required Default md5 MD5 MD5  Optional <code>None</code> sha1 SHA1 SHA1  Optional <code>None</code> sha256 SHA256 SHA256  Optional <code>None</code>"},{"location":"odm/models/safelist/#signature","title":"Signature","text":"<p>Signature</p> Field Type Description Required Default name Keyword Name of the signature  Yes <code>None</code>"},{"location":"odm/models/safelist/#source","title":"Source","text":"<p>Safelist source</p> Field Type Description Required Default classification Classification Classification of the source  Yes <code>TLP:C</code> name Keyword Name of the source  Yes <code>None</code> reason List [Keyword] Reason for why file was safelisted  Yes <code>None</code> type Enum Type of safelisting sourceSupported values are:<code>\"external\", \"user\"</code>  Yes <code>None</code>"},{"location":"odm/models/safelist/#tag","title":"Tag","text":"<p>Tag associated to file</p> Field Type Description Required Default type Keyword Tag type  Yes <code>None</code> value Keyword Tag value  Yes <code>None</code>"},{"location":"odm/models/service/","title":"Service","text":""},{"location":"odm/models/service/#service","title":"Service","text":"<p>Service Configuration</p> Field Type Description Required Default accepts Keyword Regex to accept files as identified by Assemblyline  Yes <code>.*</code> auto_update Boolean Should the service be auto-updated?  Optional <code>None</code> rejects Keyword Regex to reject files as identified by Assemblyline  Optional <code>empty|metadata/.*</code> category Keyword Which category does this service belong to?  Yes <code>Static Analysis</code> classification ClassificationString Classification of the service  Yes <code>TLP:C</code> config Mapping [String, Any] Service Configuration  Yes <code>{}</code> description Text Description of service  Yes <code>NA</code> default_result_classification ClassificationString Default classification assigned to service results  Yes <code>TLP:C</code> enabled Boolean Is the service enabled (by default)?  Yes <code>False</code> is_external Boolean Does this service perform analysis outside of Assemblyline?  Yes <code>False</code> licence_count Integer How many licences is the service allowed to use?  Yes <code>0</code> min_instances Integer The minimum number of service instances. Overrides Scaler's min_instances configuration.  Optional <code>None</code> max_queue_length Integer If more than this many jobs are queued for this service drop those over this limit. 0 is unlimited.  Yes <code>0</code> uses_tags Boolean Does this service use tags from other services for analysis?  Yes <code>False</code> uses_tag_scores Boolean Does this service use scores of tags from other services for analysis?  Yes <code>False</code> uses_temp_submission_data Boolean Does this service use temp data from other services for analysis?  Yes <code>False</code> uses_metadata Boolean Does this service use submission metadata for analysis?  Yes <code>False</code> monitored_keys List [Keyword] This service watches these temporary keys for changes when partial results are produced.  Yes <code>[]</code> name Keyword Name of service  Yes <code>None</code> version Keyword Version of service  Yes <code>None</code> privileged Boolean Should the service be able to talk to core infrastructure or just service-server for tasking?  Yes <code>False</code> disable_cache Boolean Should the result cache be disabled for this service?  Yes <code>False</code> stage Keyword Which execution stage does this service run in?  Yes <code>CORE</code> submission_params List [SubmissionParams] Submission parameters of service  Yes <code>[]</code> timeout Integer Service task timeout, in seconds  Yes <code>60</code> docker_config DockerConfig Docker configuration for service  Yes <code>None</code> dependencies Mapping [String, DependencyConfig] Dependency configuration for service  Yes See DependencyConfig for more details. update_channel Enum What channel to watch for service updates?Supported values are:<code>\"beta\", \"dev\", \"rc\", \"stable\"</code>  Yes <code>stable</code> update_config UpdateConfig Update configuration for fetching external resources  Optional <code>None</code> recursion_prevention List [Keyword] List of service names/categories where recursion is prevented.  Yes <code>[]</code>"},{"location":"odm/models/service/#dependencyconfig","title":"DependencyConfig","text":"<p>Container's Dependency Configuration</p> Field Type Description Required Default container DockerConfig Docker container configuration for dependency  Yes <code>None</code> volumes Mapping [String, PersistentVolume] Volume configuration for dependency  Yes See PersistentVolume for more details. run_as_core Boolean Should this dependency run as other core components?  Yes <code>False</code>"},{"location":"odm/models/service/#dockerconfig","title":"DockerConfig","text":"<p>Docker Container Configuration</p> Field Type Description Required Default allow_internet_access Boolean Does the container have internet-access?  Yes <code>False</code> command List [Keyword] Command to run when container starts up.  Optional <code>None</code> cpu_cores Float CPU allocation  Yes <code>1.0</code> environment List [EnvironmentVariable] Additional environemnt variables for the container  Yes <code>[]</code> image Keyword Complete name of the Docker image with tag, may include registry  Yes <code>None</code> registry_username Keyword The username to use when pulling the image  Optional `` registry_password Keyword The password or token to use when pulling the image  Optional `` registry_type Enum The type of container registrySupported values are:<code>\"docker\", \"harbor\"</code>  Yes <code>docker</code> ports List [Keyword] What ports of container to expose?  Yes <code>[]</code> ram_mb Integer Container RAM limit  Yes <code>512</code> ram_mb_min Integer Container RAM request  Yes <code>256</code> service_account Keyword None  Optional <code>None</code> labels List [EnvironmentVariable] Additional container labels.  Yes <code>[]</code>"},{"location":"odm/models/service/#environmentvariable","title":"EnvironmentVariable","text":"<p>Environment Variable Model</p> Field Type Description Required Default name Keyword Name of Environment Variable  Yes <code>None</code> value Keyword Value of Environment Variable  Yes <code>None</code>"},{"location":"odm/models/service/#persistentvolume","title":"PersistentVolume","text":"<p>Container's Persistent Volume Configuration</p> Field Type Description Required Default mount_path Keyword Path into the container to mount volume  Yes <code>None</code> capacity Keyword The amount of storage allocated for volume  Yes <code>None</code> storage_class Keyword Storage class used to create volume  Yes <code>None</code> access_mode Enum Access mode for volumeSupported values are:<code>\"ReadWriteMany\", \"ReadWriteOnce\"</code>  Yes <code>ReadWriteOnce</code>"},{"location":"odm/models/service/#dockerconfig_1","title":"DockerConfig","text":"<p>Docker Container Configuration</p> Field Type Description Required Default allow_internet_access Boolean Does the container have internet-access?  Yes <code>False</code> command List [Keyword] Command to run when container starts up.  Optional <code>None</code> cpu_cores Float CPU allocation  Yes <code>1.0</code> environment List [EnvironmentVariable] Additional environemnt variables for the container  Yes <code>[]</code> image Keyword Complete name of the Docker image with tag, may include registry  Yes <code>None</code> registry_username Keyword The username to use when pulling the image  Optional `` registry_password Keyword The password or token to use when pulling the image  Optional `` registry_type Enum The type of container registrySupported values are:<code>\"docker\", \"harbor\"</code>  Yes <code>docker</code> ports List [Keyword] What ports of container to expose?  Yes <code>[]</code> ram_mb Integer Container RAM limit  Yes <code>512</code> ram_mb_min Integer Container RAM request  Yes <code>256</code> service_account Keyword None  Optional <code>None</code> labels List [EnvironmentVariable] Additional container labels.  Yes <code>[]</code>"},{"location":"odm/models/service/#environmentvariable_1","title":"EnvironmentVariable","text":"<p>Environment Variable Model</p> Field Type Description Required Default name Keyword Name of Environment Variable  Yes <code>None</code> value Keyword Value of Environment Variable  Yes <code>None</code>"},{"location":"odm/models/service/#submissionparams","title":"SubmissionParams","text":"<p>Submission Parameters for Service</p> Field Type Description Required Default default Any Default value (must match value in <code>value</code> field)  Yes <code>None</code> name Keyword Name of parameter  Yes <code>None</code> type Enum Type of parameterSupported values are:<code>\"bool\", \"int\", \"list\", \"str\"</code>  Yes <code>None</code> value Any Default value (must match value in <code>default</code> field)  Yes <code>None</code> list Any List of values if <code>type: list</code>  Optional <code>None</code> hide Boolean Should this parameter be hidden?  Yes <code>False</code>"},{"location":"odm/models/service/#updateconfig","title":"UpdateConfig","text":"<p>Update Configuration for Signatures</p> Field Type Description Required Default generates_signatures Boolean Does the updater produce signatures?  Yes <code>False</code> sources List [UpdateSource] List of external sources  Yes <code>[]</code> update_interval_seconds Integer Update check interval, in seconds  Yes <code>None</code> wait_for_update Boolean Should the service wait for updates first?  Yes <code>False</code> signature_delimiter Enum Delimiter used when given a list of signaturesSupported values are:<code>\"comma\", \"custom\", \"double_new_line\", \"file\", \"new_line\", \"none\", \"pipe\", \"space\"</code>  Yes <code>double_new_line</code> custom_delimiter Keyword Custom delimiter definition  Optional <code>None</code> default_pattern Text Default pattern used for matching files  Yes <code>.*</code>"},{"location":"odm/models/service/#updatesource","title":"UpdateSource","text":"<p>Update Source Configuration</p> Field Type Description Required Default name Keyword Name of source  Yes <code>None</code> password Keyword Password used to authenticate with source  Optional `` pattern Keyword Pattern used to find files of interest from source  Optional `` private_key Keyword Private key used to authenticate with source  Optional `` ca_cert Keyword CA cert for source  Optional `` ssl_ignore_errors Boolean Ignore SSL errors when reaching out to source?  Yes <code>False</code> proxy Keyword Proxy server for source  Optional `` uri Keyword URI to source  Yes <code>None</code> username Keyword Username used to authenticate with source  Optional `` headers List [EnvironmentVariable] Headers  Yes <code>[]</code> default_classification Classification Default classification used in absence of one defined in files from source  Yes <code>TLP:C</code> use_managed_identity Boolean Use managed identity for authentication with Azure DevOps  Yes <code>False</code> git_branch Keyword Branch to checkout from Git repository.  Optional `` sync Boolean Synchronize signatures with remote source. Allows system to auto-disable signatures no longer found in source.  Yes <code>False</code> fetch_method Enum Fetch method to be used with sourceSupported values are:<code>\"GET\", \"GIT\", \"POST\"</code>  Yes <code>GET</code> enabled Boolean Is this source active for periodic fetching?  Yes <code>True</code> override_classification Boolean Should the source's classfication override the signature's self-defined classification, if any?  Yes <code>False</code> configuration Mapping [String, Any] Processing configuration for source  Yes <code>{}</code> update_interval Integer Update check interval, in seconds, for this source  Optional <code>None</code> ignore_cache Boolean Ignore source caching and forcefully fetch from source  Yes <code>False</code> data Text Data that's sent in a POST request (<code>fetch_method=\"POST\"</code>)  Optional <code>None</code>"},{"location":"odm/models/service/#environmentvariable_2","title":"EnvironmentVariable","text":"<p>Environment Variable Model</p> Field Type Description Required Default name Keyword Name of Environment Variable  Yes <code>None</code> value Keyword Value of Environment Variable  Yes <code>None</code>"},{"location":"odm/models/service_delta/","title":"ServiceDelta","text":""},{"location":"odm/models/service_delta/#servicedelta","title":"ServiceDelta","text":"<p>Service Delta relative to Initial Service Configuration</p> Field Type Description Required Default accepts Keyword Refer to:Service  Optional <code>None</code> auto_update Boolean Refer to:Service  Optional <code>None</code> rejects Keyword Refer to:Service  Optional <code>None</code> category Keyword Refer to:Service  Optional <code>None</code> classification ClassificationString Refer to:Service  Optional <code>None</code> config Mapping [String, Any] Refer to:Service  Optional <code>None</code> description Text Refer to:Service  Optional <code>None</code> default_result_classification ClassificationString Refer to:Service  Optional <code>None</code> enabled Boolean Refer to:Service  Optional <code>None</code> is_external Boolean Refer to:Service  Optional <code>None</code> licence_count Integer Refer to:Service  Optional <code>None</code> max_queue_length Integer Refer to:Service  Optional <code>None</code> min_instances Integer Refer to:Service  Optional <code>None</code> uses_tags Boolean Refer to:Service  Optional <code>None</code> uses_tag_scores Boolean Refer to:Service  Optional <code>None</code> uses_temp_submission_data Boolean Refer to:Service  Optional <code>None</code> uses_metadata Boolean Refer to:Service  Optional <code>None</code> monitored_keys List [Keyword] None  Optional <code>None</code> name Keyword Refer to:Service  Optional <code>None</code> version Keyword Refer to:Service  Yes <code>None</code> privileged Boolean Refer to:Service  Optional <code>None</code> disable_cache Boolean Refer to:Service  Optional <code>None</code> stage Keyword Refer to:Service  Optional <code>None</code> submission_params List [SubmissionParamsDelta] Refer to:Service  Optional <code>None</code> timeout Integer Refer to:Service  Optional <code>None</code> docker_config DockerConfigDelta Refer to:Service  Optional <code>None</code> dependencies Mapping [String, DependencyConfigDelta] Refer to:Service  Yes See DependencyConfigDelta for more details. update_channel Enum Refer to:ServiceSupported values are:<code>\"beta\", \"dev\", \"rc\", \"stable\"</code>  Optional <code>None</code> update_config UpdateConfigDelta Refer to:Service  Optional <code>None</code> recursion_prevention List [Keyword] Refer to:Service  Optional <code>None</code>"},{"location":"odm/models/service_delta/#dependencyconfigdelta","title":"DependencyConfigDelta","text":"<p>None</p> Field Type Description Required Default container DockerConfigDelta Refer to:Service - DependencyConfig  Optional <code>None</code> volumes Mapping [String, PersistentVolumeDelta] Refer to:Service - DependencyConfig  Yes See PersistentVolumeDelta for more details. run_as_core Boolean Refer to:Service - DependencyConfig  Optional <code>None</code>"},{"location":"odm/models/service_delta/#dockerconfigdelta","title":"DockerConfigDelta","text":"<p>Docker Configuration Delta</p> Field Type Description Required Default allow_internet_access Boolean Refer to:Service - DockerConfig  Optional <code>None</code> command List [Keyword] Refer to:Service - DockerConfig  Optional <code>None</code> cpu_cores Float Refer to:Service - DockerConfig  Optional <code>None</code> environment List [EnvironmentVariable] Refer to:Service - DockerConfig  Optional <code>None</code> image Keyword Refer to:Service - DockerConfig  Optional <code>None</code> registry_username Keyword Refer to:Service - DockerConfig  Optional `` registry_password Keyword Refer to:Service - DockerConfig  Optional `` registry_type Enum Refer to:Service - DockerConfigSupported values are:<code>\"docker\", \"harbor\"</code>  Optional <code>None</code> ports List [Keyword] Refer to:Service - DockerConfig  Optional <code>None</code> ram_mb Integer Refer to:Service - DockerConfig  Optional <code>None</code> ram_mb_min Integer Refer to:Service - DockerConfig  Optional <code>None</code> service_account Keyword None  Optional <code>None</code> labels List [EnvironmentVariable] Refer to:Service - DockerConfig  Optional <code>None</code>"},{"location":"odm/models/service_delta/#environmentvariable","title":"EnvironmentVariable","text":"<p>None</p> Field Type Description Required Default name Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code> value Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code>"},{"location":"odm/models/service_delta/#persistentvolumedelta","title":"PersistentVolumeDelta","text":"<p>None</p> Field Type Description Required Default mount_path Keyword Refer to:Service - PeristentVolume  Optional <code>None</code> capacity Keyword Refer to:Service - PeristentVolume  Optional <code>None</code> storage_class Keyword Refer to:Service - PeristentVolume  Optional <code>None</code> access_mode Enum Refer to:Service - PeristentVolumeSupported values are:<code>\"ReadWriteMany\", \"ReadWriteOnce\"</code>  Optional <code>None</code>"},{"location":"odm/models/service_delta/#dockerconfigdelta_1","title":"DockerConfigDelta","text":"<p>Docker Configuration Delta</p> Field Type Description Required Default allow_internet_access Boolean Refer to:Service - DockerConfig  Optional <code>None</code> command List [Keyword] Refer to:Service - DockerConfig  Optional <code>None</code> cpu_cores Float Refer to:Service - DockerConfig  Optional <code>None</code> environment List [EnvironmentVariable] Refer to:Service - DockerConfig  Optional <code>None</code> image Keyword Refer to:Service - DockerConfig  Optional <code>None</code> registry_username Keyword Refer to:Service - DockerConfig  Optional `` registry_password Keyword Refer to:Service - DockerConfig  Optional `` registry_type Enum Refer to:Service - DockerConfigSupported values are:<code>\"docker\", \"harbor\"</code>  Optional <code>None</code> ports List [Keyword] Refer to:Service - DockerConfig  Optional <code>None</code> ram_mb Integer Refer to:Service - DockerConfig  Optional <code>None</code> ram_mb_min Integer Refer to:Service - DockerConfig  Optional <code>None</code> service_account Keyword None  Optional <code>None</code> labels List [EnvironmentVariable] Refer to:Service - DockerConfig  Optional <code>None</code>"},{"location":"odm/models/service_delta/#environmentvariable_1","title":"EnvironmentVariable","text":"<p>None</p> Field Type Description Required Default name Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code> value Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code>"},{"location":"odm/models/service_delta/#submissionparamsdelta","title":"SubmissionParamsDelta","text":"<p>None</p> Field Type Description Required Default default Any Refer to:Service - SubmissionParams  Optional <code>None</code> name Keyword Refer to:Service - SubmissionParams  Optional <code>None</code> type Enum Refer to:Service - SubmissionParamsSupported values are:<code>\"bool\", \"int\", \"list\", \"str\"</code>  Optional <code>None</code> value Any Refer to:Service - SubmissionParams  Optional <code>None</code> list Any Refer to:Service - SubmissionParams  Optional <code>None</code> hide Boolean Refer to:Service - SubmissionParams  Optional <code>None</code>"},{"location":"odm/models/service_delta/#updateconfigdelta","title":"UpdateConfigDelta","text":"<p>None</p> Field Type Description Required Default generates_signatures Boolean Refer to:Service - UpdateConfig  Optional <code>None</code> sources List [UpdateSourceDelta] Refer to:Service - UpdateConfig  Optional <code>None</code> update_interval_seconds Integer Refer to:Service - UpdateConfig  Optional <code>None</code> wait_for_update Boolean Refer to:Service - UpdateConfig  Optional <code>None</code> signature_delimiter Enum Refer to:Service - UpdateConfigSupported values are:<code>\"comma\", \"custom\", \"double_new_line\", \"file\", \"new_line\", \"none\", \"pipe\", \"space\"</code>  Optional <code>None</code> custom_delimiter Keyword Refer to:Service - UpdateConfig  Optional <code>None</code> default_pattern Text Refer to:Service - UpdateConfig  Optional <code>None</code>"},{"location":"odm/models/service_delta/#updatesourcedelta","title":"UpdateSourceDelta","text":"<p>None</p> Field Type Description Required Default name Keyword Refer to:Service - UpdateSource  Optional <code>None</code> password Keyword Refer to:Service - UpdateSource  Optional `` pattern Keyword Refer to:Service - UpdateSource  Optional `` private_key Keyword Refer to:Service - UpdateSource  Yes `` ca_cert Keyword Refer to:Service - UpdateSource  Optional `` ssl_ignore_errors Boolean Refer to:Service - UpdateSource  Yes <code>False</code> proxy Keyword Refer to:Service - UpdateSource  Optional `` uri Keyword Refer to:Service - UpdateSource  Optional <code>None</code> username Keyword Refer to:Service - UpdateSource  Optional `` headers List [EnvironmentVariable] Refer to:Service - UpdateSource  Optional <code>None</code> default_classification Classification Refer to:Service - UpdateSource  Optional <code>None</code> use_managed_identity Boolean Refer to:Service - UpdateSource  Optional <code>False</code> git_branch Keyword Refer to:Service - UpdateSource  Optional `` sync Boolean Refer to:Service - UpdateSource  Optional <code>False</code> fetch_method Enum Refer to:Service - UpdateSourceSupported values are:<code>\"GET\", \"GIT\", \"POST\"</code>  Optional <code>GET</code> enabled Boolean Refer to:Service - UpdateSource  Optional <code>True</code> override_classification Boolean Refer to:Service - UpdateSource  Optional <code>False</code> configuration Mapping [String, Any] Refer to:Service - UpdateSource  Optional <code>{}</code> update_interval Integer Refer to:Service - UpdateSource  Optional <code>None</code> ignore_cache Boolean Refer to:Service - UpdateSource  Optional <code>False</code> data Text Refer to:Service - UpdateSource  Optional <code>None</code>"},{"location":"odm/models/service_delta/#environmentvariable_2","title":"EnvironmentVariable","text":"<p>None</p> Field Type Description Required Default name Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code> value Keyword Refer to:Service - Enviroment Variable  Yes <code>None</code>"},{"location":"odm/models/signature/","title":"Signature","text":""},{"location":"odm/models/signature/#signature","title":"Signature","text":"<p>None</p> Field Type Description Required Default classification Classification Security classification assigned to the signature based on its contents and context.  Yes <code>TLP:C</code> data Text None  Yes <code>None</code> last_modified Date Notes the last modification timestamp of the signature.  Yes <code>NOW</code> name Keyword Name of the signature.  Yes <code>None</code> order Integer  no longer used in v4  Yes <code>1</code> revision Keyword  Yes <code>1</code> signature_id Keyword ID associated with the signature.  Optional <code>None</code> source Keyword Source or author of the signature.  Yes <code>None</code> state_change_date Date Date the signature's state was last changed.  Optional <code>None</code> state_change_user Keyword User who last changed the signature's state.  Optional <code>None</code> stats Statistics Stats associated with count, average, min, max, and sum of various signature metrics.  Yes See Statistics for more details. status Enum The current state of the signature (i.e. NOISY, DISABLED, DEPLOYED, etc.).Supported values are:<code>\"DEPLOYED\", \"DISABLED\", \"INVALID\", \"NOISY\", \"STAGING\", \"TESTING\"</code>  Yes <code>None</code> type Keyword The service type that the signature is associated with.  Yes <code>None</code>"},{"location":"odm/models/statistics/","title":"Statistics","text":""},{"location":"odm/models/statistics/#statistics","title":"Statistics","text":"<p>Statistics Model</p> Field Type Description Required Default count Integer Count of statistical hits  Yes <code>0</code> min Integer Minimum value of all stastical hits  Yes <code>0</code> max Integer Maximum value of all stastical hits  Yes <code>0</code> avg Integer Average of all stastical hits  Yes <code>0</code> sum Integer Sum of all stastical hits  Yes <code>0</code> first_hit Date Date of first hit of statistic  Optional <code>None</code> last_hit Date Date of last hit of statistic  Optional <code>None</code>"},{"location":"odm/models/submission/","title":"Submission","text":""},{"location":"odm/models/submission/#submission","title":"Submission","text":"<p>Model of Submission</p> Field Type Description Required Default archive_ts Date None  Optional <code>None</code> archived Boolean Submission is present in the malware archive.  Yes <code>False</code> classification Classification Overall security classification of the submission.  Yes <code>None</code> tracing_events List [TraceEvent] None  Yes <code>[]</code> error_count Integer Total number of errors in the submission.  Yes <code>None</code> errors List [Keyword] List of error keys present in the submission.  Yes <code>None</code> expiry_ts Date Timestamp for when the submission record expires.  Optional <code>None</code> file_count Integer Total number of files in the submission.  Yes <code>None</code> files List [File] List of files that were originally submitted.  Yes <code>None</code> max_score Integer The highest score across all files within a submission.  Yes <code>None</code> metadata FlatMapping Metadata associated with the submission.  Yes <code>{}</code> params SubmissionParams Submission parameter details.  Yes <code>None</code> results List [Wildcard] List of result keys from the submission.  Yes <code>None</code> sid UUID The ID associated with a submission.  Yes <code>None</code> state Enum State of the submission (ie. completed).Supported values are:<code>\"completed\", \"failed\", \"submitted\"</code>  Yes <code>None</code> to_be_deleted Boolean This submission is going to be deleted as soon as it finishes.  Yes <code>False</code> times Times Submission-specific times.  Yes See Times for more details. verdict Verdict Relates to the verdict of the submission (i.e. Malicious or Non-Malicious).  Yes See Verdict for more details. from_archive Boolean Was loaded from the archive.  Yes <code>False</code> scan_key Keyword None  Optional <code>None</code>"},{"location":"odm/models/submission/#file","title":"File","text":"<p>File Model of Submission.</p> Field Type Description Required Default name Keyword Name of the submission.  Yes <code>None</code> size Long Size of the submitted file in bytes.  Optional <code>None</code> sha256 SHA256 SHA256 hash of the submitted file.  Yes <code>None</code>"},{"location":"odm/models/submission/#submissionparams","title":"SubmissionParams","text":"<p>Submission Parameters</p> Field Type Description Required Default classification Classification Original classification of the submission.  Yes <code>TLP:C</code> deep_scan Boolean Select to perform a deep scan.  Yes <code>False</code> description Text User-supplied information applied to Submission Details.  Yes <code>None</code> generate_alert Boolean Generate alert upon completion of analysis.  Yes <code>False</code> groups List [Keyword] List relevant group or organization related to this scan.  Yes <code>[]</code> ignore_cache Boolean Ignore cached service results.  Yes <code>False</code> ignore_recursion_prevention Boolean Ignore recursions prevention to avoid performance issues.  Yes <code>False</code> ignore_filtering Boolean Ignore services in the FILTER category (i.e. Safelist).  Yes <code>False</code> ignore_size Boolean Ignore the file size limits.  Yes <code>False</code> never_drop Boolean Ingestion of submission will not be dropped as a result of ingestion queue volume.  Yes <code>False</code> malicious Boolean User confirmation that the submission is known to be malicious.  Yes <code>False</code> max_extracted Integer Max number of extracted files.  Yes <code>500</code> max_supplementary Integer Max number of supplementary files.  Yes <code>500</code> priority Integer Determines order in which submission is analyzed relative to the queue.  Yes <code>1000</code> psid UUID Submission ID of 'parent' submission that has not been resubmitted for extended scan.  Optional <code>None</code> quota_item Boolean Does this submission count against quota?  Yes <code>False</code> services ServiceSelection Identify which services will run in the relevant submission.  Yes See ServiceSelection for more details. service_spec Mapping [String, Mapping [String, Any]] Service-specific parameters for the relevant submission.  Yes <code>{}</code> submitter Keyword User who submitted the file.  Yes <code>None</code> trace Boolean Collect debug information about the processing of a submission.  Yes <code>False</code> ttl Integer Time, in days, to live for this submission.  Yes <code>0</code> type Keyword Source of submission (i.e. 'USER' or a particular sensor).  Yes <code>USER</code> initial_data Text Initialization for temporary submission data.  Optional <code>None</code> auto_archive Boolean Send submission to the archive upon completion of analysis.  Yes <code>False</code> delete_after_archive Boolean When the submission is archived, immediately delete from hot storage.  Yes <code>False</code> use_archive_alternate_dtl Boolean use alternating dtl when archiving.  Yes <code>False</code>"},{"location":"odm/models/submission/#serviceselection","title":"ServiceSelection","text":"<p>Service Selection Scheme.</p> Field Type Description Required Default selected List [Keyword] List of selected services.  Yes <code>['Filtering', 'Antivirus', 'Static Analysis', 'Extraction', 'Networking']</code> excluded List [Keyword] List of excluded services.  Yes <code>[]</code> rescan List [Keyword] List of services to rescan when initial run scores as malicious.  Yes <code>[]</code> resubmit List [Keyword] Add to service selection when resubmitting.  Yes <code>[]</code>"},{"location":"odm/models/submission/#times","title":"Times","text":"<p>Submission-Relevant Times.</p> Field Type Description Required Default completed Date Date at which the submission finished scanning.  Optional <code>None</code> submitted Date Date at which the submission started scanning.  Yes <code>NOW</code>"},{"location":"odm/models/submission/#traceevent","title":"TraceEvent","text":"<p>A logging event describing the processing of a submission.</p> Field Type Description Required Default timestamp Date None  Yes <code>NOW</code> event_type Keyword None  Yes <code>None</code> service Keyword None  Optional <code>None</code> file SHA256 None  Optional <code>None</code> message Keyword None  Optional <code>None</code>"},{"location":"odm/models/submission/#verdict","title":"Verdict","text":"<p>Submission Verdict</p> Field Type Description Required Default malicious List [Keyword] List all submissions that were labelled malicious by a specific user.  Yes <code>[]</code> non_malicious List [Keyword] List all submissions that were labelled non-malicious by a specific user.  Yes <code>[]</code>"},{"location":"odm/models/submission_summary/","title":"SubmissionSummary","text":""},{"location":"odm/models/submission_summary/#submissionsummary","title":"SubmissionSummary","text":"<p>Submission Summary Model</p> Field Type Description Required Default classification Classification Classification of the cache  Yes <code>TLP:C</code> filtered Boolean Has this cache entry been filtered?  Yes <code>False</code> expiry_ts Date Expiry timestamp  Yes <code>None</code> tags Text Tags cache  Yes <code>None</code> attack_matrix Text ATT&amp;CK Matrix cache  Yes <code>None</code> heuristics Text Heuristics cache  Yes <code>None</code> heuristic_sections Text All sections mapping to the heuristics  Yes <code>None</code> heuristic_name_map Text Map of heuristic names to IDs  Yes <code>None</code>"},{"location":"odm/models/submission_tree/","title":"SubmissionTree","text":""},{"location":"odm/models/submission_tree/#submissiontree","title":"SubmissionTree","text":"<p>Submission Tree Model</p> Field Type Description Required Default classification Classification Classification of the cache  Yes <code>TLP:C</code> filtered Boolean Has this cache entry been filtered?  Yes <code>False</code> expiry_ts Date Expiry timestamp  Yes <code>None</code> supplementary Text Tree of supplementary files  Yes <code>None</code> tree Text File tree cache  Yes <code>None</code>"},{"location":"odm/models/tagging/","title":"Tagging","text":""},{"location":"odm/models/tagging/#tagging","title":"Tagging","text":"<p>Top-level model containing all tagging metadata for an analysis.</p> Field Type Description Required Default attribution Attribution All attribution-related tags (actors, campaigns, tooling, etc.).  Optional <code>None</code> av AV Tags derived from antivirus detection names and heuristics.  Optional <code>None</code> cert Cert Tags derived from digital certificates and related fields.  Optional <code>None</code> code Code Tags capturing relationships to other code samples.  Optional <code>None</code> dynamic Dynamic Tags generated from sandbox or other dynamic analysis.  Optional <code>None</code> info Info General informational tags not covered by other categories.  Optional <code>None</code> file File Tags describing file content, structure, and embedded formats.  Optional <code>None</code> network Network Tags describing network indicators and communication patterns.  Optional <code>None</code> source List [Keyword] Tags describing where the sample or tagging information originated.  Optional <code>None</code> technique Technique Tags summarizing techniques, tactics, and tradecraft used.  Optional <code>None</code> vector List [Keyword] Tags describing delivery or infection vectors for the sample.  Optional <code>None</code>"},{"location":"odm/models/tagging/#av","title":"AV","text":"<p>Tags derived from antivirus detections and heuristics.</p> Field Type Description Required Default heuristic List [Keyword] Antivirus heuristic names or identifiers triggered by the sample.  Optional <code>None</code> virus_name List [Keyword] Virus or malware names reported by antivirus engines.  Optional <code>None</code>"},{"location":"odm/models/tagging/#attribution","title":"Attribution","text":"<p>Attribution-related tags such as actors, campaigns, and families.</p> Field Type Description Required Default actor List [UpperKeyword] Threat actors or groups attributed to this sample.  Optional <code>None</code> campaign List [UpperKeyword] Named campaigns or operations associated with this sample.  Optional <code>None</code> category List [UpperKeyword] High-level attribution categories (e.g. crimeware, nation-state).  Optional <code>None</code> exploit List [UpperKeyword] Named exploits or vulnerability identifiers used by this sample.  Optional <code>None</code> implant List [UpperKeyword] Malware implants or tools linked to the attributed actor.  Optional <code>None</code> family List [UpperKeyword] Malware families or codebases related to this sample.  Optional <code>None</code> network List [UpperKeyword] Network infrastructure or clusters used for attribution.  Optional <code>None</code>"},{"location":"odm/models/tagging/#cert","title":"Cert","text":"<p>Metadata tags extracted from digital certificates.</p> Field Type Description Required Default extended_key_usage List [Keyword] Extended key usage values indicating allowed certificate purposes.  Optional <code>None</code> issuer List [Keyword] Issuer distinguished name fields for the certificate.  Optional <code>None</code> key_usage List [Keyword] Key usage flags describing how the certificate key may be used.  Optional <code>None</code> owner List [Keyword] Subject entity that owns or controls the certificate.  Optional <code>None</code> serial_no List [Keyword] Certificate serial numbers.  Optional <code>None</code> signature_algo List [Keyword] Signature algorithm used to sign the certificate.  Optional <code>None</code> subject List [Keyword] Certificate subject distinguished name.  Optional <code>None</code> subject_alt_name List [Keyword] Subject alternative names (e.g. DNS names, IPs, emails).  Optional <code>None</code> thumbprint List [Keyword] Certificate thumbprints (hashes of the full certificate).  Optional <code>None</code> valid CertValid Structured validity period information for the certificate.  Optional <code>None</code> version List [Keyword] Certificate version numbers.  Optional <code>None</code>"},{"location":"odm/models/tagging/#certvalid","title":"CertValid","text":"<p>Certificate validity period (notBefore / notAfter).</p> Field Type Description Required Default start List [Keyword] Earliest date from which the certificate is valid.  Optional <code>None</code> end List [Keyword] Latest date until which the certificate is valid.  Optional <code>None</code>"},{"location":"odm/models/tagging/#code","title":"Code","text":"<p>Tags describing code-level relationships between samples.</p> Field Type Description Required Default sha256 List [SHA256] SHA256 hashes of related code blobs, modules, or snippets.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamic","title":"Dynamic","text":"<p>Tags produced by dynamic/sandbox analysis about runtime behavior.</p> Field Type Description Required Default autorun_location List [Keyword] Locations where persistence or autorun entries were created.  Optional <code>None</code> dos_device List [Keyword] DOS device paths (e.g. \\.) referenced during execution.  Optional <code>None</code> mutex List [Keyword] Mutex names used for synchronization or infection markers.  Optional <code>None</code> registry_key List [Keyword] Registry keys created, read, or modified at runtime.  Optional <code>None</code> process DynamicProcess Structured process information from sandbox execution.  Optional <code>None</code> signature DynamicSignature Structured list of sandbox or dynamic signatures that fired.  Optional <code>None</code> ssdeep DynamicSSDeep SSDeep-based fingerprints derived from dynamic artifacts.  Optional <code>None</code> window DynamicWindow Windows opened during dynamic analysis.  Optional <code>None</code> operating_system DynamicOperatingSystem Operating-system metadata from the sandbox environment.  Optional <code>None</code> processtree_id List [Keyword] Identifiers for nodes in the sandbox process tree.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamicoperatingsystem","title":"DynamicOperatingSystem","text":"<p>Operating system environment in the sandbox.</p> Field Type Description Required Default platform List [Platform] OS platform identifiers (e.g. Windows, Linux).  Optional <code>None</code> version List [Keyword] OS version strings observed (e.g. 10.0.19045).  Optional <code>None</code> processor List [Processor] CPU architecture (e.g. x86, x64) used in the sandbox.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamicprocess","title":"DynamicProcess","text":"<p>Processes observed during dynamic execution.</p> Field Type Description Required Default command_line List [Keyword] Command-line strings for processes started at runtime.  Optional <code>None</code> file_name List [Keyword] Executable or script filenames launched by the sample.  Optional <code>None</code> shortcut List [Keyword] Shortcut (.lnk) names or targets created or accessed.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamicssdeep","title":"DynamicSSDeep","text":"<p>SSDeep-based similarity hashes for dynamic artifacts.</p> Field Type Description Required Default cls_ids List [SSDeepHash] SSDeep hashes of CLSID-like identifiers seen during analysis.  Optional <code>None</code> dynamic_classes List [SSDeepHash] SSDeep hashes of dynamically loaded classes or COM objects.  Optional <code>None</code> regkeys List [SSDeepHash] SSDeep hashes of registry key strings accessed at runtime.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamicsignature","title":"DynamicSignature","text":"<p>Dynamic analysis signatures that fired.</p> Field Type Description Required Default category List [Keyword] High-level behavioral category for the dynamic signature.  Optional <code>None</code> family List [Keyword] Malware family name associated with the dynamic signature.  Optional <code>None</code> name List [Keyword] Human-readable name of the dynamic analysis signature.  Optional <code>None</code>"},{"location":"odm/models/tagging/#dynamicwindow","title":"DynamicWindow","text":"<p>Raw Windows-related identifiers from dynamic analysis.</p> Field Type Description Required Default cls_ids List [Keyword] CLSIDs or similar identifiers observed during execution.  Optional <code>None</code> dynamic_classes List [Keyword] Names of dynamically loaded classes or COM objects.  Optional <code>None</code> regkeys List [Keyword] Registry key paths accessed or modified.  Optional <code>None</code>"},{"location":"odm/models/tagging/#file","title":"File","text":"<p>Tags describing file structure, content, and embedded formats.</p> Field Type Description Required Default ancestry List [Keyword] Tags describing file genealogy or derivation relationships.  Optional <code>None</code> behavior List [Keyword] Behavioral characteristics inferred from analysis.  Optional <code>None</code> compiler List [Keyword] Compiler or toolchain used to build the file.  Optional <code>None</code> config List [Keyword] Configuration blocks or key-value settings extracted from the file.  Optional <code>None</code> date FileDate Structured date and timestamp metadata for the file.  Optional <code>None</code> elf FileELF Structured properties specific to ELF binaries.  Optional <code>None</code> lib List [Keyword] Libraries the file depends on or bundles.  Optional <code>None</code> lsh List [Keyword] Locality-sensitive hashes (LSH) computed for fuzzy similarity.  Optional <code>None</code> name FileName Structured tags describing observed file names and anomalies.  Optional <code>None</code> path List [Keyword] File system or archive paths where the file was seen.  Optional <code>None</code> rule Mapping [String, List [Keyword]] Rules or signatures that matched this file, grouped by source.  Optional <code>None</code> string FileStrings Structured categories of strings extracted from the file.  Optional <code>None</code> apk FileAPK Detailed properties specific to Android APK files.  Optional <code>None</code> jar FileJAR Detailed properties specific to Java JAR archives.  Optional <code>None</code> img FileIMG Detailed properties specific to image files.  Optional <code>None</code> ole FileOLE Detailed properties specific to OLE/Office documents.  Optional <code>None</code> pe FilePE Detailed properties specific to Windows PE binaries.  Optional <code>None</code> pdf FilePDF Detailed properties specific to PDF documents.  Optional <code>None</code> plist FilePList Detailed properties specific to Apple plist files.  Optional <code>None</code> powershell FilePowerShell Detailed properties specific to PowerShell scripts.  Optional <code>None</code> shortcut FileShortcut Detailed properties specific to Windows shortcut files.  Optional <code>None</code> swf FileSWF Detailed properties specific to SWF files.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileapk","title":"FileAPK","text":"<p>Metadata extracted from Android APK packages.</p> Field Type Description Required Default activity List [Keyword] Declared Android activities within the APK.  Optional <code>None</code> app FileAPKApp Application-level information from the APK.  Optional <code>None</code> feature List [Keyword] Optional hardware or software features requested by the app.  Optional <code>None</code> locale List [Keyword] Locales or languages supported by the application.  Optional <code>None</code> permission List [Keyword] Android permissions requested by the application.  Optional <code>None</code> pkg_name List [Keyword] Application package names (e.g. com.example.app).  Optional <code>None</code> provides_component List [Keyword] Components exposed by the APK (activities, services, providers, etc.).  Optional <code>None</code> sdk FileAPKSDK Structured Android SDK version information for the app.  Optional <code>None</code> used_library List [Keyword] Third-party or system libraries referenced by the APK.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileapkapp","title":"FileAPKApp","text":"<p>High-level information about the Android application.</p> Field Type Description Required Default label List [Keyword] User-facing application label shown on the device.  Optional <code>None</code> version List [Keyword] Application version strings from the manifest.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileapksdk","title":"FileAPKSDK","text":"<p>Android SDK version requirements.</p> Field Type Description Required Default min List [Keyword] Minimum Android SDK/API level required to run the app.  Optional <code>None</code> target List [Keyword] Target Android SDK/API level the app was built for.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filedate","title":"FileDate","text":"<p>Timestamp-related metadata associated with the file.</p> Field Type Description Required Default creation List [Keyword] File creation timestamps.  Optional <code>None</code> last_modified List [Keyword] File last-modified timestamps.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileelf","title":"FileELF","text":"<p>Metadata extracted from ELF binaries.</p> Field Type Description Required Default libraries List [Keyword] Shared libraries linked by the ELF file.  Optional <code>None</code> interpreter List [Keyword] Dynamic loader or interpreter path used by the ELF.  Optional <code>None</code> sections FileELFSections Structured metadata for ELF sections.  Optional <code>None</code> segments FileELFSegments Structured metadata for ELF program segments.  Optional <code>None</code> notes FileELFNotes Structured metadata for ELF notes.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileelfnotes","title":"FileELFNotes","text":"<p>Metadata contained in ELF NOTE segments.</p> Field Type Description Required Default name List [Keyword] ELF note or owner names.  Optional <code>None</code> type List [Keyword] ELF note type identifiers.  Optional <code>None</code> type_core List [Keyword] Core-dump related ELF note type identifiers.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileelfsections","title":"FileELFSections","text":"<p>Information about individual ELF sections.</p> Field Type Description Required Default name List [Keyword] Names of sections within the ELF file.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileelfsegments","title":"FileELFSegments","text":"<p>Information about ELF program segments.</p> Field Type Description Required Default type List [Keyword] Segment type identifiers (e.g. LOAD, DYNAMIC).  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileimg","title":"FileIMG","text":"<p>Metadata extracted from image files and containers.</p> Field Type Description Required Default exif_tool FileIMGExiftool Exiftool metadata for the image.  Optional <code>None</code> mega_pixels List [Keyword] Image size expressed in megapixels.  Optional <code>None</code> mode List [Keyword] Mode field from image metadata, typically indicating how the image was captured or encoded.  Optional <code>None</code> size List [Keyword] Image dimensions or overall size information.  Optional <code>None</code> sorted_metadata_hash List [Keyword] Hash of normalized and sorted metadata fields.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileimgexiftool","title":"FileIMGExiftool","text":"<p>Exiftool-derived metadata about the image.</p> Field Type Description Required Default creator_tool List [Keyword] Application or tool reported as having created the image.  Optional <code>None</code> derived_document_id List [Keyword] Identifier for a document derived from the original source.  Optional <code>None</code> document_id List [Keyword] Original document identifier stored in metadata.  Optional <code>None</code> instance_id List [Keyword] Unique identifier for this specific file instance.  Optional <code>None</code> toolkit List [Keyword] Toolkit or library used to generate or edit the image.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filejar","title":"FileJAR","text":"<p>Metadata extracted from Java JAR archives.</p> Field Type Description Required Default main_class List [Keyword] Main class specified in the JAR manifest.  Optional <code>None</code> main_package List [Keyword] Package containing the main class.  Optional <code>None</code> imported_package List [Keyword] Referenced or imported Java packages.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filename","title":"FileName","text":"<p>Observed file name variants and anomalies.</p> Field Type Description Required Default anomaly List [Keyword] Suspicious or unusual filename patterns.  Optional <code>None</code> extracted List [Keyword] Names of files extracted from the original sample.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileole","title":"FileOLE","text":"<p>Metadata extracted from OLE/Office compound documents.</p> Field Type Description Required Default macro FileOLEMacro Structured metadata describing macros embedded in the file.  Optional <code>None</code> summary FileOLESummary Structured summary/document-property metadata.  Optional <code>None</code> clsid List [Keyword] Class IDs (CLSIDs) for embedded OLE objects.  Optional <code>None</code> dde_link List [Keyword] Dynamic Data Exchange (DDE) link targets.  Optional <code>None</code> fib_timestamp List [Keyword] Timestamps from the File Information Block (FIB).  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileolemacro","title":"FileOLEMacro","text":"<p>Information about embedded OLE macros.</p> Field Type Description Required Default sha256 List [SHA256] SHA256 hashes of extracted macro streams.  Optional <code>None</code> suspicious_string List [Keyword] Strings from macros that were flagged as suspicious.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileolesummary","title":"FileOLESummary","text":"<p>Standard document summary properties from OLE.</p> Field Type Description Required Default author List [Keyword] Document author metadata.  Optional <code>None</code> codepage List [Keyword] Character encoding or code page information.  Optional <code>None</code> comment List [Keyword] Document comments or summary notes.  Optional <code>None</code> company List [Keyword] Company or organization name from metadata.  Optional <code>None</code> create_time List [Keyword] Original document creation timestamp.  Optional <code>None</code> last_printed List [Keyword] Timestamp when the document was last printed.  Optional <code>None</code> last_saved_by List [Keyword] User name that last saved the document.  Optional <code>None</code> last_saved_time List [Keyword] Timestamp when the document was last saved.  Optional <code>None</code> manager List [Keyword] Manager field from document properties.  Optional <code>None</code> subject List [Keyword] Document subject or brief description.  Optional <code>None</code> title List [Keyword] Document title string.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepdf","title":"FilePDF","text":"<p>Metadata and analysis artifacts from PDF documents.</p> Field Type Description Required Default date FilePDFDate Structured collection of PDF date-related metadata.  Optional <code>None</code> javascript FilePDFJavascript Structured metadata about JavaScript embedded inside the PDF.  Optional <code>None</code> stats FilePDFStats Structured statistics metadata describing the PDF layout.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepdfdate","title":"FilePDFDate","text":"<p>Date-related metadata fields from the PDF.</p> Field Type Description Required Default modified List [Keyword] PDF modification timestamps.  Optional <code>None</code> pdfx List [Keyword] PDF/X standard-related metadata values.  Optional <code>None</code> source_modified List [Keyword] Timestamp when the source document was last modified.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepdfjavascript","title":"FilePDFJavascript","text":"<p>Metadata about JavaScript embedded in the PDF.</p> Field Type Description Required Default sha1 List [SHA1] SHA1 hashes of JavaScript streams found in the PDF.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepdfstats","title":"FilePDFStats","text":"<p>Statistical fingerprints for the PDF structure.</p> Field Type Description Required Default sha1 List [SHA1] SHA1 hashes representing PDF structural statistics.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepe","title":"FilePE","text":"<p>Metadata extracted from Windows PE executables and libraries.</p> Field Type Description Required Default api_vector List [Keyword] Vector of imported or used APIs summarizing sample behavior.  Optional <code>None</code> authenticode FilePEAuthenticode Authenticode signature metadata for the PE.  Optional <code>None</code> debug FilePEDebug Debug directory metadata from the PE.  Optional <code>None</code> exports FilePEExports PE export table metadata.  Optional <code>None</code> imports FilePEImports PE import table metadata and associated hashes.  Optional <code>None</code> linker FilePELinker PE linker metadata.  Optional <code>None</code> oep FilePEOEP Entry-point bytes and hexdump information.  Optional <code>None</code> pdb_filename List [Keyword] Names or paths of referenced PDB debug symbol files.  Optional <code>None</code> resources FilePEResources PE resource metadata.  Optional <code>None</code> rich_header FilePERichHeader Rich header metadata for the PE.  Optional <code>None</code> sections FilePESections Metadata describing PE sections.  Optional <code>None</code> versions FilePEVersions Version resource metadata.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeauthenticode","title":"FilePEAuthenticode","text":"<p>Authenticode signature and catalog metadata.</p> Field Type Description Required Default spc_sp_opus_info FilePEAuthenticodeSpcSpOpusInfo SpcSpOpusInfo metadata about the signed program.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeauthenticodespcspopusinfo","title":"FilePEAuthenticodeSpcSpOpusInfo","text":"<p>SpcSpOpusInfo attributes describing the signed program.</p> Field Type Description Required Default program_name List [Keyword] Program name string from the Authenticode signature.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepedebug","title":"FilePEDebug","text":"<p>Debug directory information from the PE file.</p> Field Type Description Required Default guid List [Keyword] Debug GUIDs (e.g. PDB signature identifiers).  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeexports","title":"FilePEExports","text":"<p>Information about exported PE functions.</p> Field Type Description Required Default function_name List [Keyword] Names of functions exported by the PE file.  Optional <code>None</code> module_name List [Keyword] Name of the module (DLL/EXE) providing the export.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeimports","title":"FilePEImports","text":"<p>Information and fingerprints of imported PE functions.</p> Field Type Description Required Default fuzzy List [SSDeepHash] SSDeep hashes computed over the import table.  Optional <code>None</code> md5 List [MD5] MD5 hashes representing imported symbols or modules.  Optional <code>None</code> imphash List [MD5] Canonical import-hash (imphash) values for the PE.  Optional <code>None</code> sorted_fuzzy List [SSDeepHash] Fuzzy hashes computed over sorted import entries.  Optional <code>None</code> sorted_sha1 List [SHA1] SHA1 hashes computed over sorted import entries.  Optional <code>None</code> gimphash List [SHA256] Go-style import-hash values for Go binaries.  Optional <code>None</code> suspicious List [Keyword] Flags or descriptors for suspicious import patterns.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepelinker","title":"FilePELinker","text":"<p>Metadata related to the PE linker.</p> Field Type Description Required Default timestamp List [Keyword] Linker timestamp value from the PE header.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeoep","title":"FilePEOEP","text":"<p>Metadata about the PE original entry point (OEP).</p> Field Type Description Required Default bytes List [Keyword] Raw bytes taken around the entry point.  Optional <code>None</code> hexdump List [Keyword] Hexadecimal dump of bytes at the entry point.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileperesources","title":"FilePEResources","text":"<p>Metadata about embedded PE resources.</p> Field Type Description Required Default language List [Keyword] Resource language identifiers.  Optional <code>None</code> name List [Keyword] Resource names or identifiers.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileperichheader","title":"FilePERichHeader","text":"<p>Information about the PE Rich header.</p> Field Type Description Required Default hash List [Keyword] Hashes summarizing Rich header contents.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepesections","title":"FilePESections","text":"<p>Information about sections within the PE file.</p> Field Type Description Required Default hash List [Keyword] Hashes of section contents or characteristics.  Optional <code>None</code> name List [Keyword] Section names (e.g. .text, .rsrc).  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepeversions","title":"FilePEVersions","text":"<p>Version-information resources from the PE file.</p> Field Type Description Required Default description List [Keyword] Product or file description from version info.  Optional <code>None</code> filename List [Keyword] Original filename recorded in version info.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplist","title":"FilePList","text":"<p>Metadata extracted from Apple property list (plist) files.</p> Field Type Description Required Default installer_url List [Keyword] URL used to obtain or install the app.  Optional <code>None</code> min_os_version List [Keyword] Minimum OS version required to run the software.  Optional <code>None</code> requests_open_access List [Keyword] Indicates whether a component (e.g. keyboard) requests full access.  Optional <code>None</code> build FilePListBuild Structured build-environment details from the plist.  Optional <code>None</code> cf_bundle FilePListCFBundle Structured CFBundle-related metadata.  Optional <code>None</code> dt FilePListDT Structured developer tools (DT*) metadata.  Optional <code>None</code> ls FilePListLS Structured Launch Services configuration from the plist.  Optional <code>None</code> ns FilePListNS Structured Cocoa (NS*) configuration and behaviors.  Optional <code>None</code> ui FilePListUI Structured UI behavior metadata.  Optional <code>None</code> wk FilePListWK Structured WatchKit (WK*) metadata.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistbuild","title":"FilePListBuild","text":"<p>Build-environment metadata from the plist.</p> Field Type Description Required Default machine_os List [Keyword] Operating system version of the build machine.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistcfbundle","title":"FilePListCFBundle","text":"<p>CFBundle-related bundle metadata.</p> Field Type Description Required Default development_region List [Keyword] Default localization or development region.  Optional <code>None</code> display_name List [Keyword] Human-readable application display name.  Optional <code>None</code> executable List [Keyword] Name of the main executable binary.  Optional <code>None</code> identifier List [Keyword] Bundle identifier string (e.g. com.example.app).  Optional <code>None</code> name List [Keyword] Internal bundle name.  Optional <code>None</code> pkg_type List [Keyword] Package type code (e.g. APPL).  Optional <code>None</code> signature List [Keyword] Legacy creator/signature code values.  Optional <code>None</code> url_scheme List [Keyword] Custom URL schemes registered by the application.  Optional <code>None</code> version FilePListCFBundleVersion Structured bundle version information.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistcfbundleversion","title":"FilePListCFBundleVersion","text":"<p>Bundle version metadata.</p> Field Type Description Required Default long List [Keyword] Full or long-form bundle version string.  Optional <code>None</code> short List [Keyword] Short marketing version string.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistdt","title":"FilePListDT","text":"<p>Developer tools (DT*) metadata fields.</p> Field Type Description Required Default compiler List [Keyword] Compiler or build tool identifier.  Optional <code>None</code> platform FilePListDTPlatform Structured platform metadata used for building the app.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistdtplatform","title":"FilePListDTPlatform","text":"<p>Platform-specific build metadata.</p> Field Type Description Required Default build List [Keyword] Platform build identifier.  Optional <code>None</code> name List [Keyword] Platform name (e.g. iPhoneOS, MacOSX).  Optional <code>None</code> version List [Keyword] Platform version number.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistls","title":"FilePListLS","text":"<p>Launch Services (LS*) metadata from the plist.</p> Field Type Description Required Default background_only List [Keyword] Indicates whether the app is background-only.  Optional <code>None</code> min_system_version List [Keyword] Minimum operating system version required by Launch Services.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistns","title":"FilePListNS","text":"<p>Cocoa (NS*) behavior flags from the plist.</p> Field Type Description Required Default apple_script_enabled List [Keyword] Whether AppleScript automation is allowed for the app.  Optional <code>None</code> principal_class List [Keyword] Name of the app's principal Objective-C class.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistui","title":"FilePListUI","text":"<p>User-interface-related plist keys.</p> Field Type Description Required Default background_modes List [Keyword] UI background modes the app declares.  Optional <code>None</code> requires_persistent_wifi List [Keyword] Indicates if the app requires persistent Wi-Fi connectivity.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileplistwk","title":"FilePListWK","text":"<p>WatchKit or WK* related metadata.</p> Field Type Description Required Default app_bundle_identifier List [Keyword] Bundle identifier of the associated application.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filepowershell","title":"FilePowerShell","text":"<p>Metadata extracted from PowerShell files or commands.</p> Field Type Description Required Default cmdlet List [Keyword] PowerShell cmdlets referenced or invoked by the script.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileswf","title":"FileSWF","text":"<p>Metadata extracted from Adobe Flash (SWF) files.</p> Field Type Description Required Default header FileSWFHeader Structured SWF header metadata.  Optional <code>None</code> tags_ssdeep List [SSDeepHash] SSDeep hashes computed over SWF tags for similarity.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileswfheader","title":"FileSWFHeader","text":"<p>Header-level metadata from the SWF file.</p> Field Type Description Required Default frame FileSWFHeaderFrame Structured SWF header frame information.  Optional <code>None</code> version List [Keyword] SWF file format version.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileswfheaderframe","title":"FileSWFHeaderFrame","text":"<p>Frame-rate and size information from the SWF header.</p> Field Type Description Required Default count List [Integer] Total number of frames in the SWF animation.  Optional <code>None</code> rate List [Keyword] Frame rate (speed) of the SWF animation.  Optional <code>None</code> size List [Keyword] Logical stage size or frame dimensions.  Optional <code>None</code>"},{"location":"odm/models/tagging/#fileshortcut","title":"FileShortcut","text":"<p>Metadata from Windows shortcut (.lnk) files.</p> Field Type Description Required Default command_line List [Keyword] Command line stored in or invoked by the shortcut.  Optional <code>None</code> icon_location List [Keyword] Path of the icon referenced by the shortcut.  Optional <code>None</code> machine_id List [Keyword] Machine identifier recorded within the shortcut.  Optional <code>None</code> tracker_mac List [Keyword] Potential MAC addresses recovered from the shortcut tracker block.  Optional <code>None</code>"},{"location":"odm/models/tagging/#filestrings","title":"FileStrings","text":"<p>Categorized strings extracted from the file.</p> Field Type Description Required Default api List [Keyword] Extracted strings that resemble API or function names.  Optional <code>None</code> blacklisted List [Keyword] Strings matching blacklist patterns or known bad indicators.  Optional <code>None</code> decoded List [Keyword] Strings obtained after decoding or deobfuscation.  Optional <code>None</code> extracted List [Keyword] Raw printable strings extracted from the file.  Optional <code>None</code>"},{"location":"odm/models/tagging/#info","title":"Info","text":"<p>General informational tags extracted from content.</p> Field Type Description Required Default phone_number List [PhoneNumber] Phone numbers extracted from the sample.  Optional <code>None</code> password List [Keyword] Passwords or password-like strings extracted from the sample.  Optional <code>None</code>"},{"location":"odm/models/tagging/#network","title":"Network","text":"<p>Tags for network indicators and traffic-related artifacts.</p> Field Type Description Required Default attack List [Keyword] High-level classification of observed or attempted attacks.  Optional <code>None</code> dynamic NetworkIOCs Network IOCs derived from dynamic/sandbox analysis.  Optional <code>None</code> email NetworkEmail Structured email-related network metadata.  Optional <code>None</code> mac_address List [MAC] MAC addresses observed in network or related artifacts.  Optional <code>None</code> port List [Integer] Network port numbers used by the sample.  Optional <code>None</code> protocol List [Keyword] Application or transport protocols observed (e.g. HTTP, TCP).  Optional <code>None</code> signature NetworkSignature Structured metadata for network detection signatures.  Optional <code>None</code> static NetworkIOCs Network IOCs derived from static analysis of the sample.  Optional <code>None</code> tls NetworkTLS Structured TLS handshake and fingerprint information.  Optional <code>None</code> user_agent List [Keyword] HTTP or other user-agent strings observed.  Optional <code>None</code>"},{"location":"odm/models/tagging/#networkemail","title":"NetworkEmail","text":"<p>Metadata from email-related network artifacts.</p> Field Type Description Required Default address List [Email] Sender or recipient email addresses observed.  Optional <code>None</code> date List [Keyword] Email date header values.  Optional <code>None</code> subject List [Keyword] Email subject lines.  Optional <code>None</code> msg_id List [Keyword] Email Message-ID header values.  Optional <code>None</code>"},{"location":"odm/models/tagging/#networkiocs","title":"NetworkIOCs","text":"<p>Network indicators of compromise (IOCs).</p> Field Type Description Required Default domain List [Domain] Domain names contacted, embedded, or otherwise referenced.  Optional <code>None</code> ip List [IP] IP addresses contacted, embedded, or otherwise referenced.  Optional <code>None</code> unc_path List [UNCPath] Windows UNC paths (\\server\\share) used by the sample.  Optional <code>None</code> uri List [URI] Full URIs or URLs observed (including scheme and host).  Optional <code>None</code> uri_path List [URIPath] URI path components without scheme or host.  Optional <code>None</code>"},{"location":"odm/models/tagging/#networksignature","title":"NetworkSignature","text":"<p>Network IDS/IPS or rule-engine signatures.</p> Field Type Description Required Default signature_id List [Keyword] Identifier of the network detection signature (e.g. SID).  Optional <code>None</code> message List [Keyword] Human-readable description of the network signature.  Optional <code>None</code>"},{"location":"odm/models/tagging/#networktls","title":"NetworkTLS","text":"<p>TLS fingerprint and metadata tags.</p> Field Type Description Required Default ja3_hash List [MD5] MD5 hash of the JA3 TLS client fingerprint.  Optional <code>None</code> ja3_string List [Keyword] Raw JA3 TLS client fingerprint string.  Optional <code>None</code> ja3s_hash List [MD5] MD5 hash of the JA3S TLS server fingerprint.  Optional <code>None</code> ja3s_string List [Keyword] Raw JA3S TLS server fingerprint string.  Optional <code>None</code> ja4_hash List [ValidatedKeyword] Validated JA4 TLS client fingerprint hash.  Optional <code>None</code> ja4s_hash List [ValidatedKeyword] Validated JA4S TLS server fingerprint hash.  Optional <code>None</code> sni List [Keyword] Server Name Indication (SNI) values from TLS handshakes.  Optional <code>None</code>"},{"location":"odm/models/tagging/#technique","title":"Technique","text":"<p>Tags capturing techniques and tradecraft used by the sample.</p> Field Type Description Required Default comms_routine List [Keyword] Patterns or routines used for C2 or other communications.  Optional <code>None</code> config List [Keyword] Technique-related configuration data (e.g. keys, flags).  Optional <code>None</code> crypto List [Keyword] Use of cryptographic algorithms, keys, or primitives.  Optional <code>None</code> exploit List [Keyword] Exploit techniques or identifiers used by the sample.  Optional <code>None</code> keylogger List [Keyword] Keylogging components or behaviors.  Optional <code>None</code> macro List [Keyword] Macro-based execution techniques or mechanisms.  Optional <code>None</code> masking_algo List [Keyword] Algorithms used for masking, encoding, or hiding data.  Optional <code>None</code> obfuscation List [Keyword] Obfuscation or anti-analysis techniques observed.  Optional <code>None</code> packer List [Keyword] Packers or protectors used to wrap the sample.  Optional <code>None</code> persistence List [Keyword] Persistence techniques used to survive reboot or logoff.  Optional <code>None</code> shellcode List [Keyword] Shellcode payloads or shellcode-based techniques.  Optional <code>None</code> string List [Keyword] Technique-related string patterns (e.g. markers, protocol strings).  Optional <code>None</code>"},{"location":"odm/models/user/","title":"User","text":""},{"location":"odm/models/user/#user","title":"User","text":"<p>Model of User</p> Field Type Description Required Default agrees_with_tos Date Date the user agree with terms of service  Optional <code>None</code> api_quota Integer None  Optional <code>None</code> api_daily_quota Integer None  Optional <code>None</code> apps Mapping [String, Apps] Applications with access to the account  Yes See Apps for more details. can_impersonate Boolean Allowed to query on behalf of others?  Yes <code>False</code> classification Classification Maximum classification for the user  Yes <code>TLP:C</code> dn Keyword User's LDAP DN  Optional <code>None</code> email Email User's email address  Optional <code>None</code> organization Text Organization the user belongs to  Optional <code>None</code> groups List [UpperKeyword] List of groups the user submits to  Yes <code>[]</code> identity_id Keyword ID of the matching object in your identity provider (used for logging in as another application)  Optional <code>None</code> is_active Boolean Is the user active?  Yes <code>True</code> name Keyword Full name of the user  Yes <code>None</code> otp_sk Keyword Secret key to generate one time passwords  Optional <code>None</code> password Keyword BCrypt hash of the user's password  Yes <code>None</code> submission_quota Integer None  Optional <code>None</code> submission_async_quota Integer None  Optional <code>None</code> submission_daily_quota Integer None  Optional <code>None</code> type List [Enum] Type of user  Yes <code>['user']</code> roles List [Enum] Default roles for user  Yes <code>[]</code> security_tokens Mapping [String, Keyword] Map of security tokens  Yes <code>{}</code> uname Keyword Username  Yes <code>None</code>"},{"location":"odm/models/user/#apps","title":"Apps","text":"<p>Model of Apps used of OBO (On Behalf Of)</p> Field Type Description Required Default client_id Keyword Username allowed to impersonate the current user  Yes <code>None</code> netloc Keyword DNS hostname for the server  Yes <code>None</code> scope Enum Scope of access for the App tokenSupported values are:<code>\"c\", \"r\", \"rw\", \"w\"</code>  Yes <code>None</code> server Keyword Name of the server that has access  Yes <code>None</code> roles List [Enum] List of roles tied to the App token  Yes <code>[]</code>"},{"location":"odm/models/user_favorites/","title":"UserFavorites","text":""},{"location":"odm/models/user_favorites/#userfavorites","title":"UserFavorites","text":"<p>Model of User Favorites</p> Field Type Description Required Default alert List [Favorite] Alert page favorites  Yes <code>[]</code> error List [Favorite] Error page favorites  Yes <code>[]</code> search List [Favorite] Search page favorites  Yes <code>[]</code> signature List [Favorite] Signature page favorites  Yes <code>[]</code> submission List [Favorite] Submission page favorites  Yes <code>[]</code>"},{"location":"odm/models/user_favorites/#favorite","title":"Favorite","text":"<p>Abstract Model of Favorite</p> Field Type Description Required Default created_by Keyword Who created the favorite  Yes <code>None</code> classification Classification Classification of the favorite  Yes <code>TLP:C</code> name Keyword Name of the favorite  Yes <code>None</code> query Keyword Query for the favorite  Yes <code>None</code>"},{"location":"odm/models/user_settings/","title":"UserSettings","text":""},{"location":"odm/models/user_settings/#usersettings","title":"UserSettings","text":"<p>Model of User Settings</p> Field Type Description Required Default download_encoding Enum Default download encoding when downloading filesSupported values are:<code>\"cart\", \"raw\", \"zip\"</code>  Yes <code>cart</code> default_external_sources List [Keyword] List of sha256 sources to check by default  Yes <code>[]</code> default_zip_password Text Default user-defined password for creating password protected ZIPs when downloading files  Yes <code>infected</code> default_metadata Mapping [String, Text] Default metadata to add to submissions  Yes <code>{}</code> executive_summary Boolean Should executive summary sections be shown?  Yes <code>True</code> expand_min_score Integer Auto-expand section when score bigger then this  Yes <code>500</code> preferred_submission_profile Text Preferred submission profile  Optional <code>None</code> submission_profiles Mapping [String, SubmissionProfileParams] Default submission profile settings  Yes See SubmissionProfileParams for more details. submission_view Enum Default view for completed submissionsSupported values are:<code>\"details\", \"report\"</code>  Yes <code>report</code>"},{"location":"odm/models/workflow/","title":"Workflow","text":""},{"location":"odm/models/workflow/#workflow","title":"Workflow","text":"<p>Model of Workflow</p> Field Type Description Required Default classification Classification Classification of the workflow  Yes <code>TLP:C</code> creation_date Date Creation date of the workflow  Yes <code>NOW</code> creator Keyword UID of the creator of the workflow  Yes <code>None</code> edited_by Keyword UID of the last user to edit the workflow  Yes <code>None</code> enabled Boolean Is this workflow enabled?  Yes <code>True</code> first_seen Date Date of first hit on workflow  Optional <code>None</code> hit_count Integer Number of times there was a workflow hit  Yes <code>0</code> labels List [Keyword] Labels applied by the workflow  Yes <code>[]</code> last_edit Date Date of last edit on workflow  Yes <code>NOW</code> last_seen Date Date of last hit on workflow  Optional <code>None</code> name Keyword Name of the workflow  Yes <code>None</code> origin Keyword Which did this originate from?  Optional <code>None</code> priority Enum Priority applied by the workflowSupported values are:<code>\"CRITICAL\", \"HIGH\", \"LOW\", \"MEDIUM\", None</code>  Optional <code>None</code> query Keyword Query that the workflow runs  Yes <code>None</code> status Enum Status applied by the workflowSupported values are:<code>\"ASSESS\", \"MALICIOUS\", \"NON-MALICIOUS\", \"TRIAGE\", None</code>  Optional <code>None</code> workflow_id UUID ID of the workflow  Optional <code>None</code>"},{"location":"odm/models/ontology/file/","title":"File","text":""},{"location":"odm/models/ontology/file/#file","title":"File","text":"<p>File Characteristics</p> Field Type Description Required Default md5 MD5 MD5 of file  Yes <code>None</code> sha1 SHA1 SHA1 of file  Yes <code>None</code> sha256 SHA256 SHA256 of file  Yes <code>None</code> type Keyword None  Optional <code>None</code> size Integer Size of the file in bytes  Yes <code>None</code> names List [Text] Known filenames associated to file  Optional <code>None</code> parent SHA256 Absolute parent of file relative to submission  Optional <code>None</code> pe PE Properties related to PE  Optional <code>None</code>"},{"location":"odm/models/ontology/ontology/","title":"ResultOntology","text":""},{"location":"odm/models/ontology/ontology/#resultontology","title":"ResultOntology","text":"<p>Assemblyline Result Ontology</p> Field Type Description Required Default odm_type Text Type of ODM Model  Yes <code>Assemblyline Result Ontology</code> odm_version Text Version of ODM Model  Yes <code>1.10</code> classification ClassificationString Classification of Ontological Record  Yes <code>None</code> file File Descriptors about file being analyzed  Yes <code>None</code> service Service Information about Service  Yes <code>None</code> submission Submission Information about Submission  Optional <code>None</code> results Results Ontological Results  Optional <code>None</code>"},{"location":"odm/models/ontology/ontology/#results","title":"Results","text":"<p>Ontological Results</p> Field Type Description Required Default antivirus List [Antivirus] List of Antivirus Ontologies  Optional <code>None</code> http List [HTTP] List of HTTP Ontologies  Optional <code>None</code> malwareconfig List [MalwareConfig] List of MalwareConfig Ontologies  Optional <code>None</code> netflow List [NetworkConnection] List of Network Ontologies  Optional <code>None</code> process List [Process] List of Process Ontologies  Optional <code>None</code> sandbox List [Sandbox] List of Sandbox Ontologies  Optional <code>None</code> signature List [Signature] List of Signature Ontologies  Optional <code>None</code> tags Mapping [String, List [Any]] Tags raised during analysis. Refer to Tagging  Optional <code>None</code> heuristics List [Heuristics] Heuristics raised during analysis  Optional <code>None</code> score Integer None  Optional <code>None</code> other Mapping [String, Text] Miscellaneous unstructured data recorded during analysis  Optional <code>None</code>"},{"location":"odm/models/ontology/ontology/#heuristics","title":"Heuristics","text":"<p>Heuristics raised</p> Field Type Description Required Default heur_id Keyword Heuristic ID  Yes <code>None</code> score Integer Score associated to heurstic  Yes <code>None</code> times_raised Integer The number of times the heuristic was raised  Yes <code>None</code> name Text Name of the heuristic raised  Yes <code>None</code> tags Mapping [String, List [Any]] Tags associated to heuristic. Refer to Tagging  Yes <code>None</code>"},{"location":"odm/models/ontology/ontology/#service","title":"Service","text":"<p>Service Details</p> Field Type Description Required Default name Keyword Service Name  Yes <code>None</code> version Keyword Service Version  Yes <code>None</code> tool_version Keyword Service Tool Version  Optional ``"},{"location":"odm/models/ontology/ontology/#submission","title":"Submission","text":"<p>Submission Details</p> Field Type Description Required Default date Date Date of analysis  Optional <code>None</code> metadata Mapping [String, Text] Metadata associated to submission  Yes <code>None</code> sid Keyword Submission ID associated to file  Optional <code>None</code> source_system Text Which Assemblyline instance does the result originate from?  Optional <code>None</code> original_source Text Source as specified by submitter (from metadata)  Optional <code>None</code> classification ClassificationString Submitted classification  Yes <code>TLP:C</code> submitter Keyword Submitter  Optional <code>None</code> retention_id Keyword Reference to knowledge base for long-term data retention.  Optional <code>None</code> max_score Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/","title":"PE","text":""},{"location":"odm/models/ontology/filetypes/pe/#pe","title":"PE","text":"<p>None</p> Field Type Description Required Default name EmptyableKeyword None  Optional <code>None</code> format EmptyableKeyword None  Optional <code>None</code> imphash EmptyableKeyword None  Optional <code>None</code> entrypoint Integer None  Optional <code>None</code> header Header None  Optional <code>None</code> optional_header Optional_Header None  Optional <code>None</code> dos_header Dos_Header None  Optional <code>None</code> rich_header Rich_Header None  Optional <code>None</code> nx Boolean None  Optional <code>None</code> authentihash Authentihash None  Optional <code>None</code> tls TLS None  Optional <code>None</code> position_independent Boolean None  Optional <code>None</code> is_reproducible_build Boolean None  Optional <code>None</code> size_of_headers Integer None  Optional <code>None</code> virtual_size Integer None  Optional <code>None</code> size Integer None  Optional <code>None</code> sections List [Sections] None  Optional <code>None</code> debugs List [Debug] None  Optional <code>None</code> export Export None  Optional <code>None</code> imports List [Import] None  Optional <code>None</code> load_configuration Load_Configuration None  Optional <code>None</code> resources_manager Resources_Manager None  Optional <code>None</code> resources List [Resource] None  Optional <code>None</code> verify_signature EmptyableKeyword None  Optional <code>None</code> signatures List [Signature] None  Optional <code>None</code> overlay Overlay None  Optional <code>None</code> relocations List [Relocation] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#authentihash","title":"Authentihash","text":"<p>None</p> Field Type Description Required Default sha512 EmptyableKeyword None  Optional <code>None</code> sha384 EmptyableKeyword None  Optional <code>None</code> sha256 SHA256 None  Optional <code>None</code> sha1 SHA1 None  Optional <code>None</code> md5 MD5 None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#debug","title":"Debug","text":"<p>None</p> Field Type Description Required Default addressof_rawdata Integer None  Optional <code>None</code> characteristics Integer None  Optional <code>None</code> major_version Integer None  Optional <code>None</code> minor_version Integer None  Optional <code>None</code> pointerto_rawdata Integer None  Optional <code>None</code> sizeof_data Integer None  Optional <code>None</code> timestamp Integer None  Optional <code>None</code> hr_timestamp Date None  Optional <code>None</code> type EmptyableKeyword None  Optional <code>None</code> code_view CodeView None  Optional <code>None</code> pogo POGO None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#codeview","title":"CodeView","text":"<p>None</p> Field Type Description Required Default age Integer None  Optional <code>None</code> cv_signature EmptyableKeyword None  Optional <code>None</code> filename EmptyableKeyword None  Optional <code>None</code> guid EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#pogo","title":"POGO","text":"<p>None</p> Field Type Description Required Default entries List [Entry] None  Optional <code>None</code> signature EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#entry","title":"Entry","text":"<p>None</p> Field Type Description Required Default name EmptyableKeyword None  Optional <code>None</code> size Integer None  Optional <code>None</code> start_rva Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#dos_header","title":"Dos_Header","text":"<p>None</p> Field Type Description Required Default addressof_new_exeheader Integer None  Optional <code>None</code> addressof_relocation_table Integer None  Optional <code>None</code> checksum Integer None  Optional <code>None</code> file_size_in_pages Integer None  Optional <code>None</code> header_size_in_paragraphs Integer None  Optional <code>None</code> initial_ip Integer None  Optional <code>None</code> initial_relative_cs Integer None  Optional <code>None</code> initial_relative_ss Integer None  Optional <code>None</code> initial_sp Integer None  Optional <code>None</code> magic Integer None  Optional <code>None</code> maximum_extra_paragraphs Integer None  Optional <code>None</code> minimum_extra_paragraphs Integer None  Optional <code>None</code> numberof_relocation Integer None  Optional <code>None</code> oem_id Integer None  Optional <code>None</code> oem_info Integer None  Optional <code>None</code> overlay_number Integer None  Optional <code>None</code> used_bytes_in_the_last_page Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#export","title":"Export","text":"<p>None</p> Field Type Description Required Default entries List [Entry] None  Optional <code>None</code> export_flags Integer None  Optional <code>None</code> major_version Integer None  Optional <code>None</code> minor_version Integer None  Optional <code>None</code> name EmptyableKeyword None  Optional <code>None</code> ordinal_base Integer None  Optional <code>None</code> timestamp Integer None  Optional <code>None</code> hr_timestamp Date None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#entry_1","title":"Entry","text":"<p>None</p> Field Type Description Required Default address Integer None  Optional <code>None</code> forward_information Forward_Information None  Optional <code>None</code> function_rva Integer None  Optional <code>None</code> is_extern Boolean None  Optional <code>None</code> name EmptyableKeyword None  Optional <code>None</code> ordinal Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#forward_information","title":"Forward_Information","text":"<p>None</p> Field Type Description Required Default function EmptyableKeyword None  Optional <code>None</code> library EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#header","title":"Header","text":"<p>None</p> Field Type Description Required Default characteristics_hash Integer None  Optional <code>None</code> characteristics_list List [EmptyableKeyword] None  Optional <code>None</code> machine EmptyableKeyword None  Optional <code>None</code> numberof_sections Integer None  Optional <code>None</code> numberof_symbols Integer None  Optional <code>None</code> signature List [Integer] None  Optional <code>None</code> timestamp Integer None  Optional <code>None</code> hr_timestamp Date None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#import","title":"Import","text":"<p>None</p> Field Type Description Required Default library EmptyableKeyword None  Optional <code>None</code> data Integer None  Optional <code>None</code> hint Integer None  Optional <code>None</code> iat_address Integer None  Optional <code>None</code> iat_value Integer None  Optional <code>None</code> is_ordinal Boolean None  Optional <code>None</code> name EmptyableKeyword None  Optional <code>None</code> ordinal Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#load_configuration","title":"Load_Configuration","text":"<p>None</p> Field Type Description Required Default characteristics Integer None  Optional <code>None</code> critical_section_default_timeout Integer None  Optional <code>None</code> csd_version Integer None  Optional <code>None</code> decommit_free_block_threshold Integer None  Optional <code>None</code> decommit_total_free_threshold Integer None  Optional <code>None</code> editlist Integer None  Optional <code>None</code> global_flags_clear Integer None  Optional <code>None</code> global_flags_set Integer None  Optional <code>None</code> lock_prefix_table Integer None  Optional <code>None</code> major_version Integer None  Optional <code>None</code> maximum_allocation_size Integer None  Optional <code>None</code> minor_version Integer None  Optional <code>None</code> process_affinity_mask Integer None  Optional <code>None</code> process_heap_flags Integer None  Optional <code>None</code> reserved1 Integer None  Optional <code>None</code> security_cookie Integer None  Optional <code>None</code> timedatestamp Integer None  Optional <code>None</code> hr_timedatestamp Date None  Optional <code>None</code> version EmptyableKeyword None  Optional <code>None</code> virtual_memory_threshold Integer None  Optional <code>None</code> se_handler_count Integer None  Optional <code>None</code> se_handler_table Integer None  Optional <code>None</code> guard_cf_check_function_pointer Integer None  Optional <code>None</code> guard_cf_dispatch_function_pointer Integer None  Optional <code>None</code> guard_cf_flags_list List [EmptyableKeyword] None  Optional <code>None</code> guard_cf_function_count Integer None  Optional <code>None</code> guard_cf_function_table Integer None  Optional <code>None</code> guard_flags EmptyableKeyword None  Optional <code>None</code> code_integrity Code_Integrity None  Optional <code>None</code> guard_address_taken_iat_entry_count Integer None  Optional <code>None</code> guard_address_taken_iat_entry_table Integer None  Optional <code>None</code> guard_long_jump_target_count Integer None  Optional <code>None</code> guard_long_jump_target_table Integer None  Optional <code>None</code> dynamic_value_reloc_table Integer None  Optional <code>None</code> hybrid_metadata_pointer Integer None  Optional <code>None</code> dynamic_value_reloctable_offset Integer None  Optional <code>None</code> dynamic_value_reloctable_section Integer None  Optional <code>None</code> guard_rf_failure_routine Integer None  Optional <code>None</code> guard_rf_failure_routine_function_pointer Integer None  Optional <code>None</code> reserved2 Integer None  Optional <code>None</code> guard_rf_verify_stackpointer_function_pointer Integer None  Optional <code>None</code> hotpatch_table_offset Integer None  Optional <code>None</code> addressof_unicode_string Integer None  Optional <code>None</code> reserved3 Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#code_integrity","title":"Code_Integrity","text":"<p>None</p> Field Type Description Required Default catalog Integer None  Optional <code>None</code> catalog_offset Integer None  Optional <code>None</code> flags Integer None  Optional <code>None</code> reserved Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#optional_header","title":"Optional_Header","text":"<p>None</p> Field Type Description Required Default addressof_entrypoint Integer None  Optional <code>None</code> baseof_code Integer None  Optional <code>None</code> baseof_data Integer None  Optional <code>None</code> checksum Integer None  Optional <code>None</code> computed_checksum Integer None  Optional <code>None</code> dll_characteristics Integer None  Optional <code>None</code> dll_characteristics_lists List [EmptyableKeyword] None  Optional <code>None</code> file_alignment Integer None  Optional <code>None</code> imagebase Integer None  Optional <code>None</code> loader_flags Integer None  Optional <code>None</code> magic EmptyableKeyword None  Optional <code>None</code> major_image_version Integer None  Optional <code>None</code> major_linker_version Integer None  Optional <code>None</code> major_operating_system_version Integer None  Optional <code>None</code> major_subsystem_version Integer None  Optional <code>None</code> minor_image_version Integer None  Optional <code>None</code> minor_linker_version Integer None  Optional <code>None</code> minor_operating_system_version Integer None  Optional <code>None</code> minor_subsystem_version Integer None  Optional <code>None</code> numberof_rva_and_size Integer None  Optional <code>None</code> section_alignment Integer None  Optional <code>None</code> sizeof_code Integer None  Optional <code>None</code> sizeof_headers Integer None  Optional <code>None</code> sizeof_heap_commit Integer None  Optional <code>None</code> sizeof_heap_reserve Integer None  Optional <code>None</code> sizeof_image Integer None  Optional <code>None</code> sizeof_initialized_data Integer None  Optional <code>None</code> sizeof_stack_commit Integer None  Optional <code>None</code> sizeof_stack_reserve Integer None  Optional <code>None</code> sizeof_uninitialized_data Integer None  Optional <code>None</code> subsystem EmptyableKeyword None  Optional <code>None</code> win32_version_value Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#overlay","title":"Overlay","text":"<p>None</p> Field Type Description Required Default size Integer None  Optional <code>None</code> entropy Float None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#relocation","title":"Relocation","text":"<p>None</p> Field Type Description Required Default virtual_address Integer None  Optional <code>None</code> entries List [Entry] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#entry_2","title":"Entry","text":"<p>None</p> Field Type Description Required Default address Integer None  Optional <code>None</code> data Integer None  Optional <code>None</code> position Integer None  Optional <code>None</code> size Integer None  Optional <code>None</code> type EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#resource","title":"Resource","text":"<p>None</p> Field Type Description Required Default parent_resource_ids EmptyableKeyword None  Optional <code>None</code> parent_labels List [EmptyableKeyword] None  Optional <code>None</code> characteristics Integer None  Optional <code>None</code> num_childs Integer None  Optional <code>None</code> depth Integer None  Optional <code>None</code> name EmptyableKeyword None  Optional <code>None</code> resource_id Integer None  Optional <code>None</code> resource_type EmptyableKeyword None  Optional <code>None</code> is_data Boolean None  Optional <code>None</code> is_directory Boolean None  Optional <code>None</code> major_version Integer None  Optional <code>None</code> minor_version Integer None  Optional <code>None</code> numberof_id_entries Integer None  Optional <code>None</code> numberof_name_entries Integer None  Optional <code>None</code> time_date_stamp Integer None  Optional <code>None</code> hr_time_date_stamp Date None  Optional <code>None</code> code_page Integer None  Optional <code>None</code> sha256 SHA256 None  Optional <code>None</code> entropy Float None  Optional <code>None</code> offset Integer None  Optional <code>None</code> reserved Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#resources_manager","title":"Resources_Manager","text":"<p>None</p> Field Type Description Required Default langs_available List [EmptyableKeyword] None  Optional <code>None</code> sublangs_available List [EmptyableKeyword] None  Optional <code>None</code> accelerators List [Accelerator] None  Optional <code>None</code> dialogs List [Dialog] None  Optional <code>None</code> html Text None  Optional <code>None</code> icons List [Icon] None  Optional <code>None</code> manifest Text None  Optional <code>None</code> string_table List [EmptyableKeyword] None  Optional <code>None</code> version Version None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#accelerator","title":"Accelerator","text":"<p>None</p> Field Type Description Required Default accelerator_id Integer None  Optional <code>None</code> padding Integer None  Optional <code>None</code> ansi EmptyableKeyword None  Optional <code>None</code> flags EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#dialog","title":"Dialog","text":"<p>None</p> Field Type Description Required Default charset Integer None  Optional <code>None</code> cx Integer None  Optional <code>None</code> cy Integer None  Optional <code>None</code> dialogbox_style_list List [EmptyableKeyword] None  Optional <code>None</code> extended_style EmptyableKeyword None  Optional <code>None</code> extended_style_list List [EmptyableKeyword] None  Optional <code>None</code> help_id Integer None  Optional <code>None</code> items List [Item] None  Optional <code>None</code> lang EmptyableKeyword None  Optional <code>None</code> point_size Integer None  Optional <code>None</code> signature Integer None  Optional <code>None</code> style EmptyableKeyword None  Optional <code>None</code> style_list List [EmptyableKeyword] None  Optional <code>None</code> sub_lang EmptyableKeyword None  Optional <code>None</code> title EmptyableKeyword None  Optional <code>None</code> typeface EmptyableKeyword None  Optional <code>None</code> version Integer None  Optional <code>None</code> weight Integer None  Optional <code>None</code> x Integer None  Optional <code>None</code> y Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#item","title":"Item","text":"<p>None</p> Field Type Description Required Default cx Integer None  Optional <code>None</code> cy Integer None  Optional <code>None</code> extended_style Integer None  Optional <code>None</code> help_id Integer None  Optional <code>None</code> item_id Integer None  Optional <code>None</code> is_extended Boolean None  Optional <code>None</code> style EmptyableKeyword None  Optional <code>None</code> title EmptyableKeyword None  Optional <code>None</code> x Integer None  Optional <code>None</code> y Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#icon","title":"Icon","text":"<p>None</p> Field Type Description Required Default icon_id Integer None  Optional <code>None</code> planes Integer None  Optional <code>None</code> height Integer None  Optional <code>None</code> width Integer None  Optional <code>None</code> lang EmptyableKeyword None  Optional <code>None</code> sublang EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#version","title":"Version","text":"<p>None</p> Field Type Description Required Default type Integer None  Optional <code>None</code> fixed_file_info Fixed_File_Info None  Optional <code>None</code> string_file_info String_File_Info None  Optional <code>None</code> var_file_info Var_File_Info None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#fixed_file_info","title":"Fixed_File_Info","text":"<p>None</p> Field Type Description Required Default file_date_ls Integer None  Optional <code>None</code> file_date_ms Integer None  Optional <code>None</code> file_flags Integer None  Optional <code>None</code> file_flags_mask Integer None  Optional <code>None</code> file_os EmptyableKeyword None  Optional <code>None</code> file_subtype EmptyableKeyword None  Optional <code>None</code> file_type EmptyableKeyword None  Optional <code>None</code> file_version_ls Integer None  Optional <code>None</code> file_version_ms Integer None  Optional <code>None</code> product_version_ls Integer None  Optional <code>None</code> product_version_ms Integer None  Optional <code>None</code> signature Integer None  Optional <code>None</code> struct_version Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#string_file_info","title":"String_File_Info","text":"<p>None</p> Field Type Description Required Default key EmptyableKeyword None  Optional <code>None</code> type Integer None  Optional <code>None</code> langcode_items List [LangCode_Item] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#langcode_item","title":"LangCode_Item","text":"<p>None</p> Field Type Description Required Default key EmptyableKeyword None  Optional <code>None</code> type Integer None  Optional <code>None</code> lang EmptyableKeyword None  Optional <code>None</code> sublang EmptyableKeyword None  Optional <code>None</code> code_page EmptyableKeyword None  Optional <code>None</code> items List [Item] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#item_1","title":"Item","text":"<p>None</p> Field Type Description Required Default key EmptyableKeyword None  Optional <code>None</code> value EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#var_file_info","title":"Var_File_Info","text":"<p>None</p> Field Type Description Required Default key EmptyableKeyword None  Optional <code>None</code> type Integer None  Optional <code>None</code> translations List [Integer] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#rich_header","title":"Rich_Header","text":"<p>None</p> Field Type Description Required Default key Integer None  Optional <code>None</code> hash EmptyableKeyword None  Optional <code>None</code> entries List [Entry] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#entry_3","title":"Entry","text":"<p>None</p> Field Type Description Required Default build_id Integer None  Optional <code>None</code> count Integer None  Optional <code>None</code> entry_id Integer None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#sections","title":"Sections","text":"<p>None</p> Field Type Description Required Default name EmptyableKeyword None  Optional <code>None</code> characteristics_hash Integer None  Optional <code>None</code> characteristics_list List [EmptyableKeyword] None  Optional <code>None</code> entropy Float None  Optional <code>None</code> entropy_without_padding Float None  Optional <code>None</code> md5 MD5 None  Optional <code>None</code> offset Integer None  Optional <code>None</code> size Integer None  Optional <code>None</code> sizeof_raw_data Integer None  Optional <code>None</code> virtual_address Integer None  Optional <code>None</code> virtual_size Integer None  Optional <code>None</code> fullname EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#signature","title":"Signature","text":"<p>None</p> Field Type Description Required Default version Integer None  Optional <code>None</code> algorithm EmptyableKeyword None  Optional <code>None</code> signers List [Signer] None  Optional <code>None</code> certificates List [Certificate] None  Optional <code>None</code> content_info Content_Info None  Optional <code>None</code> check EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#certificate","title":"Certificate","text":"<p>None</p> Field Type Description Required Default version Integer None  Optional <code>None</code> subject EmptyableKeyword None  Optional <code>None</code> issuer EmptyableKeyword None  Optional <code>None</code> serial_number EmptyableKeyword None  Optional <code>None</code> key_size Integer None  Optional <code>None</code> key_type EmptyableKeyword None  Optional <code>None</code> key_usage List [EmptyableKeyword] None  Optional <code>None</code> certificate_policies List [EmptyableKeyword] None  Optional <code>None</code> ext_key_usage List [EmptyableKeyword] None  Optional <code>None</code> valid_from Date None  Optional <code>None</code> valid_to Date None  Optional <code>None</code> signature EmptyableKeyword None  Optional <code>None</code> signature_algorithm EmptyableKeyword None  Optional <code>None</code> is_trusted EmptyableKeyword None  Optional <code>None</code> raw_hex EmptyableKeyword None  Optional <code>None</code> rsa_info RSA_Info None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#rsa_info","title":"RSA_Info","text":"<p>None</p> Field Type Description Required Default d_param EmptyableKeyword None  Optional <code>None</code> e_param EmptyableKeyword None  Optional <code>None</code> n_param EmptyableKeyword None  Optional <code>None</code> p_param EmptyableKeyword None  Optional <code>None</code> q_param EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#content_info","title":"Content_Info","text":"<p>None</p> Field Type Description Required Default algorithm EmptyableKeyword None  Optional <code>None</code> digest EmptyableKeyword None  Optional <code>None</code> content_type EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#signer","title":"Signer","text":"<p>None</p> Field Type Description Required Default version Integer None  Optional <code>None</code> issuer EmptyableKeyword None  Optional <code>None</code> serial_number EmptyableKeyword None  Optional <code>None</code> encryption_algorithm EmptyableKeyword None  Optional <code>None</code> digest_algorithm EmptyableKeyword None  Optional <code>None</code> encrypted_digest EmptyableKeyword None  Optional <code>None</code> cert Certificate None  Optional <code>None</code> authenticated_attributes List [EmptyableKeyword] None  Optional <code>None</code> unauthenticated_attributes List [EmptyableKeyword] None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#certificate_1","title":"Certificate","text":"<p>None</p> Field Type Description Required Default version Integer None  Optional <code>None</code> subject EmptyableKeyword None  Optional <code>None</code> issuer EmptyableKeyword None  Optional <code>None</code> serial_number EmptyableKeyword None  Optional <code>None</code> key_size Integer None  Optional <code>None</code> key_type EmptyableKeyword None  Optional <code>None</code> key_usage List [EmptyableKeyword] None  Optional <code>None</code> certificate_policies List [EmptyableKeyword] None  Optional <code>None</code> ext_key_usage List [EmptyableKeyword] None  Optional <code>None</code> valid_from Date None  Optional <code>None</code> valid_to Date None  Optional <code>None</code> signature EmptyableKeyword None  Optional <code>None</code> signature_algorithm EmptyableKeyword None  Optional <code>None</code> is_trusted EmptyableKeyword None  Optional <code>None</code> raw_hex EmptyableKeyword None  Optional <code>None</code> rsa_info RSA_Info None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#rsa_info_1","title":"RSA_Info","text":"<p>None</p> Field Type Description Required Default d_param EmptyableKeyword None  Optional <code>None</code> e_param EmptyableKeyword None  Optional <code>None</code> n_param EmptyableKeyword None  Optional <code>None</code> p_param EmptyableKeyword None  Optional <code>None</code> q_param EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/filetypes/pe/#tls","title":"TLS","text":"<p>None</p> Field Type Description Required Default section EmptyableKeyword None  Optional <code>None</code>"},{"location":"odm/models/ontology/results/antivirus/","title":"Antivirus","text":""},{"location":"odm/models/ontology/results/antivirus/#antivirus","title":"Antivirus","text":"<p>Antivirus Ontology Model</p> Field Type Description Required Default objectid ObjectID The object ID of the antivirus object  Yes <code>None</code> engine_name Keyword Name of antivirus engine  Yes <code>None</code> engine_version Keyword Version of antivirus engine  Optional <code>None</code> engine_definition_version Keyword Version of definition set  Optional <code>None</code> virus_name Keyword The name of the virus  Optional <code>None</code> category Enum What category does the verdict fall under?<ul><li><code>type-unsupported</code>: File sent to antivirus is unsupported</li><li><code>undetected</code>: File not detected by antivirus</li><li><code>failure</code>: Antivirus failed during detection</li><li><code>suspicious</code>: Antivirus deems suspicious</li><li><code>malicious</code>: Antivirus deems malicious</li></ul>Supported values are:<code>\"failure\", \"malicious\", \"suspicious\", \"type-unsupported\", \"undetected\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/http/","title":"HTTP","text":""},{"location":"odm/models/ontology/results/http/#http","title":"HTTP","text":"<p>HTTP Task</p> Field Type Description Required Default response_code Integer The status code of the main page  Yes <code>None</code> redirection_url Keyword The final page of the requested url  Optional <code>None</code> redirects List [HTTPRedirect] List of Redirects  Optional <code>None</code> favicon File The file information of the main favicon  Optional <code>None</code> title Keyword The title of the main page after any redirection  Optional <code>None</code>"},{"location":"odm/models/ontology/results/http/#httpredirect","title":"HTTPRedirect","text":"Field Type Description Required Default from_url Keyword  Yes <code>None</code> to_url Keyword  Yes <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/","title":"MalwareConfig","text":""},{"location":"odm/models/ontology/results/malware_config/#malwareconfig","title":"MalwareConfig","text":"<p>Extracted Malware Configuration</p> Field Type Description Required Default config_extractor Keyword Name of extractor  Yes <code>None</code> family List [Text] What family is this associated to?  Yes <code>None</code> version Text Version of the malware  Optional <code>None</code> category List [Enum] Category of malware  Optional <code>None</code> attack List [Enum] ATT&amp;CK ID associated  Optional <code>None</code> capability_enabled List [Text] Enabled Capabilities  Optional <code>None</code> capability_disabled List [Text] Disabled Capabilities  Optional <code>None</code> campaign_id List [Text] Campaign ID  Optional <code>None</code> identifier List [Text] Identifier  Optional <code>None</code> decoded_strings List [Text] Decoded Strings  Optional <code>None</code> password List [Text] Passwords  Optional <code>None</code> mutex List [Text] Mutex  Optional <code>None</code> pipe List [Text] Pipe  Optional <code>None</code> ipc List [IPC] IPC (similar to 'pipe' field but more detailed)  Optional <code>None</code> sleep_delay Integer Sleep Delay  Optional <code>None</code> sleep_delay_jitter Integer Sleep Delay Jitter  Optional <code>None</code> inject_exe List [Text] Injected EXE  Optional <code>None</code> binaries List [Binary] Binaries  Optional <code>None</code> ftp List [FTP] FTPs  Optional <code>None</code> smtp List [SMTP] SMTPs  Optional <code>None</code> http List [HTTP] HTTPs  Optional <code>None</code> ssh List [SSH] SSHs  Optional <code>None</code> proxy List [Proxy] Proxies  Optional <code>None</code> dns List [DNS] DNS  Optional <code>None</code> tcp List [GeneralConnection] TCPs  Optional <code>None</code> udp List [GeneralConnection] UDPs  Optional <code>None</code> encryption List [Encryption] Encryptions  Optional <code>None</code> service List [Service] Services  Optional <code>None</code> cryptocurrency List [Cryptocurrency] Cryptocurrencies  Optional <code>None</code> paths List [Path] Paths  Optional <code>None</code> registry List [Registry] Registry  Optional <code>None</code> other Mapping [String, Any] Other information  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#binary","title":"Binary","text":"<p>Binary data extracted by decoder</p> Field Type Description Required Default datatype Enum NoneSupported values are:<code>\"config\", \"other\", \"payload\"</code>  Optional <code>None</code> data Text None  Yes <code>None</code> other Mapping [String, Any] Other information  Optional <code>None</code> encryption List [Encryption] None  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#encryption","title":"Encryption","text":"<p>Encryption details</p> Field Type Description Required Default algorithm Text Algorithm  Optional <code>None</code> public_key Text Public Key  Optional <code>None</code> key Text Key  Optional <code>None</code> provider Text Provider  Optional <code>None</code> mode Text Mode  Optional <code>None</code> iv Text Initialization Vector  Optional <code>None</code> seed Text Seed  Optional <code>None</code> nonce Text Nonce value  Optional <code>None</code> constants List [Text] Constants  Optional <code>None</code> usage Enum Purpose of encryptionsSupported values are:<code>\"binary\", \"communication\", \"config\", \"other\", \"ransom\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#cryptocurrency","title":"Cryptocurrency","text":"<p>Cryptocoin usage (ransomware/miner)</p> Field Type Description Required Default coin Text Name of coin used  Optional <code>None</code> address Text Wallet address  Optional <code>None</code> random_amount Integer Ransom amount  Optional <code>None</code> usage Enum Use of cryptocurrencySupported values are:<code>\"miner\", \"other\", \"ransomware\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#dns","title":"DNS","text":"<p>Usage of DNS connection</p> Field Type Description Required Default ip IP IP of DNS server  Optional <code>None</code> port Integer Port of DNS server  Optional <code>None</code> hostname Text Hostname used in query  Optional <code>None</code> record_type Enum Type of DNS recordSupported values are:<code>\"A\", \"AAAA\", \"AFSDB\", \"APL\", \"CAA\", \"CDNSKEY\", \"CDS\", \"CERT\", \"CNAME\", \"CSYNC\", \"DHCID\", \"DLV\", \"DNAME\", \"DNSKEY\", \"DS\", \"EUI48\", \"EUI64\", \"HINFO\", \"HIP\", \"HTTPS\", \"IPSECKEY\", \"KEY\", \"KX\", \"LOC\", \"MX\", \"NAPTR\", \"NS\", \"NSEC\", \"NSEC3\", \"NSEC3PARAM\", \"OPENPGPKEY\", \"PTR\", \"RP\", \"RRSIG\", \"SIG\", \"SMIMEA\", \"SOA\", \"SRV\", \"SSHFP\", \"SVCB\", \"TA\", \"TKEY\", \"TLSA\", \"TSIG\", \"TXT\", \"URI\", \"ZONEMD\"</code>  Optional <code>None</code> usage Enum Purpose of DNS connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#encryption_1","title":"Encryption","text":"<p>Encryption details</p> Field Type Description Required Default algorithm Text Algorithm  Optional <code>None</code> public_key Text Public Key  Optional <code>None</code> key Text Key  Optional <code>None</code> provider Text Provider  Optional <code>None</code> mode Text Mode  Optional <code>None</code> iv Text Initialization Vector  Optional <code>None</code> seed Text Seed  Optional <code>None</code> nonce Text Nonce value  Optional <code>None</code> constants List [Text] Constants  Optional <code>None</code> usage Enum Purpose of encryptionsSupported values are:<code>\"binary\", \"communication\", \"config\", \"other\", \"ransom\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#ftp","title":"FTP","text":"<p>Usage of FTP connection</p> Field Type Description Required Default username Text Username  Optional <code>None</code> password Text Password  Optional <code>None</code> hostname Text FTP Host  Optional <code>None</code> port Integer FTP Port  Optional <code>None</code> path Text FTP Path  Optional <code>None</code> usage Enum Purpose of FTP connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#generalconnection","title":"GeneralConnection","text":"<p>Usage of General TCP/UDP connection</p> Field Type Description Required Default client_ip IP Client IP  Optional <code>None</code> client_port Integer Client Port  Optional <code>None</code> server_ip IP Server IP  Optional <code>None</code> server_domain Domain Server Domain  Optional <code>None</code> server_port Integer Server Port  Optional <code>None</code> usage Enum Purpose of TCP/UDP connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#http","title":"HTTP","text":"<p>Usage of HTTP connection</p> Field Type Description Required Default uri URI URI  Optional <code>None</code> protocol Enum ProtocolSupported values are:<code>\"http\", \"https\"</code>  Optional <code>None</code> username Text Username  Optional <code>None</code> password Text Password  Optional <code>None</code> hostname Text HTTP server  Optional <code>None</code> port Integer HTTP Port  Optional <code>None</code> path URIPath URI Path  Optional <code>None</code> query Text Query parameters  Optional <code>None</code> fragment Text Fragment  Optional <code>None</code> user_agent Text User Agent  Optional <code>None</code> method Enum MethodSupported values are:<code>\"BCOPY\", \"BDELETE\", \"BMOVE\", \"BPROPFIND\", \"BPROPPATCH\", \"CONNECT\", \"COPY\", \"DELETE\", \"GET\", \"HEAD\", \"LOCK\", \"MKCOL\", \"MOVE\", \"NOTIFY\", \"OPTIONS\", \"PATCH\", \"POLL\", \"POST\", \"PROPFIND\", \"PROPPATCH\", \"PUT\", \"SEARCH\", \"SUBSCRIBE\", \"TRACE\", \"UNLOCK\", \"UNSUBSCRIBE\", \"X-MS-ENUMATTS\"</code>  Optional <code>None</code> headers Mapping [String, Text] HTTP Headers  Optional <code>None</code> max_size Integer Maximum size  Optional <code>None</code> usage Enum Purpose of HTTP connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#ipc","title":"IPC","text":"<p>Inter-Process Communications</p> Field Type Description Required Default file List [Text] A record stored on disk, or a record synthesized on demand by a file server, which can be accessed by multiple processes.  Optional <code>None</code> socket List [Text] Data sent over a network interface, either to a different process on the same computer or to another computer on the network. Stream oriented (TCP; data written through a socket requires formatting to preserve message boundaries) or more rarely message-oriented (UDP, SCTP).  Optional <code>None</code> unix_domain_socket List [Text] Similar to an internet socket, but all communication occurs within the kernel. Domain sockets use the file system as their address space. Processes reference a domain socket as an inode, and multiple processes can communicate with one socket.  Optional <code>None</code> memory_mapped_file List [Text] A file mapped to RAM and can be modified by changing memoryaddresses directly instead of outputting to a stream. This shares the same benefits as a standard file.  Optional <code>None</code> message_queue List [Text] A data stream similar to a socket, but which usually preserves message boundaries. Typically implemented by the operating system, they allow multiple processes to read and write to the message queue without being directly connected to each other.  Optional <code>None</code> anonymous_pipe List [Text] A unidirectional data channel using standard input and output. Data written to the write-end of the pipe is buffered by the operating system until it is read from the read-end of the pipe. Two-way communication between processes can be achieved by using two pipes in opposite \"directions\".  Optional <code>None</code> named_pipe List [Text] A pipe that is treated like a file. Instead of using standard input and output as with an anonymous pipe, processes write to and read from a named pipe, as if it were a regular file.  Optional <code>None</code> process_names List [Text] The process names involved in the IPC communication  Optional <code>None</code> shared_memory Text Multiple processes are given access to the same block of memory, which creates a shared buffer for the processes to communicate with each other.  Optional <code>None</code> usage Enum Purpose of connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#path","title":"Path","text":"<p>File Paths</p> Field Type Description Required Default path Text Path  Optional <code>None</code> usage Enum Use of pathSupported values are:<code>\"c2\", \"config\", \"install\", \"logs\", \"other\", \"plugins\", \"storage\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#proxy","title":"Proxy","text":"<p>Usage of Proxy connection</p> Field Type Description Required Default username Text Username  Optional <code>None</code> password Text Password  Optional <code>None</code> hostname Text Proxy Host  Optional <code>None</code> port Integer Proxy Port  Optional <code>None</code> usage Enum Purpose of proxy connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code> protocol Text Protocol used  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#registry","title":"Registry","text":"<p>Registry</p> Field Type Description Required Default key Text Registry key  Yes <code>None</code> value Text Registry  Optional <code>None</code> usage Enum Use of registry keySupported values are:<code>\"other\", \"persistence\", \"read\", \"store_data\", \"store_payload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#smtp","title":"SMTP","text":"<p>Usage of SMTP connection</p> Field Type Description Required Default username Text Username  Optional <code>None</code> password Text Password  Optional <code>None</code> hostname Text SMTP Host  Optional <code>None</code> port Integer SMTP Port  Optional <code>None</code> mail_to List [Text] Sent to  Optional <code>None</code> mail_from Text Sent from  Optional <code>None</code> subject Text Subject  Optional <code>None</code> usage Enum Purpose of SMTP connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#ssh","title":"SSH","text":"<p>Usage of SSH connection</p> Field Type Description Required Default username Text Username  Optional <code>None</code> password Text Password  Optional <code>None</code> public_key Text SSH Public Key  Optional <code>None</code> hostname Text SSH Host  Optional <code>None</code> port Integer SSH Port  Optional <code>None</code> usage Enum Purpose of SSH connectionSupported values are:<code>\"c2\", \"decoy\", \"download\", \"other\", \"propagate\", \"ransom\", \"tunnel\", \"upload\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/malware_config/#service","title":"Service","text":"<p>Operating System services affected</p> Field Type Description Required Default dll Text DLL associated to service  Optional <code>None</code> name Text Name of service  Optional <code>None</code> display_name Text Display Name of service  Optional <code>None</code> description Text Service Description  Optional <code>None</code>"},{"location":"odm/models/ontology/results/network/","title":"NetworkConnection","text":""},{"location":"odm/models/ontology/results/network/#networkconnection","title":"NetworkConnection","text":"<p>Details for a low-level network connection by IP</p> Field Type Description Required Default objectid ObjectID The object ID of the network object  Yes <code>None</code> destination_ip IP The destination IP of the connection  Optional <code>None</code> destination_port Integer The destination port of the connection  Optional <code>None</code> transport_layer_protocol Enum The transport layer protocol of the connectionSupported values are:<code>\"tcp\", \"udp\"</code>  Optional <code>None</code> direction Enum The direction of the network connectionSupported values are:<code>\"inbound\", \"outbound\", \"unknown\"</code>  Optional <code>None</code> process Process The process that spawned the network connection  Optional <code>None</code> source_ip IP The source IP of the connection  Optional <code>None</code> source_port Integer The source port of the connection  Optional <code>None</code> http_details NetworkHTTP HTTP-specific details of request  Optional <code>None</code> dns_details NetworkDNS DNS-specific details of request  Optional <code>None</code> smtp_details NetworkSMTP SMTP-specific details of request  Optional <code>None</code> connection_type Enum NoneSupported values are:<code>\"dns\", \"http\", \"smtp\", \"tls\"</code>  Optional <code>None</code>"},{"location":"odm/models/ontology/results/network/#networkdns","title":"NetworkDNS","text":"<p>Details for a DNS request</p> Field Type Description Required Default domain Domain The domain requested  Yes <code>None</code> resolved_ips List [IP] A list of IPs that were resolved  Optional <code>None</code> resolved_domains List [Domain] A list of domains that were resolved  Optional <code>None</code> lookup_type Enum The type of DNS requestSupported values are:<code>\"A\", \"AAAA\", \"AFSDB\", \"APL\", \"CAA\", \"CDNSKEY\", \"CDS\", \"CERT\", \"CNAME\", \"CSYNC\", \"DHCID\", \"DLV\", \"DNAME\", \"DNSKEY\", \"DS\", \"EUI48\", \"EUI64\", \"HINFO\", \"HIP\", \"HTTPS\", \"IPSECKEY\", \"KEY\", \"KX\", \"LOC\", \"MX\", \"NAPTR\", \"NS\", \"NSEC\", \"NSEC3\", \"NSEC3PARAM\", \"OPENPGPKEY\", \"PTR\", \"RP\", \"RRSIG\", \"SIG\", \"SMIMEA\", \"SOA\", \"SRV\", \"SSHFP\", \"SVCB\", \"TA\", \"TKEY\", \"TLSA\", \"TSIG\", \"TXT\", \"URI\", \"ZONEMD\"</code>  Yes <code>None</code>"},{"location":"odm/models/ontology/results/network/#networkhttp","title":"NetworkHTTP","text":"<p>Details for an HTTP request</p> Field Type Description Required Default request_uri URI The URI requested  Yes <code>None</code> request_headers Mapping [String, Json] Headers included in the request  Yes <code>None</code> request_method Enum The method of the requestSupported values are:<code>\"BCOPY\", \"BDELETE\", \"BMOVE\", \"BPROPFIND\", \"BPROPPATCH\", \"CONNECT\", \"COPY\", \"DELETE\", \"GET\", \"HEAD\", \"LOCK\", \"MKCOL\", \"MOVE\", \"NOTIFY\", \"OPTIONS\", \"PATCH\", \"POLL\", \"POST\", \"PROPFIND\", \"PROPPATCH\", \"PUT\", \"SEARCH\", \"SUBSCRIBE\", \"TRACE\", \"UNLOCK\", \"UNSUBSCRIBE\", \"X-MS-ENUMATTS\"</code>  Yes <code>None</code> response_headers Mapping [String, Json] Headers included in the response  Yes <code>None</code> request_body Text The body of the request  Optional <code>None</code> response_status_code Integer The status code of the response  Optional <code>None</code> response_body Text The body of the response  Optional <code>None</code> response_content_fileinfo File The file information of the response content  Optional <code>None</code> response_content_mimetype Text The response content mimetype returned by the server  Optional <code>None</code>"},{"location":"odm/models/ontology/results/network/#networksmtp","title":"NetworkSMTP","text":"<p>Details for an SMTP request</p> Field Type Description Required Default mail_from Email Sender of the email  Yes <code>None</code> mail_to List [Email] Recipients of the email  Yes <code>None</code> attachments List [File] The file information about the attachments  Optional <code>None</code>"},{"location":"odm/models/ontology/results/process/","title":"Process","text":""},{"location":"odm/models/ontology/results/process/#process","title":"Process","text":"<p>Details about a process</p> Field Type Description Required Default objectid ObjectID The object ID of the process object  Yes <code>None</code> image Text The image of the process  Yes <code>&lt;unknown_image&gt;</code> start_time Date The time of creation for the process  Yes <code>None</code> pobjectid ObjectID The object ID of the parent process object  Optional <code>None</code> pimage Text The image of the parent process that spawned this process  Optional <code>None</code> pcommand_line Text The command line that the parent process ran  Optional <code>None</code> ppid Integer The process ID of the parent process  Optional <code>None</code> pid Integer The process ID  Optional <code>None</code> command_line Text The command line that the process ran  Optional <code>None</code> end_time Date The time of termination for the process  Optional <code>None</code> integrity_level Text The integrity level of the process  Optional <code>None</code> image_hash Text The hash of the file run  Optional <code>None</code> original_file_name Text The original name of the file  Optional <code>None</code>"},{"location":"odm/models/ontology/results/process/#objectid","title":"ObjectID","text":"<p>Details about the characteristics used to identify an object</p> Field Type Description Required Default tag Text The normalized tag of the object  Yes <code>None</code> ontology_id Keyword Deterministic identifier of ontology. This value should be able to be replicable between services that have access to similar object details, such that it can be used for relating objects in post-processing.  Yes <code>None</code> service_name Keyword Component that generated this section  Yes <code>unknown</code> guid Text The GUID associated with the object  Optional <code>None</code> treeid Text The hash of the tree ID  Optional <code>None</code> processtree Keyword Human-readable tree ID (concatenation of tags)  Optional <code>None</code> time_observed Date The time at which the object was observed  Optional <code>None</code> session Keyword Unifying session name/ID  Optional <code>None</code>"},{"location":"odm/models/ontology/results/sandbox/","title":"Sandbox","text":""},{"location":"odm/models/ontology/results/sandbox/#sandbox","title":"Sandbox","text":"<p>Sandbox Ontology Model</p> Field Type Description Required Default objectid ObjectID The object ID of the sandbox object  Yes <code>None</code> analysis_metadata AnalysisMetadata Metadata for the analysis  Yes <code>None</code> sandbox_name Keyword The name of the sandbox  Yes <code>None</code> sandbox_version Keyword The version of the sandbox  Optional <code>None</code>"},{"location":"odm/models/ontology/results/sandbox/#analysismetadata","title":"AnalysisMetadata","text":"<p>The metadata of the analysis, per analysis</p> Field Type Description Required Default task_id Keyword The ID used for identifying the analysis task  Optional <code>None</code> start_time Date The start time of the analysis  Yes <code>None</code> end_time Date The end time of the analysis  Optional <code>None</code> routing Keyword The routing used in the sandbox setup (Spoofed, Internet, Tor, VPN)  Optional <code>None</code> machine_metadata MachineMetadata The metadata of the analysis  Optional <code>None</code> window_size Keyword The resolution used for the analysis  Optional <code>None</code>"},{"location":"odm/models/ontology/results/sandbox/#machinemetadata","title":"MachineMetadata","text":"<p>The metadata regarding the machine where the analysis took place</p> Field Type Description Required Default ip IP The IP of the machine used for analysis  Optional <code>None</code> hypervisor Keyword The hypervisor of the machine used for analysis  Optional <code>None</code> hostname Keyword The name of the machine used for analysis  Optional <code>None</code> platform Platform The platform of the machine used for analysis  Optional <code>None</code> version Keyword The version of the operating system of the machine used for analysis  Optional <code>None</code> architecture Processor The architecture of the machine used for analysis  Optional <code>None</code>"},{"location":"odm/models/ontology/results/signature/","title":"Signature","text":""},{"location":"odm/models/ontology/results/signature/#signature","title":"Signature","text":"<p>A signature that was raised during the analysis of the task</p> Field Type Description Required Default objectid ObjectID The object ID of the signature object  Yes <code>None</code> name Keyword The name of the signature  Yes <code>None</code> type Enum Type of signatureSupported values are:<code>\"CUCKOO\", \"SIGMA\", \"SURICATA\", \"YARA\"</code>  Yes <code>None</code> classification ClassificationString Classification of signature  Yes <code>None</code> attributes List [Attribute] Attributes about the signature  Optional <code>None</code> attacks List [Attack] A list of ATT&amp;CK patterns and categories of the signature  Optional <code>None</code> actors List [Text] List of actors of the signature  Optional <code>None</code> malware_families List [Text] List of malware families of the signature  Optional <code>None</code> signature_id Text ID of signature  Optional <code>None</code>"},{"location":"odm/models/ontology/results/signature/#attribute","title":"Attribute","text":"<p>Attribute relating to the signature that was raised during the analysis of the task</p> Field Type Description Required Default source ObjectID Object that the rule triggered on  Yes <code>None</code> target ObjectID Object targetted by source object  Optional <code>None</code> action Enum NoneSupported values are:<code>\"clipboard_capture\", \"create_remote_thread\", \"create_stream_hash\", \"dns_query\", \"driver_loaded\", \"file_change\", \"file_creation\", \"file_delete\", \"image_loaded\", \"network_connection\", \"network_connection_linux\", \"pipe_created\", \"process_access\", \"process_creation\", \"process_creation_linux\", \"process_tampering\", \"process_terminated\", \"raw_access_thread\", \"registry_add\", \"registry_delete\", \"registry_event\", \"registry_rename\", \"registry_set\", \"sysmon_error\", \"sysmon_status\", \"wmi_event\"</code>  Optional <code>None</code> meta Text Metadata about the detection  Optional <code>None</code> event_record_id Text Event Record ID (Event Logs)  Optional <code>None</code> domain Domain Domain  Optional <code>None</code> uri URI URI  Optional <code>None</code> file_hash SHA256 SHA256 of file  Optional <code>None</code>"},{"location":"overview/community_services/","title":"Community services","text":""},{"location":"overview/community_services/#community-services","title":"Community services","text":"<p>The Assemblyline community has been hard at work to improve Assemblyline's ability to detect malicious files and extract information about them.</p> <p>This page lists all the services that our members have created and shared with the public.</p> <p>Warning</p> <p>These services are not managed by the Assemblyline team so make sure that you check their source thoroughly and that you are comfortable with what they do before you install them on your system.</p>"},{"location":"overview/community_services/#service-list","title":"Service list","text":"Service Name Description Author AutoItRipper AutoIt unpacker service NVISO ClamAV Assemblyline service which submits a file to ClamAV and displays the result NVISO HybridAnalysis Uses the Hybrid Analysis service to provide additional threat intelligence and malware analysis capabilities boredchilada MalwareBazaar Assemblyline service fetching Malware Bazaar report NVISO MsgParser Simple MSG extractor AssemblyLine service NVISO MetaDefender Sandbox Submits a file or a URL to MetaDefender Sandbox OPSWAT PythonExeUnpack Python exe unpacker service NVISO ReversingLabsSpectraIntelligence This service uses ReversingLabs' Spectra Intelligence service for obtaining detailed and high precision file reputation and analysis information on submitted files. ReversingLabs StegFinder AssemblyLine service which scans for embedded data in image using StegExpose NVISO Unfurl Assemblyline service parsing a submitted URL to unshorten it. NVISO UrlScanIo This service fetches the results of a urlscan.io scan. NVISO WindowsDefender Windows defender service being adapted from an Assemblyline community conversation Adam McHugh"},{"location":"overview/community_services/#building-a-community-service","title":"Building a Community Service","text":"<ol> <li>Obtain the service source code</li> <li> <p>Edit the service manifest and ensure the following is set</p> <pre><code>version: $SERVICE_TAG\n...\ndocker_config:\n  image: ${REGISTRY}&lt;service_container_image&gt;:$SERVICE_TAG\n</code></pre> </li> <li> <p>Build image and push to your local registry:</p> Warning <p>It's strongly recommended to tag service images following the Assemblyline format. Otherwise, the system will disable your service because it will deem it incompatible with the rest of the components.</p> <p> Service versions should follow the format <code>A.B.C.(dev|stable).D</code>, where:</p> <ul> <li><code>A, B</code> represents the framework and system version, respectively.</li> <li><code>C, D</code> can be used to indicate the major and minor of a service, respectively.</li> <li>The <code>dev</code> or <code>stable</code> portion of the tag should indicate the state of the service build. This is also relevant for providing service updates under a certain channel.</li> </ul> <p>The following is an example of a service build targetted for an Assemblyline deployment running release 4.5.x.x:</p> <pre><code>docker build . -t &lt;private_registry&gt;/&lt;service_container_image&gt;:4.5.0.stable0 --build-arg version=4.5.0.stable0\ndocker push &lt;private_registry&gt;/&lt;service_container_image&gt;:4.5.0.stable0\n</code></pre> </li> <li> <p>Add contents of service manifest to UI or using the REST API to add the service to Assemblyline</p> </li> </ol>"},{"location":"overview/community_services/#add-your-service","title":"Add your service","text":"<p>Contact us on Discord to get your service featured on this page.</p>"},{"location":"overview/how_it_works/","title":"How it works","text":""},{"location":"overview/how_it_works/#how-it-works","title":"How it works","text":"<p>Assemblyline minimizes the number of harmless files that IT practitioners are required to inspect every day, allowing them to collaborate with other users to customize and improve the platform in the process.</p> <p></p> <p>A) Assemblyline works very much like a conveyor belt: files arrive in the system and are triaged in a certain sequence.</p> <p>B) Assemblyline generates information about each file and assigns a unique identifier that travels with the file as it flows through the system.</p> <p>C) Users can add their own analytics, which we refer to as services, to Assemblyline.</p> <p>D) The services selected by the user in Assemblyline then analyze the files, looking for indications of maliciousness and/or extracting features for further analysis.</p> <ul> <li>The system generates alerts about a malicious file at any point during the analysis and assigns the file a score.</li> <li>The system can also trigger automated defensive systems.</li> <li>Malicious indicators generated by the system can be distributed to other defense systems.</li> </ul>"},{"location":"overview/services/","title":"Assemblyline services","text":""},{"location":"overview/services/#assemblyline-services","title":"Assemblyline services","text":"<p>Services currently installed on a system can be found under <code>Help &gt; Service Listing</code>.</p> <p>This is the list of all the services that are bundled with Assemblyline and that are maintained by the Assemblyline team:</p> Service Name Speciality Description APIVector Windows binaries Extracts library imports from windows PE files or memory dump to generate api vector classification. APKaye Android APK APKs are decompiled and inspected. Network indicators and information found in the APK manifest file are displayed AntiVirus Anti-virus Generic ICAP client to integrate with most Anti-virus enterprise scanners Batchdeobfuscator Deobfuscation Deobfuscate batch file through variable resolution CAPA Windows binaries CAPA open-source tool integration Characterize Entropy analysis Partitions the file and calculates visual entropy for each partition, extract Exif metadata ConfigExtractor IoC extraction Extract malware configuration file, allowing to get list of C2, encryption material etc. CAPE Sandbox Provides dynamic malware analysis through sandboxing. DeobfuScripter Deobfuscation Static script de-obfuscator. The purpose is not to get surgical de-obfuscation, but rather to extract obfuscated IOCs. ELF Linux binaries Extracts attributes (sections, segments, ...) from ELF files using LIEF ELFPARSER Linux binaries ELFParser open-source tool integration EmlParser Email Parse emails using GOVCERT-LU eml_parser library while extracting header information, attachments, URIs Espresso Java All classes are extracted, decompiled, and analyzed for malicious behaviour Extract Compressed file This service extracts embedded files from file containers (like ZIP, RAR, 7z, ...) Floss IoC extraction Automatically extract obfuscated strings from malware using FireEye Labs Obfuscated String Solver FrankenStrings IoC extraction This service performs file and IOC extractions using pattern matching, simple encoding decoder and script de-obfuscators Intezer File genome identification Interface between Intezer Analyze API 2.0, submits file for analysis if hash is not present in Intezer database IPArse Apple IOS Analyze Apple apps JsJaws Javascript Analyze malicious Javascript MetaPeek Meta data analysis Checks submission metadata for indicators of potential malicious behaviour (double file extensions, ...) Oletools Office documents This service extracts metadata, network information and reports anomalies in Microsoft OLE and XML documents using the Python library py-oletools by Philippe Lagadec Overpower PowerShell De-obfuscate PowerShell scripts PDFId PDF This service extracts metadata from PDFs using Didier Stevens PDFId &amp; PDFParse PE Windows binaries Extract attributes (imports, exports, sections, ...) from PE files using LIEF PeePDF PDF This service uses the Python PeePDF library information from PDFs including JavaScript blocks which it will attempt to de-obfuscate, if necessary, for further analysis PixAxe Images Extract text from images Safelist Safelisting Allow for hash, IoC and signature safelisting, including support for downloading NSRL Sigma Eventlog signatures Scan event logs (e.g. from sandbox or a compromised host) using Sigma Suricata Network signatures Scan network capture (.pcap) submitted and extracted from analysis via Suricata Swiffer Adobe Shockwave This service extracts metadata and performs anomaly detection on Adobe Shockwave (.swf) files TagCheck Tag signatures YARA signatures on Assemblyline Tags (build your own signatures to hit on specific tags) TorrentSlicer Torrent files Extracts information from torrent files Unpacker UPX Unpacker This service unpacks UPX packed executables for further analysis Unpac.me Unpacker Integrate with unpac.me URLCreator URL File creation Create URI files from URLs tags that are seemingly malicious URLDownloader URL Fetching Fetches URLs from URI files ViperMonkey Office documents ViperMonkey is a VBA Emulation engine by https://linktr.ee/decalage VirusTotal Anti-virus This service checks (and optionally submits) files/URLs to VirusTotal for analysis. XLMMacroDeobfuscator Office documents Analyze Excel 4.0 macros YARA File signatures Signature for file <p>End of life, no longer actively supported:</p> Service Name Speciality Description Cuckoo Sandbox Provides dynamic malware analysis through sandboxing. IntezerStatic File genome identification Interface between Intezer Analyze API 2.0, performs hash lookups of submitted file MetaDefender Anti-virus Service for OPSWAT MetaDefender anti-virus (multi-engine) PEFile Windows binaries This service extracts attributes (imports, exports, section names, ...) from windows PE files using the Python library pefile VirusTotalDynamic Anti-virus Checks and actively sends files to VirusTotal for analysis. VirusTotalStatic Anti-virus Checks VirusTotal for existing analysis about submitted file."},{"location":"user_manual/llm/","title":"OpenAI Integration","text":""},{"location":"user_manual/llm/#openai-integration","title":"OpenAI Integration","text":"<p>Assemblyline now supports LLM integration using the OpenAI Chat Completions API.</p>"},{"location":"user_manual/llm/#current-features-of-the-ai-integration","title":"Current Features of the AI Integration","text":"<p>In its current state, it will allow Assemblyline users to have the LLM perform the following:</p> <ul> <li>Create an hybrid report where the summary of the findings and detailed analysis is generated by the LLM and the associated IOCs and extracted files are statically extracted from the API</li> <li>Create an executive summary of the detailed analysis of a submission or a file</li> <li>Analyze code and provide a summary and detailed analysis of what that code does</li> </ul>"},{"location":"user_manual/llm/#hybrid-reports","title":"Hybrid reports","text":"<p>Now as part of the report view of Assemblyline, a button was added at the top that lets you use the LLM-generated report.</p> <p></p> <p>By clicking the button, Assemblyline will bundle the Assemblyline report in a compact format and ask the LLM to create an executive summary followed by a detailed analysis of the submission. At the end of the detailed analysis, you will find a \"Powered by AI\" watermark that indicates that this section was generated by the LLM.</p> <p></p>"},{"location":"user_manual/llm/#executive-summaries","title":"Executive Summaries","text":"<p>While viewing either the details of a submission or the details of a file, when the AI Integration is turned on, the system will automatically ask the LLM to generate executive summaries of the submission or file that you are currently viewing.</p> <p>These executive summaries will be in their own section and will be followed by the same \"Powered by AI\" watermark found in the report so that you can easily spot that this data is not generated by Assemblyline.</p> <p></p>"},{"location":"user_manual/llm/#code-analysis","title":"Code Analysis","text":"<p>While in the Assemblyline file viewer, if you are viewing a file of type <code>code/*</code>, Assembyline will present you with a button to analyze the code and, when clicked, the full code will be sent to the LLM which will generate a summary of what the code does as well as more detail on specific parts of the code.</p> <p>The code analysis section will also be followed by the same \"Powered by AI\" watermark as the executive summary and the hybrid report.</p> <p></p>"},{"location":"user_manual/llm/#implementation","title":"Implementation","text":""},{"location":"user_manual/llm/#special-report-format","title":"Special report format","text":"<p>To get the best result possible out of the LLM, a special version of the Assemblyline report was created where all of the fields that are not very important to the LLM are stripped out. The Object Data Model (ODM) used by Assemblyline now has an <code>ai</code> switch for all of its fields that when set to <code>False</code> will omit that field for the special LLM output.</p> <p>This special format will also generate the Assemblyline report as <code>YAML</code> instead of <code>JSON</code> because this creates less tokens for the LLM to parse which allow us to analyze larger Assemblyline outputs.</p>"},{"location":"user_manual/llm/#caching-of-output","title":"Caching of output","text":"<p>To optimally reduce the cost of operation of the LLM integration, Assemblyline caches the result of the LLM both at the Assemblyline API level and at the user interface, this way if someone has asked for the same executive summary, hybrid report or code analysis that you are requesting, the response will be instant as you will get the cached version.</p> <p>If you are unhappy with the result you have got from the LLM you can always click the \"Powered by AI\" watermark found at the button of each AI-generated sections which will let you regenerate the LLM output by skipping the cache.</p>"},{"location":"user_manual/llm/#limitations","title":"Limitations","text":"<p>There are a few limitations to understand about the current implementation of the AI integration:</p> <ol> <li>The output generated by the LLM is absolutely unpredictable, you will get a different result each time you ask for new output. The caching layer will help remove a little bit of randomness but only for the time that the output remains in the cache.</li> <li>Some Assemblyline output and some code files are just too big to be processed by the LLM and we have no report/code-splitting in place as of this writing to be able to process these in multiple chunks. This means that you may get token errors from time-to-time saying that your message exceeded the model's maximum context length.   </li> <li>The speed at which the LLM responds to the your query depends on which model you use and how big the data you are sending to the model. By default, AL uses the <code>GPT3.5-turbo</code> model as this is the one that was responding the fastest for us and was giving results that are good enough for what we were expecting.</li> <li>The quality of the response you will get from the LLM is directly linked with the quality of the prompt that you sent to the model and which model you use. You can edit the prompts sent to the model in your <code>config.yml</code> file as well as which model you want to use if you are not happy with the results. We feel like what we have as default is good enough for now but there is always room for improvements.</li> </ol>"},{"location":"user_manual/llm/#enabling-and-configuring-the-ai-integration","title":"Enabling and configuring the AI Integration","text":"<p>The AI integration can easily be turned on by editing the AI configuration section in your <code>config.yml</code> file to enable the integration and set your API Key.</p> Minimal AI configuration block <pre><code>...\n\nui:\n  ai_backends:\n    enabled: True\n    api_connections:\n      - headers:\n          Authorization: Bearer &lt;OPENAI_APIKEY&gt;\n\n...\n</code></pre> <p>The <code>ai</code> configuration block will also let you specify the other parameters sent to the OpenAI API. Here is an example of what the configuration of the AI integration looks like:</p> Full AI configuration block <pre><code>...\n\nui:\n  ai_backends:\n    # Enabled/disabled AI integration\n    enabled: true\n\n    # List of API definitions use in the API Pool. Multiple connections can be specified in list order (e.g. main connection, fallback connection)\n    api_connections:\n\n      # URL to the chat completion OpenAI API, you can change this to your own if you are not using the default OpenAI endpoints.\n      - chat_url: \"https://api.openai.com/v1/chat/completions\"\n        # Type of chat API we are communicating with (e.g. openai, cohere)\n        api_type: \"openai\"\n        # Headers sent to the API\n        headers:\n          Authorization: Bearer &lt;OPENAI_APIKEY&gt;\n          Content-Type: \"application/json\"\n        # Model used for the AI Integration\n        model_name: \"gpt-3.5-turbo\"\n        # Should the SSL cert to the API endpoint be verified?\n        verify: true\n\n    # Definition of each parameters used in the different AI functions\n    function_params:\n\n      # Configuration of the assemblyline assistant feature\n      assistant:\n        # System message sent to the AI API describing how the AI should interpret the messages received\n        system_message: \"...\",\n        # Task description sent to the AI API\n        task: \"...\"\n        # Maximum number of tokens returned as a response by the AI API\n        max_tokens: 512,\n        # Other optional parameters sent to the API\n        options:\n          frequency_penalty: 0,\n          presence_penalty: 0,\n          temperature: 0,\n          top_p: 1\n\n      # Configuration of the code analysis feature\n      #     Same type of configuration block as the assistant\n      code:\n\n      # Configuration of the Detailed AL report analysis (used in hybrid reporting)\n      #     Same type of configuration block as the assistant\n      detailed_report:\n\n      # Configuration of the Executive Summary analysis (used in submission and file detail views)\n      #     Same type of configuration block as the assistant\n      executive_summary:\n\n...\n</code></pre>"},{"location":"user_manual/llm/#contributing","title":"Contributing","text":"<p>The Assemblyline team does not have any AI experts so if you have any insight on what we can tweak in our default configuration to get better results out of the LLM do not hesitate to reach out to us on Discord so we can provide a better default configuration for others.</p>"},{"location":"user_manual/malware_archive/","title":"Using Malware Archive","text":""},{"location":"user_manual/malware_archive/#using-malware-archive","title":"Using Malware Archive","text":""},{"location":"user_manual/malware_archive/#overview","title":"Overview","text":"<p>The Malware Archive feature allows users to preserve important submission information indefinitely. Most documents in Assemblyline are subjected to the Expiry process which will delete them after they reach their expiry date. This is done to avoid bloating the system with unimportant data. However, if a user wants to preserve a relevant submission for future use, they can archive it to keep it forever!</p> <p>To start using Malware Archive, make sure you have the correct configuration.</p> <p>If you want to understand how Malware Archive works, a detailed description can be found in the System Architecture</p>"},{"location":"user_manual/malware_archive/#archiving-a-submission","title":"Archiving a submission","text":"<p>To archive a submission, go to its detail page and click on the \u201cArchive\u201d button to submit a request to archive this submission. If the archive functionality is enabled and working properly, you should see a successful snackbar message appear from the bottom.</p> <p></p>"},{"location":"user_manual/malware_archive/#view-archived-files","title":"View archived files","text":"<p>The next step is to navigate to the Malware Archive page using the left navbar. This interface is based around files instead of submissions. Therefore, all the files that are part of the submission, including supplementary files, generated during the analysis will be archived and be found in this search interface.</p> <p></p>"},{"location":"user_manual/malware_archive/#searching-through-the-archived-files","title":"Searching through the archived files","text":"<p>This search interface offers methods to quickly filter and allow users to find the relevant files. Actions such as clicking on the three quick action buttons on the right side of the search bar, on a graph element, or a label in the table, will add a filter chip below the search bar. You can click on that chip to get the opposite effect marked in red.</p> <p></p>"},{"location":"user_manual/malware_archive/#analysing-an-archived-file","title":"Analysing an archived file","text":"<p>Clicking on a file in the table will open its archived detail page. To make this more convenient, we have included familiar and new sections needed to analyze this file:</p> <ul> <li>Details: This section summarize the file's information.</li> <li>Detections: This section contains the detailed results of this file's analysis by Assemblyline.</li> <li>Tags: This section displays all the heuristics and tags in a tabular format. This table allows users to sort and filter the data using the headers. Clicking on a row submits a request to find results sharing this tag type and value.</li> <li>Relations: The goal of this section is to find similar results sharing similar properties.</li> <li>ASCII, Strings, Hex: Instead of navigating to another page, we have included the file viewers in these sections.</li> <li>Community: This section contains all the user-provided actions such as labelling this file and adding comments. Note that labels and comments are also search parameters.</li> </ul> <p></p>"},{"location":"user_manual/results/","title":"Assemblyline results","text":""},{"location":"user_manual/results/#assemblyline-results","title":"Assemblyline results","text":"<p>Let's look at what an Assemblyline submission looks like! But before we do that, we need to go over some verbiage.</p>"},{"location":"user_manual/results/#submission","title":"Submission","text":"<p>A record of analysis for a given file and all its children with associated metadata and analysis parameters.</p>"},{"location":"user_manual/results/#service","title":"Service","text":"<p>A service in Assemblyline is a configurable component that performs specific tasks within the file triage and malware analysis system. Each service is identified by a unique name and categorized into predefined groups such as Antivirus, Dynamic Analysis, Extraction, etc. Services operate at different stages of the analysis pipeline, such as FILTER, EXTRACT, CORE, SECONDARY, POST, and REVIEW, to ensure an organized and efficient workflow. They process specific file types and can either operate within Assemblyline's infrastructure or interact with external systems. The service's mode of operation, classification level, and enabled status are also key attributes that define how it functions within the system.</p>"},{"location":"user_manual/results/#heuristic","title":"Heuristic","text":"<p>A specific feature/behaviour that a service is looking for in a file can generate a score and alert the analyst of potentially malicious intent.</p> <p>A heuristic has:</p> <ul> <li>id</li> <li>name</li> <li>description</li> <li>score (used to label heuristic as MALICIOUS / SUSPICIOUS / INFO)</li> <li>MITRE Att&amp;ck ID</li> <li>Signatures are often raised under a heuristic to provide more context</li> </ul> <p>Heuristics are tracked by the system to provide statistics on the number of hits and scores. Doing so can help with adjusting well-performing and under-performing heuristics. You can find specific details about a heuristic by selecting one of the heuristic cards in the Heuristic Management interface. This will bring you to a detailed view of the heuristic, where you can see information such as its name, description, score, and MITRE ATT&amp;CK ID. Additionally, you can scroll down to the \"Statistics\" section to visualize when certain files with the same features come into the system.</p> <p></p> <p>Heuristics will be shown in the UI and will be colour-coded based on the level of their maliciousness.</p> <p></p>"},{"location":"user_manual/results/#tags","title":"Tags","text":"<p>Important features extracted from a file which can be used to group multiple files.</p> <p>All tag names follow the same naming convention. They are namespaced to improve organization and searching for specific information in the system</p> <p>Tags are also all indexed, which allow for blazingly fast results.</p> <p><code>network.static.ip \u2192 All IPs extracted statically will be found with this tag regardless of which service extracted it.</code></p> <p>All tags registered with the system will be listed under the Help menu &gt; Current Configuration page &gt; Tags section of your Assemblyline instance.</p> <p>There are 202 different types of tags available as of this writing (Ex: IP, Domain, URL, Implant Family, PE Imports hash, Process Name, etc.).</p>"},{"location":"user_manual/results/#score","title":"Score","text":"<p>A single number summarizing the risk associated with a file according to the services that analyzed it.</p> <p>The score of a submission (maliciousness level) is determined by the highest score of any file extracted during the analysis process.</p> <p>For example, let's take a <code>zip</code> file. The <code>zip</code> file itself might score 0 but if it contained two files, one of which scored 100 and the other 500, then the max score of the submission will be 500. It is possible to drill down into the file structure to understand exactly what contributed to each score.</p>"},{"location":"user_manual/results/#verdict","title":"Verdict","text":"<p>The text version of the risk associated with a file or result. Can be one of the following:</p> <pre><code>-1000: safe\n0 - 299: informative\n300 - 699: suspicious\n700 - 999: highly suspicious\n&gt;= 1000: malicious\n</code></pre> <p>The configuration model for these mappings can be found here.</p> <p>For more details on how to interpret the score and verdict, head to Assemblyline Verdicts</p>"},{"location":"user_manual/results/#submission-report","title":"Submission Report","text":"<p>The first page that will appear when you view a submission is the submission report. This page is a high-level summary to allow an analyst to decide if it is worth digging deeper.</p> <p>You can easily print out / save the report as a PDF via the printer icon button on the top right.</p>"},{"location":"user_manual/results/#general-information","title":"General information","text":"<p>At the top of the report, you will find important information such as submission classification, submission ID, timestamps, the file type detected, file size, and various hashes representing the file contents. The analysis verdict is also given with the maximum score revealed when hovering over the verdict (i.e. 'Malicious').</p> <p></p>"},{"location":"user_manual/results/#heuristics","title":"Heuristics","text":"<p>Under this section you will find all the heuristics that were raised for the submitted file as well as its children files, and the details for each heuristic can be found below the heuristic title. It looks like there were some de-obfuscated network indicators of compromise seen as well as an external file was downloaded from a batch script.</p> <p></p>"},{"location":"user_manual/results/#attribution","title":"Attribution","text":"<p>This section will provide attribution from Yara signatures (if the actor tag is provided in your rule's metadata) and anti-virus virus names. Assemblyline uses the CCCS YARA standard. For best results, follow the CCCS standard when writing Yara rules.</p> <p></p>"},{"location":"user_manual/results/#indicators-of-compromise","title":"Indicators of Compromise","text":"<p>This section where network indicators such as URIs, domains, and IPs are brought forward for the  reader to easily see.</p> <p></p>"},{"location":"user_manual/results/#submission-details","title":"Submission Details","text":"<p>The \"Submission Details\" button is located at the top of the submission report.</p> <p></p> <p>Submission details will display submission parameters such as which services were selected when the file was submitted and submission metadata.</p>"},{"location":"user_manual/results/#extracted-file-tree","title":"Extracted File Tree","text":"<p>The extracted file tree section will show a view of all the files that were processed and extracted along with their respective score and file type.</p> <p>Clicking on the files will reveal Assemblyline's most interesting section the File details page.</p> <p></p>"},{"location":"user_manual/results/#file-details","title":"File Details","text":"<p>Under the \"File Details\" section you will find everything about a specific file. Regardless of which submission it came from.</p> <p>In the top right corner, you will find a series of useful functions</p> Icon Description Find all related submissions Download file (by default the file will be inserted in the CaRT format to prevent accidental self-infection) File viewer (Ascii, Strings, Hex view)  Resubmit the file for analysis Add file to the safelist"},{"location":"user_manual/results/#file-frequency","title":"File Frequency","text":"<p>You can find statistics related to file frequency in the individual \"File Details\" pages within your submissions. This section will tell you how many times the file has been seen, along with a first and last seen timestamp. These values will be affected by the retention period of the file in the system.</p> <p></p>"},{"location":"user_manual/results/#file-tags","title":"File Tags","text":"<p>This section will include all the tags extracted within this file, grouped by type. This is where you will find IPs, URLs and many other IoCs (indicators of compromise) which you can harvest to support your investigation or use to start a dynamic action (e.g. issue blocks on your firewalls).</p> <p>If you click on one of the tags it will highlight which service it came from.</p> <p></p>"},{"location":"user_manual/results/#service-results","title":"Service Results","text":"<p>This section lets you visualize the output of each service along with any heuristics and tags raised. You can also see which services were the source of \"extracted files\" at the end of each service result. The cached file results are ignored every time a service is updated; if multiple service result versions are available, they will be shown in a dropdown which will let you look at older analysis results.</p> <p>You can expand the details by clicking on a service result section.</p> <p></p>"},{"location":"user_manual/retrohunt/","title":"Using Retrohunt","text":""},{"location":"user_manual/retrohunt/#using-retrohunt","title":"Using Retrohunt","text":""},{"location":"user_manual/retrohunt/#overview","title":"Overview","text":"<p>The Retrohunt feature allows users to scan the historical collection of files in Assemblyline using YARA rules in order to detect early versions of an attack using newer rule sets to understand how an attack has evolved over time.</p> <p>To start using Retrohunt, make sure you have the correct configuration.</p> <p>If you want to understand how Retrohunt works, a detailed description can be found in the System Architecture</p>"},{"location":"user_manual/retrohunt/#retrohunt-interface","title":"Retrohunt Interface","text":"<p>Click the \"Retrohunt\" left navbar link to access the main Retrohunt interface. This page displays all the Retrohunt searches that users have created while taking into consideration their classification. You can filter these searches using your own search parameters, and it also provides quick search actions (on the right side of the search bar) to display jobs that were completed in the last 24h and your own jobs.</p> <p></p>"},{"location":"user_manual/retrohunt/#creating-a-new-retrohunt-search","title":"Creating a new Retrohunt search","text":"<p>To create a new search job, click the green \"plus\" button on the top right of the main interface to open the Create Retrohunt Search page. Enter the following details:</p> <ul> <li>Search information: Provide a description for the search, select which indices to search on (search space) and enter the expiry date (days to live).</li> <li>Maximum file classification: This property defines the scope of the included files in the search, but note that even with higher classification levels, users with a lower classification will not be able to see all the resulting hits after the search has completed.</li> <li>YARA rule: When writing a YARA rule, starting to type <code>rule</code> will show a snippet that will create a basic rule template. There are a number of additional restrictions on YARA rules for retrohunting that are described in the searching details.</li> </ul> <p>When you're done, click the \"ADD RETROHUNT JOB\" and confirm the job to create it. If no errors have occurred, a green snack bar should appear from the bottom and the interface should redirect to its Retrohunt Detail page. That page will show a progress bar denoting the status of the search. After the job has completed, it will show the resulting hits of the search.</p> <p></p>"},{"location":"user_manual/retrohunt/#searching-details","title":"Searching details","text":"<p>A retrohunt search takes a single valid YARA rule to target the search. This rule is also subject to additional constraints due to the nature of the search. </p> <p>The retrohunt search first runs a trigram search called the \"filtering\" stage to find files that may be hits for the YARA rule. The candidate files selected by the filtering stage then have the YARA rule run on them normally. </p> <p>Due to the particular nature of trigram searches the YARA rule is decomposed into strings which can be expressed as trigram sequences. Rules that work well for trigram searching break down into long trigram sequences that are manditory for a rule to hit, this comes from having long strings that the conditions of the rule require to be found. </p> <p>When a rule can't be broken down into trigram sequences the search will fail. In other cases rules can be broken into trigram sequences, but only short ones, this results in searches that run very slowly and may miss many results. Only a limited number of files are accepted from the filtering stage for YARA to run on, and a poor trigram decomposition will result in many false positives filling up that limited space blocking true hits on the YARA signature from being considered.</p> <p>Some rules of thumb to help choose good YARA rules for retrohunting:</p> <ul> <li>Long manditory strings are strong. A three character string is the shortest that can be processed, every byte longer than three progressively improves the quality of the filtering.</li> <li>Where strings have alternates (string A or string B) the effect of each string on filtering is slightly weakened.</li> <li>Where strings are case insensitive or use multiple encodings the effect of the string is slightly weakened.</li> <li>When a string includes a wildcard, it breaks the string into two separate shorter ones for the purposes of filtering quality. So a five character string where the middle one is wildcard contributes no trigrams to the filtering stage.</li> <li>Most rule condition logic is supported but a few features are missing, most notably loops are not supported.</li> <li>Regex is supported, but regexes are deconstructed using the same logic as the conditions and strings of the YARA rule, optional strings in a regex <code>(abc123)?</code> contribute no trigrams, but required or alternate strings contribute trigrams as usual. A regex <code>.*(abc123|xyz456)+.</code> required in the rule conditions will contribute two alternate sequences of four trigrams and will improve filtering similarly to if those strings were added with an or in the rule conditions directly.</li> </ul>"},{"location":"user_manual/retrohunt/#viewing-the-hits-of-a-retrohunt-search","title":"Viewing the hits of a Retrohunt search","text":"<p>The Retrohunt Detail page displays information in a tabular format.</p> <ul> <li>Details: This tab shows the search information and displays the resulting hits of the search. You may reduce the scope of the hits by entering a search query or by clicking on a column in the distribution graphs to add a filter value. Clicking on a table row will open the detail page of that file.</li> <li>YARA Rule: The submitted YARA rule is found in this tab.</li> <li>Errors: This tab is only accessible to administrators as it shows the warnings and errors that occurred during the Retrohunt search process, which may be used for debugging purposes.</li> </ul> <p></p>"},{"location":"user_manual/retrohunt/#repeating-a-retrohunt-search","title":"Repeating a Retrohunt search","text":"<p>The Retrohunt feature offers the ability to repeat completed jobs using the same YARA rule. To repeat a search, click on the \"Repeat this Retrohunt search\" button on the top right side of the Retrohunt Detail page to open the \"Repeat Retrohunt Search\" dialog box. You may change the <code>maximum file classification</code> and the <code>days to live</code>, but note that selecting a lower value will not update that property even though the interface doesn't prevent you from selecting them.</p> <p></p>"},{"location":"user_manual/searching/","title":"Searching in Assemblyline","text":""},{"location":"user_manual/searching/#searching-in-assemblyline","title":"Searching in Assemblyline","text":"<p>Assemblyline provides robust search capabilities within its user interface, allowing users to search for anything stored in its indices. By using the search widget, users can submit queries following the Lucene query syntax, which are then handled by the search engine. The fields available for searching are determined by several Object Data Models (ODMs) captured via Elasticsearch indices.</p>"},{"location":"user_manual/searching/#understanding-indices","title":"Understanding Indices","text":"<p>Elasticsearch indices enable Assemblyline to deduplicate most of the results in the system, which significantly enhances its scalability. Searching through indexed fields is also remarkably fast.</p> <p>Assemblyline has six primary indices:</p> <ul> <li> <p>Alert: Allows users to perform detailed searches on alerts to quickly identify and prioritize security incidents, taking into account various attributes such as threat indicators, classification, and timestamps.</p> </li> <li> <p>File: Allows users to search for specific files within a submission, identify duplicates, and gather context about a file's properties such as its classification, entropy, and observed hash values.</p> </li> <li> <p>Result: Allows users to search for specific service results, enabling the examination of the analysis performed by various services, including detailed scores, sections, and response data.</p> </li> <li> <p>Retrohunt: Allows users to search retrospective threat hunt results from Yara rules applied to previously submitted samples. This facilitates the identification and analysis of new threats based on updated threat intelligence.</p> </li> <li> <p>Signature: Allows users to search for service-specific signatures (e.g., YARA rules) and any relevant metadata, including source, statistics, classification, and status.</p> </li> <li> <p>Submission: Allows users to manage and track submissions, viewing the files involved, analysis errors, maximum scores, and the lifecycle status of the submission, which provides a holistic view of the analysis process.</p> </li> </ul> <p>You can view all indices and their indexed fields from the <code>Help &gt; Search Help</code> menu in your Assemblyline installation.</p>"},{"location":"user_manual/searching/#using-the-search-interface","title":"Using the Search Interface","text":""},{"location":"user_manual/searching/#search-bar","title":"Search Bar","text":"<p>The search bar, located at the top of the user interface, lets you perform searches across all indices.</p> <p></p>"},{"location":"user_manual/searching/#search-page","title":"Search Page","text":"<p>Additionally, you can perform searches using the generic Search page.</p> <p></p>"},{"location":"user_manual/searching/#search-results","title":"Search Results","text":"<p>Search results will be displayed across the different indices. The results are categorized by:</p> <ul> <li>SUBMISSION</li> <li>FILE</li> <li>RESULT</li> <li>SIGNATURE</li> <li>ALERT</li> <li>RETROHUNT</li> </ul> <p></p> <p>You must limit your search criteria to a single index. Searching across multiple indices simultaneously (i.e., JOIN queries) is not supported.</p> <p>This limitation can be mitigated by using the Assemblyline Client to perform queries on one index and then refine or enrich your search by querying another index.</p>"},{"location":"user_manual/searching/#search-examples","title":"Search Examples","text":""},{"location":"user_manual/searching/#basic-searches","title":"Basic Searches","text":"<p>To familiarize yourself with the indices, use the \"Find related results\" option from the tags dropdown menu, accessible by right-clicking any tag found throughout Assemblyline.</p> <p></p> <p>For example, clicking it on the <code>av.virus_name</code> tag (<code>HEUR/Macro.Downloader.MRAA.Gen</code>) will generate the following query:</p> <pre><code>result.sections.tags.av.virus_name:\"HEUR/Macro.Downloader.MRAA.Gen\"\n</code></pre>"},{"location":"user_manual/searching/#advanced-searches","title":"Advanced Searches","text":"<p>Harness the full power of the Lucene query syntax for more complex searches. Here are a few examples:</p> <pre><code># Find every result where the ViperMonkey service extracted the IP 10.10.10.10\nresult.sections.tags.network.static.ip:\"10.10.10.10\" AND response.service_name:ViperMonkey\n\n# Find all submissions with a score greater than or equal to 2000 in the last two days\nmax_score:[2000 TO *] AND times.submitted:[now-2d TO now]\n\n# Find all anti-virus results with Emotet in the signature name\nresult.sections.tags.av.virus_name:*Emotet*\n</code></pre> <p>Assemblyline supports various search parameters, including wildcards, ranges, and regex. Refer to <code>Help &gt; Search Help</code> for comprehensive syntax.</p> <p>Search queries can also be used with the Assemblyline Client to automate complex tradecraft as new files are processed by the system.</p>"},{"location":"user_manual/searching/#autofill-feature","title":"Autofill Feature","text":"<p>Given the wide range of searchable fields per index, the \"autofill\" feature can assist in constructing queries. To use autofill, navigate to an index-specific search page, such as \"Result\" (<code>/search/result</code>), and start typing. Autofill will suggest available fields:</p> <p></p> <p>For instance, if you wish to query all submissions marked as <code>TLP:CLEAR</code> and containing a service that scored greater than 500, you should search within the <code>Result</code> index:</p> <p></p>"},{"location":"user_manual/submitting_file/","title":"Submitting a File for Analysis","text":""},{"location":"user_manual/submitting_file/#submitting-a-file-for-analysis","title":"Submitting a File for Analysis","text":""},{"location":"user_manual/submitting_file/#submission-process","title":"Submission Process","text":"<p>Submitting a file for analysis in Assemblyline is straightforward via the WebUI interface. For automated tasks and integration purposes, consider using the REST API.</p> <p></p>"},{"location":"user_manual/submitting_file/#sharing-and-classification","title":"Sharing and Classification","text":"<p>Select the desired classification level or sharing restrictions by clicking on the Classification Banner, provided your system configuration includes TLP or another classification scheme.</p>"},{"location":"user_manual/submitting_file/#selecting-a-file-to-be-analyzed","title":"Selecting a File to be Analyzed","text":"<p>Upload a file for analysis either by clicking the \"Select a file to analyze\" button or using drag and drop into the designated dashed area.</p>"},{"location":"user_manual/submitting_file/#choosing-analysis-type","title":"Choosing Analysis Type","text":"<p>Select the most suitable analysis type for your file from the \"Type of Analysis\" dropdown menu.</p>"},{"location":"user_manual/submitting_file/#options","title":"Options","text":"<p>Access advanced submission options by clicking the \"Tune\" icon to open the \"Adjust\" panel. At the top, a banner indicates the level of customization privileges available to you. Users with the <code>submission_customize</code> role have the ability to modify all parameters, given that they understand the severe impact some parameters have on the system if miused.</p>"},{"location":"user_manual/submitting_file/#submission-parameters","title":"Submission Parameters","text":"<ul> <li>Description: Optionally provide a description for the analysis, or leave it blank to accept the default value set by the system.</li> <li>Priority: Designate the submission's processing priority.</li> <li>Days to live: Specify how long (in days) the file should be retained in the system.</li> <li>Generate alert: Decide whether the submission should trigger an alert upon analysis completion.</li> <li>Ignore filtering services: Opt to bypass any safelisting services.</li> <li>Ignore result cache: Instruct the system to re-analyze the file, disregarding any recent similar analysis.</li> <li>Ignore recursion prevention: Remove iteration limits for the submission.</li> <li>Perform deep analysis: Engage thorough deobfuscation, recommended for confirmed malicious or highly suspicious files.</li> </ul>"},{"location":"user_manual/submitting_file/#submission-data","title":"Submission Data","text":"<ul> <li>Decryption Password: Quickly input a password for encrypted files, bypassing the need to provide it to individual services.</li> </ul>"},{"location":"user_manual/submitting_file/#service-parameters","title":"Service Parameters","text":"<ul> <li>Service categories: Choose a preset group of services.</li> <li>Specific service: Manually select individual services for the analysis.</li> <li>Service parameters: Fine-tune service-specific parameters by expanding their individual menus.</li> </ul>"},{"location":"user_manual/submitting_file/#submission-metadata","title":"Submission Metadata","text":"<ul> <li>System Metadata: Fill in required system-generated metadata fields.</li> <li>Extra Metadata: For those with full customization abilities, all additional metadata fields are editable.</li> </ul>"},{"location":"user_manual/submitting_file/#understanding-file-analysis","title":"Understanding File Analysis","text":"<p>Upon submission, Assemblyline conducts several assessments to determine the optimal analysis path. Its recursive analysis capability is especially effective at peeling layers of obfuscation often found in malware, ultimately revealing underlying scripts or unpacked versions that can be more readily identified by traditional antivirus solutions.</p> <p></p>"},{"location":"user_manual/submitting_sha256/","title":"Submitting a SHA256 for Analysis","text":""},{"location":"user_manual/submitting_sha256/#submitting-a-sha256-for-analysis","title":"Submitting a SHA256 for Analysis","text":""},{"location":"user_manual/submitting_sha256/#how-to-submit-a-sha256","title":"How to Submit a SHA256","text":"<p>The process to submit a SHA256 hash is very similar to submitting a URL and can be accomplished directly through the Assemblyline WebUI. For those looking to automate the process or integrate with other systems, the REST API is available. Simply navigate to the \"URL/SHA256\" tab to begin.</p> <p></p>"},{"location":"user_manual/submitting_sha256/#share-level-and-classification","title":"Share Level and Classification","text":"<p>Use the Classification Banner to assign the appropriate sharing level, such as Traffic Light Protocol (TLP) or another classification scheme, to your submission.</p>"},{"location":"user_manual/submitting_sha256/#submitting-a-sha256-hash","title":"Submitting a SHA256 Hash","text":"<p>Enter the SHA256 hash you want to analyze into the \"URL/SHA256 To Scan\" textbox and click \"SCAN\" to start the submission process.</p>"},{"location":"user_manual/submitting_sha256/#important-notes-on-sha256-submissions","title":"Important Notes on SHA256 Submissions","text":"<p>When you submit a SHA256 hash, Assemblyline checks if the hash matches any file previously encountered. If a match is found, it resubmits the file for further analysis. Alternatively, if the file isn't found in the local store, Assemblyline will attempt to locate it on an external source, such as Malware Bazaar, and submit it for analysis if found.</p> <p>This behavior depends on your deployment configuration\u2014see <code>submission.file_sources</code> for setup details.</p>"},{"location":"user_manual/submitting_sha256/#options","title":"Options","text":"<p>Access advanced submission options by clicking the \"Tune\" icon to open the \"Adjust\" panel. At the top, a banner indicates the level of customization privileges available to you. Users with the <code>submission_customize</code> role have the ability to modify all parameters, given that they understand the severe impact some parameters have on the system if miused.</p>"},{"location":"user_manual/submitting_sha256/#submission-parameters","title":"Submission Parameters","text":"<ul> <li>Description: Optionally provide a description for the analysis, or leave it blank to accept the default value set by the system.</li> <li>Priority: Designate the submission's processing priority.</li> <li>Days to live: Specify how long (in days) the file should be retained in the system.</li> <li>Generate alert: Decide whether the submission should trigger an alert upon analysis completion.</li> <li>Ignore filtering services: Opt to bypass any safelisting services.</li> <li>Ignore result cache: Instruct the system to re-analyze the file, disregarding any recent similar analysis.</li> <li>Ignore recursion prevention: Remove iteration limits for the submission.</li> <li>Perform deep analysis: Engage thorough deobfuscation, recommended for confirmed malicious or highly suspicious files.</li> </ul>"},{"location":"user_manual/submitting_sha256/#submission-data","title":"Submission Data","text":"<ul> <li>Decryption Password: Quickly input a password for encrypted files, bypassing the need to provide it to individual services.</li> </ul>"},{"location":"user_manual/submitting_sha256/#service-parameters","title":"Service Parameters","text":"<ul> <li>Service categories: Choose a preset group of services.</li> <li>Specific service: Manually select individual services for the analysis.</li> <li>Service parameters: Fine-tune service-specific parameters by expanding their individual menus.</li> </ul>"},{"location":"user_manual/submitting_sha256/#submission-metadata","title":"Submission Metadata","text":"<ul> <li>System Metadata: Fill in required system-generated metadata fields.</li> <li>Extra Metadata: For those with full customization abilities, all additional metadata fields are editable.</li> </ul>"},{"location":"user_manual/submitting_url/","title":"Submitting a URL for Analysis","text":""},{"location":"user_manual/submitting_url/#submitting-a-url-for-analysis","title":"Submitting a URL for Analysis","text":""},{"location":"user_manual/submitting_url/#how-to-submit-a-url","title":"How to Submit a URL","text":"<p>Submitting a URL to Assemblyline for analysis can be performed directly through the WebUI, just like submitting a file. For automation and integration with other systems, make use of the REST API.</p> <p></p>"},{"location":"user_manual/submitting_url/#share-level-and-classification","title":"Share Level and Classification","text":"<p>If TLP or a similar classification system is configured on your system, you can select the appropriate sharing restrictions using the Classification Banner.</p>"},{"location":"user_manual/submitting_url/#url-submission-details","title":"URL Submission Details","text":"<p>Input the URL you want to analyze into the \"URL/SHA256 To Scan\" text box and click \"SCAN\".</p>"},{"location":"user_manual/submitting_url/#important-information-about-url-submissions","title":"Important Information About URL Submissions","text":"<p>Submitting a URL creates a URI file that acts as the entry point for analysis. Assemblyline can also interact with external resources like fetching a second stage payload from an HTTP/HTTPS link found in a malicious file. For successful fetching of a URI, relevant services such as the URLDownloader service should be selected.</p>"},{"location":"user_manual/submitting_url/#uri-file-structure","title":"URI File Structure","text":"<p>A URI file is a YAML file with a basic structure like this:</p> <pre><code># Assemblyline URI file\nuri: &lt;scheme&gt;://&lt;host&gt;\n</code></pre> <p>It is a yaml file that can contain more elements for use-cases where the services can leverage them. The most important parts are the \"uri\" key in the yaml file, which needs to be a valid uri with a scheme and a host, and the comment on top, to help with Identification. The scheme is going to be used to create the file type, so if you are using <code>uri: http://site.com</code> it is going to be a file of type <code>uri/http</code> and if you are using <code>uri: ftp://site.com</code> then you will have a <code>uri/ftp</code>. This will make it possible to route to different services based on the scheme. In the case of <code>uri/ftp</code>, you will probably need more information, such as yaml keys like <code>passive: True</code>.</p> <p>The following is a more complete and complex example of a URI file:</p> <pre><code># Assemblyline URI file\nuri: https://mb-api.abuse.ch/api/v1/\ndata: query=get_info&amp;hash=52307f9ce784496218f2165be83c2486ad809da98026166b871dc279d40a4d1f\nheaders:\n  Content-Type: application/x-www-form-urlencoded\nmethod: POST\n</code></pre> <p>The file type would be <code>uri/https</code> and the other yaml keys will be ignored during the identification. The extra keys in the yaml file can be used by the service handling this specific file to provide a more customized behavior, closer to what the user is asking. A specific user-agent, referer or other headers could be used to fetch a second stage for a server that would required specific values. Through those extra values, URLDownloader, now supports more methods like POST. A simple change from <code>query=get_info</code> to <code>query=get_file</code> in the data and the service should be downloading that file from MalwareBazaar!</p> <p>Since URI files are very specific to Assemblyline, we take the time to rewrite any incoming file so that the comment <code># Assemblyline URI file</code> is on the first line, then the uri key, then all the other keys in alphabetical order. This is done to de-duplicate \"identical\" files and use caching. A key like <code>extra_key: [\"first\", \"second\", \"third\", \"fourth\"]</code> will have its order preserved and is going to be converted to the following:</p> <pre><code>extra_key:\n- first\n- second\n- third\n- fourth\n</code></pre>"},{"location":"user_manual/submitting_url/#uri-files-and-proxies","title":"URI files and proxies","text":"<p>At the current time, each service that needs to go through a proxy will need an administrator to configure it. URLDownloader support proxies and may even be configured to let a user choose one of many configured proxies. In the future, we will look into normalizing this feature so that all services could leverage a central proxy selection.</p> <p>This is important because if you have a URL hosting malware and you do not want to expose your Assemblyline system to that server that the URL is pointing to, then we recommend you set up a proxy server to act as a middleman between your Assemblyline infrastructure and the server hosting malware. You can set this up in your deployment configuration under the <code>ui</code> component. The item you are looking for is <code>url_submission_proxies</code>.</p>"},{"location":"user_manual/submitting_url/#options","title":"Options","text":"<p>Access advanced submission options by clicking the \"Tune\" icon to open the \"Adjust\" panel. At the top, a banner indicates the level of customization privileges available to you. Users with the <code>submission_customize</code> role have the ability to modify all parameters, given that they understand the severe impact some parameters have on the system if miused.</p>"},{"location":"user_manual/submitting_url/#submission-parameters","title":"Submission Parameters","text":"<ul> <li>Description: Optionally provide a description for the analysis, or leave it blank to accept the default value set by the system.</li> <li>Priority: Designate the submission's processing priority.</li> <li>Days to live: Specify how long (in days) the file should be retained in the system.</li> <li>Generate alert: Decide whether the submission should trigger an alert upon analysis completion.</li> <li>Ignore filtering services: Opt to bypass any safelisting services.</li> <li>Ignore result cache: Instruct the system to re-analyze the file, disregarding any recent similar analysis.</li> <li>Ignore recursion prevention: Remove iteration limits for the submission.</li> <li>Perform deep analysis: Engage thorough deobfuscation, recommended for confirmed malicious or highly suspicious files.</li> </ul>"},{"location":"user_manual/submitting_url/#submission-data","title":"Submission Data","text":"<ul> <li>Decryption Password: Quickly input a password for encrypted files, bypassing the need to provide it to individual services.</li> </ul>"},{"location":"user_manual/submitting_url/#service-parameters","title":"Service Parameters","text":"<ul> <li>Service categories: Choose a preset group of services.</li> <li>Specific service: Manually select individual services for the analysis.</li> <li>Service parameters: Fine-tune service-specific parameters by expanding their individual menus.</li> </ul>"},{"location":"user_manual/submitting_url/#submission-metadata","title":"Submission Metadata","text":"<ul> <li>System Metadata: Fill in required system-generated metadata fields.</li> <li>Extra Metadata: For those with full customization abilities, all additional metadata fields are editable.</li> </ul>"},{"location":"user_manual/user_walkthrough/","title":"User Interface Walkthrough for Users","text":""},{"location":"user_manual/user_walkthrough/#user-interface-walkthrough-for-users","title":"User Interface Walkthrough for Users","text":"<p>Welcome to the Assemblyline User Interface Walkthrough. This guide is designed to help users navigate through the essential features and functionalities of the Assemblyline malware analysis tool. You will be acquainted with the main sections of the interface, including the landing page, account management, submissions, alerts, and how to perform searches.</p>"},{"location":"user_manual/user_walkthrough/#landing-page","title":"Landing Page","text":"<p>The landing page serves as the starting point to Assemblyline, providing a comprehensive overview and swift access to the primary features.</p> <p></p>"},{"location":"user_manual/user_walkthrough/#navigation-and-layout","title":"Navigation and Layout","text":"<ul> <li>Side Menu: The side menu, found on the left-hand side, is collapsible and features expandable options for ease of use.</li> <li>Breadcrumb Navigation: This helpful feature is located at the top, adjacent to the side menu, and helps you trace your navigational path.</li> <li>Version Information: If enabled, the Assemblyline version is conveniently displayed at the bottom near the side menu.</li> </ul>"},{"location":"user_manual/user_walkthrough/#submission-section","title":"Submission Section","text":"<p>The submission section is where you can take action and submit files, URLs, or hashes for comprehensive malware analysis.</p> <ul> <li> <p>Classification Picker (1)</p> <ul> <li>Start by using the classification picker to set the appropriate classification level for your analysis.</li> <li>Clicking the classification picker reveals the available options. (2)</li> </ul> </li> <li> <p>Submission Options (3)</p> <ul> <li>Select the file or input the hash or URL you wish to analyze, then choose the type of analysis.<ul> <li>To adjust the predefined submission settings, select the \"Tune\" icon to customize the parameters as permitted.</li> <li>If you're checking for pre-existing entries in the system, use the \"Search\" icon to perform a search and link to any existing data.</li> </ul> </li> <li>Submit your entry by clicking the 'Submit' button.</li> </ul> </li> </ul> <ol> <li></li> <li></li> <li></li> </ol> <p>For step-by-step submission instructions, please refer to the following guides:</p> <ul> <li>Submitting a File</li> <li>Submitting a URL</li> <li>Submitting a SHA256 Hash</li> </ul>"},{"location":"user_manual/user_walkthrough/#additional-features","title":"Additional Features","text":"<p>Maximize your efficiency with these useful tools integrated into Assemblyline.</p> <ul> <li> <p>Search Bar: Quickly locate specific items or information using the intuitive search bar. Simply click on the bar or use the shortcut <code>Ctrl+k</code> to focus on it. (1)</p> </li> <li> <p>Notification Panel: Stay up to date with alerts and notifications by clicking the bell icon. Here, you will find system-wide announcements, updates, and even RSS feed integrations. (2)</p> </li> <li> <p>User Menu: Personalize your experience in Assemblyline by clicking your avatar, located in the top-right corner. From here, you can switch between English and French localizations or toggle between dark and light interface themes. (3)</p> </li> </ul> <ol> <li></li> <li></li> <li></li> </ol>"},{"location":"user_manual/user_walkthrough/#account-management","title":"Account Management","text":"<p>Navigate to the \"Manage Account\" section through the user menu for an overview of your profile details, such as security settings, account classification, and group memberships.</p> <p></p>"},{"location":"user_manual/user_walkthrough/#roles-and-permissions","title":"Roles and Permissions","text":"<p>Control access and capabilities within Assemblyline with role-based access controls (RBAC). This system clearly defines permissions and available actions for each user role, enabling secure and orderly management of the system.</p>"},{"location":"user_manual/user_walkthrough/#api-quotas","title":"API Quotas","text":"<p>Manage your engagement with Assemblyline's resources by understanding the quotas applied to your account. Check limits like simultaneous API interactions or concurrent submissions by clicking on \"Manage Account\" under your profile avatar.</p>"},{"location":"user_manual/user_walkthrough/#account-security","title":"Account Security","text":"<p>Security features in the \"Manage Account\" section help safeguard your presence in Assemblyline:</p> <ul> <li> <p>Multi-Factor Authentication (MFA): Strengthen your account security by enabling MFA.</p> </li> <li> <p>API Key Management: Facilitate automated tasks by generating and managing API keys, which are crucial for scripting and tool integration with the Assemblyline API. To manage your keys, sign in, click your avatar, and go to \"Manage Account.\" In the \"Security\" tab, find \"Manage API Keys\" to add, modify, or delete keys. For new keys, be sure to copy and securely store them immediately, as they will be shown only once.</p> </li> </ul>"},{"location":"user_manual/user_walkthrough/#account-settings","title":"Account Settings","text":"<p>Customize your Assemblyline experience by adjusting your account settings. Click on the gear icon labeled \"Settings\" found near your avatar in the upper right corner. This section offers various settings related to your submissions and user interface preferences.</p> <ul> <li>Save: Click \"Save\" to apply new settings.</li> <li>Cancel: Reverts any changes to the previous state.</li> <li>Reset: Resets settings back to the system default values.</li> </ul> <p>Remember, \"Cancel\" and \"Reset\" will not submit any changes, whereas \"Save\" is required to make your changes permanent.</p>"},{"location":"user_manual/user_walkthrough/#interface-options","title":"Interface Options","text":"<p>Tailor the Assemblyline user interface to suit your needs by configuring visual and file encoding settings:</p> <p></p> <ul> <li>File Encoding: To download files without antivirus interference, set your preferred file encoding under \"Interface Options.\" Select from safe formats like CaRT or password-protected ZIP to prevent automated deletions by antivirus software. (1)</li> </ul> <ol> <li></li> </ol>"},{"location":"user_manual/user_walkthrough/#submission-profiles","title":"Submission Profiles","text":"<p>Choose from predefined Submission Profiles for your analyses. These profiles are crafted by system administrators to cater to various analysis needs. While personal profiles creation isn't permitted, users may adjust existing profile parameters according to their preferences.</p> <ul> <li>Days to Live: Default retention duration for your submissions within the system.</li> <li>Classification: Your default classification level for submissions, shaping data handling in accordance with your organization's policies.</li> </ul> <p></p>"},{"location":"user_manual/user_walkthrough/#default-service-selection-and-parameters","title":"Default Service Selection and Parameters","text":"<p>Set your default service preferences if you regularly use specific services for submissions, streamlining your process:</p> <p></p>"},{"location":"user_manual/user_walkthrough/#submissions","title":"Submissions","text":"<p>Oversee your submitted items through the \"Submissions\" option in the side menu:</p> <p></p>"},{"location":"user_manual/user_walkthrough/#submission-management","title":"Submission Management","text":"<ul> <li>Filter Bar: Refine your submission view using the filter bar to sort by certain criteria.</li> <li>Preset Queries: Filter submissions quickly by user, status, or threat level indicators.</li> </ul>"},{"location":"user_manual/user_walkthrough/#submission-report-view","title":"Submission Report View","text":"<p>Gain insights from detailed analyses by accessing the \"Submission Report\" view for a chosen submission, which presents a concise and informative summary.</p>"},{"location":"user_manual/user_walkthrough/#alerts","title":"Alerts","text":"<p>Respond to critical findings effectively with Assemblyline's alert system. When submissions trigger defined conditions, alerts are raised to draw attention for further examination.</p> <p>Set up alert generation for your submissions by selecting \"Generate alert\" within the \"Settings\" submenu under \"Submission Options.\"</p>"},{"location":"user_manual/user_walkthrough/#alerts-overview","title":"Alerts Overview","text":"<p>Access a comprehensive view of all alerts on the \"Alerts\" page. This interface section enables you to view, filter, and manage alerts with ease.</p> <p></p>"},{"location":"user_manual/user_walkthrough/#filtering-alerts","title":"Filtering Alerts","text":"<ul> <li> <p>When looking for specific alerts within Assemblyline, utilize the search bar at the interface's top or the general Search page. You can perform in-depth searches within the Alert index, honing in on security incidents by attributes such as threat indicators, classification, and timestamps. (1)</p> </li> <li> <p>Set and access your favorite queries for repeated searches, saving you time and maintaining focus. (2)</p> </li> <li> <p>Refine your search further by filtering query results, ensuring you concentrate on the most pressing alerts. (3)</p> </li> </ul> <ol> <li></li> <li></li> <li></li> </ol>"},{"location":"user_manual/user_walkthrough/#workflow-actions","title":"Workflow Actions","text":"<p>Define automated actions for alerts that meet certain conditions through workflows. For instance, mark all alerts labeled as malicious and containing \"invoice\" as \"PHISHING.\"</p> <p>There are two approaches on the Alerts page:</p> <ol> <li> <p>Create a Persistent New Workflow: Click \"Create a new workflow\" to save a new named workflow within the system, applying it to future matching alerts. Manage these persistent workflows from the \"Manage Workflows\" page. (1)</p> </li> <li> <p>Apply an Ephemeral Workflow Action Immediately: Use \"Workflow actions\" for a one-time action applied to current alert matches; these are not saved as persistent workflows. (2)</p> </li> </ol> <ol> <li></li> <li></li> </ol> <p>Workflows empower you to:</p> <ul> <li>Assign Status: Define the alert status (e.g., MALICIOUS, NON-MALICIOUS).</li> <li>Assign Priority: Specify the alert's urgency (e.g., LOW, MEDIUM, HIGH).</li> <li>Assign Labels: Categorize the alert for organized management.</li> </ul>"},{"location":"user_manual/user_walkthrough/#viewing-alert-details","title":"Viewing Alert Details","text":"<p>Click on an alert card to display detailed information, aiding in thorough analysis and response.</p> <p></p> <p>Alert details cover:</p> <ul> <li>Classification: Alert's security level.</li> <li>Basic Information: Key data about the alert.</li> <li>Verdict and Labels: The assessed threat level and relevant tags.</li> <li>File Details: Information such as filename, type, size, and hashes.</li> <li>Metadata and Indicators: Additional data points and potential security flags.</li> </ul>"},{"location":"user_manual/user_walkthrough/#alert-management-tools","title":"Alert Management Tools","text":"<p>Alert detail view presents essential tools for triage analysts (1)</p> <ul> <li>View History: Inspect the alert's change log.</li> <li>Show All Alerts From Group: Focus on alerts from the same category.</li> <li>Take Ownership: Claim the alert for case management.</li> <li>Go to Related Submission: Transition to connected submission details.</li> <li>Perform a Workflow Action: Execute predefined actions on group alerts.</li> <li>Set Alert Verdict: Overrule or confirm the system's threat assessment for accurate reporting</li> </ul> <ol> <li></li> </ol>"},{"location":"user_manual/user_walkthrough/#navigating-alerts","title":"Navigating Alerts","text":"<p>Cycle through alerts using the provided navigation arrows, allowing for quick view transitions.</p>"},{"location":"user_manual/user_walkthrough/#enhanced-search-capabilities","title":"Enhanced Search Capabilities","text":"<p>Deepen your interaction with Assemblyline's data \u2013 for comprehensive instructions, head over to the Searching in Assemblyline documentation.</p>"},{"location":"user_manual/verdicts/","title":"Assemblyline Verdicts","text":""},{"location":"user_manual/verdicts/#assemblyline-verdicts","title":"Assemblyline Verdicts","text":"<p>As per the Assemblyline documentation, verdicts are the human-readable interpretation of the score that a file received upon analysis by a service.</p> <p>By default, the following score ranges are applied to certain verdicts:</p> <pre><code>-1000: safe\n0 - 299: informative\n300 - 699: suspicious\n700 - 999: highly suspicious\n&gt;= 1000: malicious\n</code></pre>"},{"location":"user_manual/verdicts/#how-the-scores-are-calculated","title":"How the scores are calculated","text":"<p>Scores are generated by heuristics raised by a service. To see the score for a specific result in your Assemblyline submission, hover over the relevant verdict \"button\" in the service results section. These buttons will either have a grey \"I\", yellow \"S\", or red \"M\" to represent their respective result verdicts.</p> <p>For example, here is the result from the Batchdeobfuscator service:</p> <p></p> <p>You can see that there are three different square buttons with an \"I\" in this view. These buttons represent the verdicts (\"I\" stands for \"Informative\"), but each for different things. The top-most button, next to the service name, represents the service score, which is a summation of all the heuristic scores for all heuristics in the service result.</p> <p>By hovering over this \"I\" button, we can see that the service score is 250:</p> <p></p> <p>Next we hover over each of the \"I\" buttons for the result sections within the result. The first result section has a score of 0:</p> <p></p> <p>And the second result section has a score of 250:</p> <p></p> <p>These two scores are added together to give us the service score next to the service name.</p> <p>Next, we pan out and look at all of the service results for that particular file, including the Batchdeobfuscator result:</p> <p></p> <p>You can hover over these \"I\" buttons and see that the only other service that had a service score greater than 0 was DeobfuScripter, which had a score of 10:</p> <p></p> <p>Service scores for a single file are added together to create a total score for that file.</p> <p>Panning out even more to the \"Files\" section of the overall submission:</p> <p></p> <p>We were looking at the service results for the <code>default.bat</code> file, and based on what we just discussed, we expect this score to be 260. We hover over the \"I\" button next to the file name and see that this is confirmed:</p> <p></p> <p>The next area where scores are applied is for the overall submission. The submission looks at all scores for all files in the \"Files\" section, and takes the maximum score found there to be the score to represent the overall submission. The submission verdict is displayed in the \"Submission Information\" section of the \"Submission Details\" page.</p> <p></p> <p>We can hover the verdict button to see the score that rendered this verdict:</p> <p></p> <p>Interesting, I guess the <code>default.bat</code> file that scored 260 was not the highest scoring file in the \"Files\" section.</p> <p>By hovering over the other files in the section, we see that the <code>vbe_decoded</code> file is actually the highest scoring file, with a score of 273:</p> <p></p> <p>Since that is the highest scoring file in the submission, the overall submission score is set to that file's score.</p>"},{"location":"user_manual/verdicts/#how-heuristic-scores-are-assigned","title":"How heuristic scores are\u00a0assigned","text":"<p>Heuristics in a service are assigned an arbitrary score by the service writer. If the score of a heuristic is less than 500 (which would flag the file as suspicious), then the intention of the service writer is for this heuristic score to be compounded with the scores of other heuristics in the service result before the verdict of the file is deemed suspicious or worse.</p> <p>If the heuristic score is 1000 or above, then this heuristic is a high-confidence heuristic that can be trusted to flag a file as malicious with little to no false positives. Heuristics like these are found in signature-based services like AntiVirus, Intezer, ConfigExtractor, VirusTotal and Yara, to name a few.</p> <p>If the heuristic score is between 500 and 1000, then the service writer is relatively confident that this file has a feature that is suspicious or even highly suspicious, but cannot confirm that the file is definitely malicious.</p>"},{"location":"user_manual/verdicts/#interpreting-the-verdict","title":"Interpreting the\u00a0verdict","text":"<p>Now that we have covered the fundamentals of scoring and where verdicts come from, let's get into the reason why we are all here:</p> <p>\"HELP I HAVE A FILE THAT ASSEMBLYLINE SAYS IS MALICIOUS! I'VE BEEN COMPROMISED! NOOOOOOOOOOOOOO\"</p> <ol> <li>First things first, stay calm. Assemblyline has been known to give false positive results in the past, and this could be an example of this.</li> <li>Dive into the details of the submission. Which file is being scored the highest in the \"Files\" section?</li> <li>Dive deeper into the analysis results for this file. Which services were scoring the highest? Remember, these scores would then be added together.</li> <li>Dive even deeper into the services that are scoring the highest. Which heuristics are scoring the highest? Based on their scores and how we know scores are arbitrarily assigned to heuristics by service writers, do we think they are accurate?</li> </ol> <p>After you have performed the steps mentioned above, are you confident that the file is a true positive or a false positive? If the file is a false positive, please raise this to the relevant service writers so that they can adjust the heuristic scores or tweak the service to avoid this.</p>"},{"location":"fr/","title":"Accueil","text":""},{"location":"fr/overview/community_services/","title":"Services de la communaut\u00e9","text":""},{"location":"fr/overview/community_services/#services-de-la-communaute","title":"Services de la communaut\u00e9","text":"<p>La communaut\u00e9 d'Assemblyline travaille fort pour am\u00e9liorer cet outil \u00e0 d\u00e9tect\u00e9 des fichiers malicieux.</p> <p>Cette page contient la liste de services cr\u00e9e et partag\u00e9 avec le publique.</p> <p>Attention</p> <p>Ces services ne sont pas g\u00e8rer par l'\u00e9quipe d'Assemblyline, nous vous invitons \u00e0 faire une revue de leur code pour vous assurez d'\u00eatre confortable avec ce qu'ils font avant de les utilis\u00e9s dans v\u00f4tre syst\u00e8me.</p>"},{"location":"fr/overview/community_services/#liste-de-services-anglais-seulement","title":"Liste de services (anglais seulement)","text":"Nom de service Description Auteur AutoItRipper Service de d\u00e9compression AutoIt NVISO ClamAV Service d'assemblage qui soumet un fichier \u00e0 ClamAV et affiche le r\u00e9sultat NVISO HybridAnalysis Utilise le service Hybrid Analysis pour fournir des renseignements suppl\u00e9mentaires sur les menaces et des capacit\u00e9s d'analyse des logiciels malveillants. boredchilada MalwareBazaar Service de ligne d'assemblage r\u00e9cup\u00e9rant le rapport de Malware Bazaar NVISO MsgParser Service AssemblyLine d'extraction simple de MSG NVISO MetaDefender Sandbox Soumet un fichier ou une URL \u00e0 MetaDefender Sandbox OPSWAT PythonExeUnpack Service de d\u00e9compression de l'exe Python NVISO ReversingLabsSpectraIntelligence Ce service utilise le service ReversingLabs' Spectra Intelligence pour obtenir des informations d\u00e9taill\u00e9es et tr\u00e8s pr\u00e9cises sur la r\u00e9putation et l'analyse des fichiers soumis. ReversingLabs StegFinder Service AssemblyLine qui recherche des donn\u00e9es int\u00e9gr\u00e9es dans une image \u00e0 l'aide de StegExpose. NVISO Unfurl Service Assemblyline qui analyse une URL soumise pour la d\u00e9velopper. NVISO UrlScanIo Ce service r\u00e9cup\u00e8re les r\u00e9sultats d'une analyse urlscan.io. NVISO WindowsDefender Service Windows defender adapt\u00e9 d'une communaut\u00e9 Assemblyline conversation Adam McHugh"},{"location":"fr/overview/community_services/#construire-un-service-pour-la-communaute","title":"Construire un service pour la communaut\u00e9","text":"<ol> <li>Obtenir le code source du service</li> <li> <p>Modifiez le manifeste du service et assurez-vous que les \u00e9l\u00e9ments suivants sont d\u00e9finis</p> <pre><code>version: $SERVICE_TAG\n...\ndocker_config:\n  image: ${REGISTRY}&lt;service_container_image&gt;:$SERVICE_TAG\n</code></pre> </li> <li> <p>Cr\u00e9ez une image et transf\u00e9rez-la vers votre registre local:</p> Attention <p>Il est fortement recommand\u00e9 de baliser les images de service en suivant le format Assemblyline. Sinon, le syst\u00e8me d\u00e9sactivera votre service car il le jugera incompatible avec le reste des composants.</p> <p> Les versions de service doivent suivre le format <code>A.B.C.(dev|stable).D</code>, o\u00f9 :</p> <ul> <li><code>A, B</code> repr\u00e9sentent respectivement la version du framework et du syst\u00e8me.</li> <li><code>C, D</code> peuvent \u00eatre utilis\u00e9s pour indiquer respectivement le majeur et le mineur d'un service.</li> <li>La partie <code>dev</code> ou <code>stable</code> de la balise doit indiquer l'\u00e9tat de la construction du service. Ceci est \u00e9galement pertinent pour fournir des mises \u00e0 jour de service sous un certain canal.</li> </ul> <p>Voici un exemple de build de service destin\u00e9 \u00e0 un d\u00e9ploiement Assemblyline ex\u00e9cutant la version 4.5.x.x :</p> <pre><code>docker build . -t &lt;private_registry&gt;/&lt;service_container_image&gt;:4.5.0.stable0 --build-arg version=4.5.0.stable0\ndocker push &lt;private_registry&gt;/&lt;service_container_image&gt;:4.5.0.stable0\n</code></pre> </li> <li> <p>Ajoutez le contenu du manifeste de service \u00e0 l'interface utilisateur ou utilisez l'API REST pour ajouter le service \u00e0 Assemblyline.</p> </li> </ol>"},{"location":"fr/overview/community_services/#faire-nous-part-de-vos-services","title":"Faire nous part de vos services","text":"<p>Contactez-nous sur Discord pour faire ajouter vos services a cette page.</p>"},{"location":"fr/overview/how_it_works/","title":"Comment \u00e7a fonctionne","text":""},{"location":"fr/overview/how_it_works/#comment-ca-fonctionne","title":"Comment \u00e7a fonctionne","text":"<p>Assemblyline r\u00e9duit le nombre de fichiers anodins que les praticiens de TI doivent v\u00e9rifier chaque jour, ce qui leur permet de collaborer avec les autres utilisateurs \u00e0 la personnalisation et l\u2019am\u00e9lioration de la plateforme.</p> <p></p> <p>A) Assemblyline fonctionne un peu comme une cha\u00eene de montage\u00a0: les fichiers arrivent dans le syst\u00e8me et font l\u2019objet d\u2019un tri selon une certaine s\u00e9quence.</p> <p>B) Assemblyline g\u00e9n\u00e8re de l\u2019information sur chacun des fichiers, puis leur attribue un identifiant unique qui les suit alors qu\u2019ils sont achemin\u00e9s dans le syst\u00e8me.</p> <p>C) Les utilisateurs peuvent ajouter leurs propres outils d\u2019analyse, que l\u2019on appelle services.</p> <p>D) Les services s\u00e9lectionn\u00e9s par l\u2019utilisateur dans Assemblyline analysent alors les fichiers \u00e0 la recherche d\u2019indications de malveillance et en extraient les caract\u00e9ristiques \u00e0 des fins d\u2019analyse plus pouss\u00e9e.</p> <ul> <li>Lors de l\u2019analyse, le syst\u00e8me g\u00e9n\u00e8re des alertes concernant un fichier malveillant et lui accorde une note.</li> <li>Le syst\u00e8me peut \u00e9galement lancer l\u2019ex\u00e9cution de syst\u00e8mes de d\u00e9fense automatis\u00e9s.</li> <li>Il est d\u2019ailleurs possible de transmettre les indicateurs de malveillance g\u00e9n\u00e9r\u00e9s par le syst\u00e8me aux autres syst\u00e8mes de d\u00e9fense.</li> </ul>"},{"location":"fr/overview/services/","title":"Services d'Assemblyline","text":""},{"location":"fr/overview/services/#services-dassemblyline","title":"Services d'Assemblyline","text":"<p>Les Services install\u00e9 sur un syst\u00e8me peuvent \u00eatre trouv\u00e9 sous: <code>Help &gt; Service Listing</code>.</p> <p>Cette liste contient tous les services inclus et maintenue avec Assemblyline:</p> Service Name Speciality Description APIVector Windows binaries Extrait les imports de fichiers PE files ou m\u00e9moire pour g\u00e9n\u00e9rer un vecteur de classification. APKaye Android APK Les APK sont d\u00e9compil\u00e9s et inspect\u00e9s. Les indicateurs de r\u00e9seau et les informations trouv\u00e9es dans le fichier manifeste APK sont affich\u00e9s AntiVirus Anti-virus Client ICAP g\u00e9n\u00e9rique utilisant plusieurs solution d'Anti-virus commerciales Batchdeobfuscator Deobfuscation R\u00e9solution de variables locales et globales d'un fichier batch CAPA Windows binaries Int\u00e9gration de l'outil public CAPA Characterize Analyze d'entropy Calcule l'entropy des fichiers et extrait les meta-donn\u00e9e Exif. ConfigExtractor Extraction Extrait la configuration de malware connu, pour trouv\u00e9 des liste de C2s, cle d'encryption etc. CAPE Sandbox Fournit une analyse dynamique des logiciels malveillants via le sandboxing. DeobfuScripter Deobfuscation D\u00e9obfuscation de script statique. ELF Linux binaries Analyze de fichier ELF (sections, segments, ...) avec Python library LIEF ELFPARSER Linux binaries Int\u00e9gration de l'outil public ELFParser EmlParser Email Analyze de fichier email avec GOVCERT-LU eml_parser library comme les attachements, URL etc Espresso Java Extrait les classes Java, d\u00e9compile et analyze pour comportement malicieux Extract Compressed file Extrait la plus part des type de compression (like ZIP, RAR, 7z, ...) Floss IoC extraction Extrait des cha\u00eene de charracters obfusqu\u00e9 avec FireEye Labs Obfuscated String Solver FrankenStrings IoC extraction Ce service effectue des extractions de fichiers et d'IOC \u00e0 l'aide de la correspondance de mod\u00e8les, d'un d\u00e9codeur d'encodage simple et de d\u00e9sobfuscateurs de script Intezer File genome identification Interface entre Intezer Analyze API 2.0, soumet le fichier pour analyse si le hachage n'est pas pr\u00e9sent dans la base de donn\u00e9es Intezer IPArse Apple IOS Analyze de fichier Apple IOS JsJaws Javascript Analyze de fichier Javascript MetaPeek Meta data analysis D\u00e9tect les signe malicieux dans les meta-donn\u00e9es et les noms de fichier (double extension etc) Oletools Office documents Ce service analyze les fichiers Office et extrait des indicateurs de compromis avec Python library py-oletools by Philippe Lagadec Overpower PowerShell D\u00e9obfusque les fichier powershell PDFId PDF Analyze de fichier PDF avec Didier Stevens PDFId &amp; PDFParse PE Windows binaries Analyze de fichier Windows (imports, exports, sections, ...) avec LIEF PeePDF PDF Ce service utilise Python PeePDF library pour extraire de l'information de fichier PDF PixAxe Images Extrait du text des images Safelist Safelisting Permet de \"safelister\" des indicateur dans le syst\u00e8me comme des domaines, hash etc NSRL Sigma Eventlog signatures Analyze de \"Windows Event logs\" avec les r\u00e8gles Sigma Suricata Network signatures Analyze les captures r\u00e9seaux (pcap) avec Suricata Swiffer Adobe Shockwave Analyze de fichier Shockwave (.swf) TagCheck Tag signatures Signatures YARA sur les tags d'Assemblyline Tags TorrentSlicer Torrent files Analyze de fichier torrent Unpacker UPX Unpacker Extrait des executable a partir de fichier \"packer\" avec UPX Unpac.me Unpacker Int\u00e9gration avec unpac.me URLCreator Cr\u00e9ation de fichiers d'URL Cr\u00e9e des fichiers URI \u00e0 partir des tags URI apparemment malveillantes URLDownloader URL Fetching R\u00e9cup\u00e8re les URL des fichiers URI ViperMonkey Office documents ViperMonkey est un programme d'emulation pour VBA par https://linktr.ee/decalage VirusTotal Anti-virus Ce service v\u00e9rifie (et soumet \u00e9ventuellement) les fichiers/URL \u00e0 VirusTotal pour analyse. XLMMacroDeobfuscator Office documents Analyse de Macro Excel 4.0 YARA File signatures Signatures de fichier <p>En fin de vie, ne sont plus activement pris en charge:</p> Service Name Speciality Description Cuckoo Sandbox Int\u00e9gration avec la platform d'analyze dynamic Cuckoo MetaDefender Anti-virus Integration avec MetaDefender (multi-engine anti-virus) PEFile Windows binaries Analyze de fichier Windows (imports, exports, section names, ...) avec Python library pefile VirusTotalDynamic Anti-virus V\u00e9rifie et envoie activement les fichiers \u00e0 VirusTotal pour analyse. VirusTotalStatic Anti-virus V\u00e9rifie VirusTotal pour une analyse existante sur le fichier soumis."},{"location":"fr/user_manual/results/","title":"R\u00e9sultats d\u2019Assemblyline","text":""},{"location":"fr/user_manual/results/#resultats-dassemblyline","title":"R\u00e9sultats d\u2019Assemblyline","text":""},{"location":"fr/user_manual/results/#heuristiques","title":"Heuristiques","text":"<p>Par heuristique, on entend une fonction qui peut \u00eatre d\u00e9tect\u00e9e par le service dans le cadre de l\u2019analyse.</p> <p>Elle se compose de ce qui suit\u00a0:</p> <ul> <li>un identifiant;</li> <li>un nom;</li> <li>une description;</li> <li>une note (utilis\u00e9e pour signaler une heuristique comme \u00e9tant MALICIOUS [MALVEILLANTE], SUSPICIOUS [SUSPECTE] ou INFO);</li> <li>l\u2019ID de la matrice Mitre\u2019s Att&amp;ck;</li> <li>des signatures qui sont souvent utilis\u00e9es pour donner plus de contexte.</li> </ul> <p>Le syst\u00e8me fait le suivi des heuristiques pour fournir des statistiques sur le nombre d\u2019occurrences et les notes attribu\u00e9es afin d\u2019apporter des ajustements si les performances des heuristiques sont ad\u00e9quates ou insuffisantes.</p> <p></p> <p>Les heuristiques s\u2019affichent dans l\u2019interface utilisateur et sont associ\u00e9es \u00e0 des codes de couleurs bas\u00e9s sur le degr\u00e9 de malveillance.</p> <p></p>"},{"location":"fr/user_manual/results/#tags","title":"Tags","text":"<p>Les \u00e9tiquettes sont d\u2019importantes m\u00e9tadonn\u00e9es extraites d\u2019un fichier. Leur nom doit respecter la m\u00eame convention d\u2019affectation que l\u2019espace de noms, ce qui facilite leur organisation et permet de trouver plus facilement une information en particulier dans le syst\u00e8me. De plus, les \u00e9tiquettes sont index\u00e9es de mani\u00e8re \u00e0 g\u00e9n\u00e9rer des r\u00e9sultats \u00e0 une vitesse \u00e9tonnante.</p> <p><code>result.sections.tags.network.static.ip \u2192 Cette \u00e9tiquette peut trouver les adresses\u00a0IP extraites de mani\u00e8re statique peu importe le service utilis\u00e9 lors de l\u2019extraction.</code></p> <p>Toutes les \u00e9tiquettes enregistr\u00e9es dans le syst\u00e8me s\u2019affichent dans le menu Help [Aide] &gt; Searching Help [Aide \u00e0 la recherche] de votre instance d\u2019Assemblyline.</p>"},{"location":"fr/user_manual/results/#niveau-de-malveillance-score","title":"Niveau de malveillance (Score)","text":"<p>La note attribu\u00e9e \u00e0 une soumission (degr\u00e9 de malveillance) est d\u00e9termin\u00e9e en fonction de la note la plus haute ayant \u00e9t\u00e9 attribu\u00e9e aux fichiers extraits au cours du processus d\u2019analyse.</p> <p>Prenons un fichier .zip comme exemple. Le fichier .zip peut avoir obtenu une note de 0, mais s\u2019il contient deux fichiers ayant respectivement obtenus des notes de 100 et de 500, la note maximale de la soumission sera de 500. Si on pousse l\u2019analyse, il est possible de comprendre sur quoi est bas\u00e9e chacune de ces notes.</p> <p>Signification de la note (en supposant que vous ex\u00e9cutez la plupart des services, dont certains antivirus et de bonnes signatures Yara)\u00a0:</p> <pre><code>-1000: sans danger\n0 - 299: information\n300 - 699: suspicieux\n700 - 999: tr\u00e8s suspicieux\n&gt;= 1000: malicieux\n</code></pre>"},{"location":"fr/user_manual/results/#rapport-de-soumission","title":"Rapport de soumission","text":"<p>Le rapport de soumission s\u2019affiche sur la premi\u00e8re page lorsqu\u2019on consulte une soumission. Il s\u2019agit d\u2019un r\u00e9sum\u00e9 g\u00e9n\u00e9ral que l\u2019analyste peut consulter pour d\u00e9terminer s\u2019il est justifi\u00e9 de pousser l\u2019analyse plus loin.</p>"},{"location":"fr/user_manual/results/#information-generale","title":"Information g\u00e9n\u00e9rale","text":"<p>On retrouve de l\u2019information importante au haut de la fen\u00eatre\u00a0: la date et l\u2019heure, le type de fichier d\u00e9tect\u00e9, sa taille, la note maximale et des condens\u00e9s divers.</p> <p></p>"},{"location":"fr/user_manual/results/#heuristiques_1","title":"Heuristiques","text":"<p>Dans cette section, on retrouve toutes les heuristiques cat\u00e9goris\u00e9es selon leur degr\u00e9 de malveillance, ainsi que tous les fichiers connexes.</p> <p></p>"},{"location":"fr/user_manual/results/#attribution","title":"Attribution","text":"<p>Dans cette section, l\u2019attribution est effectu\u00e9e \u00e0 partir des signatures\u00a0Yara (si l\u2019\u00e9tiquette de l\u2019auteur de menace est fourni dans les m\u00e9tadonn\u00e9es de la r\u00e8gle) et les noms des virus dans l\u2019antivirus. Pour des r\u00e9sultats optimaux, il convient de mettre en pratique les r\u00e8gles Yara dict\u00e9es par les normes du CCC.</p> <p></p>"},{"location":"fr/user_manual/results/#details-de-la-soumission","title":"D\u00e9tails de la soumission","text":"<p>Le bouton Submission Details [D\u00e9tails de la soumission] est situ\u00e9 au haut du rapport de soumission.</p> <p></p> <p>Les d\u00e9tails de la soumission affichent les param\u00e8tres de la soumission, \u00e0 savoir les services s\u00e9lectionn\u00e9s au moment o\u00f9 le fichier a \u00e9t\u00e9 soumis et les m\u00e9tadonn\u00e9es de la soumission. La section la plus importante est celle qui contient le bouton.</p>"},{"location":"fr/user_manual/results/#arborescence-des-fichiers-dextraction","title":"Arborescence des fichiers d\u2019extraction","text":"<p>La section contenant l\u2019arborescence offre une vue de tous les fichiers ayant \u00e9t\u00e9 trait\u00e9s et extraits, de m\u00eame que leur note et leur type de fichier respectifs.</p> <p>En cliquant sur les fichiers, on peut r\u00e9v\u00e9ler la section la plus int\u00e9ressante d\u2019Assemblyline\u00a0: la page de d\u00e9tails des fichiers.</p> <p></p>"},{"location":"fr/user_manual/results/#details-des-fichiers","title":"D\u00e9tails des fichiers","text":"<p>La section des d\u00e9tails contient toute l\u2019information concernant un fichier en particulier. Elle ne tient pas compte de la soumission dont le fichier faisait partie.</p> <p>Dans le coin sup\u00e9rieur droit, on retrouve une s\u00e9rie de fonctions utiles.</p> Icone Decription Trouver toutes les soumissions connexes T\u00e9l\u00e9charger le fichier (lequel sera ins\u00e9r\u00e9 au format CaRT) par d\u00e9faut pour \u00e9viter une auto-infection accidentelle Visionneuse de fichiers (vue ASCII, cha\u00eenes, hexad\u00e9cimal)  Resoumettre le fichier aux fins d\u2019analyse Ajouter le fichier \u00e0 la liste s\u00fbre"},{"location":"fr/user_manual/results/#frequence-des-fichiers","title":"Fr\u00e9quence des fichiers","text":"<p>Cette section indique combien de fois ce fichier a \u00e9t\u00e9 d\u00e9tect\u00e9, ainsi que la premi\u00e8re et derni\u00e8re d\u00e9tection. Ce compte est bas\u00e9 sur la p\u00e9riode de conservation du fichier dans le syst\u00e8me.</p> <p></p>"},{"location":"fr/user_manual/results/#etiquettes-des-fichiers","title":"\u00c9tiquettes des fichiers","text":"<p>Cette section pr\u00e9sente toutes les \u00e9tiquettes regroup\u00e9es selon leur type, qui sont extraites dans ce fichier. On y retrouve l\u2019adresse\u00a0IP, l\u2019URL et plusieurs autres indicateurs de compromission (IC) que vous pouvez utiliser dans le cadre de votre enqu\u00eate ou pour lancer une action dynamique (p.\u00a0ex. mettre en place des interdictions sur vos pare-feux).</p> <p>En cliquant sur l\u2019une des \u00e9tiquettes, il est possible d\u2019afficher le service auquel elle appartient.</p> <p></p>"},{"location":"fr/user_manual/results/#resultats-du-service","title":"R\u00e9sultats du service","text":"<p>Cette section permet de consulter la sortie de chaque service, ainsi que les heuristiques et les \u00e9tiquettes observ\u00e9es. Il est \u00e9galement possible de voir les services \u00e0 l\u2019origine des \u00ab\u00a0fichiers extraits\u00a0\u00bb \u00e0 la fin des r\u00e9sultats de chaque service. Les r\u00e9sultats de fichiers mis en cache sont ignor\u00e9s chaque fois qu\u2019un service est mis \u00e0 jour. Si plusieurs versions des r\u00e9sultats sont disponibles, elles s\u2019affichent dans une liste d\u00e9roulante, ce qui permet de consulter les r\u00e9sultats des analyses pr\u00e9c\u00e9dentes.</p> <p>Pour afficher les d\u00e9tails, cliquer sur la section des r\u00e9sultats du service.</p> <p></p>"},{"location":"fr/user_manual/searching/","title":"Recherche","text":""},{"location":"fr/user_manual/searching/#recherche","title":"Recherche","text":"<p>Assemblyline tire avantage des puissantes capacit\u00e9s de recherche pratiquement infinies d'Elasticsearch.</p>"},{"location":"fr/user_manual/searching/#banque-de-documents","title":"Banque de documents","text":"<p>Les index d\u2019information sont un des principaux concepts \u00e0 approfondir. Ils permettent \u00e0 Assemblyline de d\u00e9dupliquer la plupart des r\u00e9sultats dans le syst\u00e8me, ce qui explique pourquoi l\u2019outil peut s\u2019adapter aussi efficacement. Les recherches dans les champs index\u00e9s s\u2019effectuent tr\u00e8s rapidement.</p> <p>Assemblyline dispose de six indices principaux:</p> <ul> <li> <p>Alerte: Permet aux utilisateurs d'effectuer des recherches d\u00e9taill\u00e9es sur les alertes afin d'identifier et de prioriser rapidement les incidents de s\u00e9curit\u00e9, en tenant compte de divers attributs tels que les indicateurs de menace, la classification et les horodatages.</p> </li> <li> <p>Fichier: Permet aux utilisateurs de rechercher des fichiers sp\u00e9cifiques dans une soumission, d'identifier les doublons et de recueillir des informations contextuelles sur les propri\u00e9t\u00e9s d'un fichier, telles que sa classification, son entropie et les valeurs de hachage observ\u00e9es.</p> </li> <li> <p>R\u00e9sultat: Permet aux utilisateurs de rechercher des r\u00e9sultats de service sp\u00e9cifiques, ce qui leur permet d'examiner les analyses effectu\u00e9es par divers services, y compris les scores d\u00e9taill\u00e9s, les sections et les donn\u00e9es de r\u00e9ponse.</p> </li> <li> <p>Retrohunt: Permet aux utilisateurs de rechercher les r\u00e9sultats r\u00e9trospectifs de la chasse aux menaces \u00e0 partir des r\u00e8gles Yara appliqu\u00e9es \u00e0 des \u00e9chantillons pr\u00e9c\u00e9demment soumis. Cela facilite l'identification et l'analyse des nouvelles menaces sur la base des informations actualis\u00e9es sur les menaces.</p> </li> <li> <p>Signature: Permet aux utilisateurs de rechercher des signatures sp\u00e9cifiques \u00e0 un service (par exemple, des r\u00e8gles YARA) et toutes les m\u00e9tadonn\u00e9es pertinentes, y compris la source, les statistiques, la classification et le statut.</p> </li> <li> <p>Soumission: Permet aux utilisateurs de g\u00e9rer et de suivre les soumissions, de consulter les fichiers concern\u00e9s, les erreurs d'analyse, les scores maximaux et le statut du cycle de vie de la soumission, ce qui offre une vue d'ensemble du processus d'analyse.</p> </li> </ul> <p>Vous pouvez afficher tous les index et les champs index\u00e9s connexes apr\u00e8s avoir assur\u00e9 le bon fonctionnement d\u2019Assemblyline en consultant l\u2019aide sous <code>Help &gt; Search help menu</code>.</p>"},{"location":"fr/user_manual/searching/#recherche-de-comportements-et-limitations","title":"Recherche de comportements et limitations","text":"<p>Lors d\u2019une recherche dans l\u2019interface utilisateur, la requ\u00eate est transmise dans tous les index et renvoie tous les r\u00e9sultats correspondants.</p> <p>Vous devez limiter vos crit\u00e8res de recherche \u00e0 un seul indexe. En d\u2019autres mots, vous ne pouvez pas rechercher de l\u2019information dans deux index diff\u00e9rents ou plus.</p> <p>Il est possible de contourner cette limitation en faisant appel \u00e0 l\u2019API\u00a0REST. La m\u00e9thode consiste alors \u00e0 effectuer une recherche dans un indexe, puis \u00e0 l\u2019\u00e9largir ou l\u2019affiner en recherchant ces \u00e9l\u00e9ments dans d\u2019autres index.</p>"},{"location":"fr/user_manual/searching/#exemples-de-recherche","title":"Exemples de Recherche","text":"<p>Un moyen rapide de se familiariser avec les index de recherche consiste \u00e0 utiliser l'\u00e9l\u00e9ment \"Trouver les r\u00e9sultats associ\u00e9s\" dans le menu d\u00e9roulant des balises.</p> <p></p> <p>Cliquer sur l'\u00e9l\u00e9ment \"Trouver les r\u00e9sultats associ\u00e9s\" sur la balise <code>av.virus_name</code> (<code>HEUR/Macro.Downloader.MRAA.Gen</code>) g\u00e9n\u00e9rera la requ\u00eate suivante\u00a0:</p> <pre><code>result.sections.tags.av.virus_name:\"HEUR/Macro.Downloader.MRAA.Gen\"\n</code></pre> <p>Vous pouvez \u00e9galement g\u00e9n\u00e9rer des recherches plus complexes en utilisant une syntaxe de requ\u00eate compl\u00e8te. En voici quelques exemples\u00a0:</p> <pre><code># Trouver tous les r\u00e9sultats o\u00f9 le service ViperMonkey a extrait l\u2019adresse\u00a0IP 10.10.10.10\nresult.sections.tags.network.static.ip:\"10.10.10.10\" AND response.service_name:ViperMonkey\n\n# Trouver toutes les soumissions pour lesquelles une note de 2000 ou plus a \u00e9t\u00e9 attribu\u00e9e\n# au cours des deux derniers jours\nmax_score:[2000 TO *] AND times.submitted:[now-2d TO now]\n\n# Trouver tous les r\u00e9sultats de l\u2019antivirus dont le nom de signature correspond \u00e0 Emotet\nresult.sections.tags.av.virus_name:*Emotet*\n</code></pre> <p>Le syst\u00e8me prend en charge un large \u00e9ventail de param\u00e8tres de recherche, comme les caract\u00e8res g\u00e9n\u00e9riques, les plages et les expressions r\u00e9guli\u00e8res. Vous trouverez la syntaxe compl\u00e8te dans l\u2019aide sous <code>Help &gt; Search Help</code>.</p> <p>Les requ\u00eates de recherche peuvent \u00e9galement \u00eatre utilis\u00e9es dans le client d\u2019Assemblyline pour instaurer un puissant m\u00e9canisme qui s\u2019ex\u00e9cutera automatiquement lorsque de nouveaux fichiers seront analys\u00e9s par le syst\u00e8me.</p>"},{"location":"fr/user_manual/submitting_file/","title":"Soumettre un Fichier pour Analyse","text":""},{"location":"fr/user_manual/submitting_file/#soumettre-un-fichier-pour-analyse","title":"Soumettre un Fichier pour Analyse","text":""},{"location":"fr/user_manual/submitting_file/#processus-de-soumission","title":"Processus de Soumission","text":"<p>La soumission d'un fichier pour analyse dans Assemblyline est simple via l'interface WebUI. Pour les t\u00e2ches automatis\u00e9es et les besoins d'int\u00e9gration, envisagez d'utiliser l'API REST.</p> <p></p>"},{"location":"fr/user_manual/submitting_file/#partage-et-classification","title":"Partage et Classification","text":"<p>S\u00e9lectionnez le niveau de classification souhait\u00e9 ou les restrictions de partage en cliquant sur la banni\u00e8re de classification, \u00e0 condition que votre configuration syst\u00e8me comprenne TLP ou un autre sch\u00e9ma de classification.</p>"},{"location":"fr/user_manual/submitting_file/#selection-dun-fichier-a-analyser","title":"S\u00e9lection d'un Fichier \u00e0 Analyser","text":"<p>Chargez un fichier \u00e0 analyser soit en cliquant sur le bouton \"S\u00e9lectionnez un fichier \u00e0 analyser\", soit en utilisant le glisser-d\u00e9poser dans la zone d\u00e9limit\u00e9e par des pointill\u00e9s.</p>"},{"location":"fr/user_manual/submitting_file/#choix-du-type-danalyse","title":"Choix du Type d'Analyse","text":"<p>S\u00e9lectionnez le type d'analyse le plus appropri\u00e9 pour votre fichier dans le menu d\u00e9roulant \"Type d'Analyse\".</p>"},{"location":"fr/user_manual/submitting_file/#options","title":"Options","text":"<p>Acc\u00e9dez aux options avanc\u00e9es de soumission en cliquant sur l'ic\u00f4ne \"Ajuster\" pour ouvrir le panneau \"R\u00e9glages\". En haut, une banni\u00e8re indique le niveau de privil\u00e8ges de personnalisation disponibles. Les utilisateurs ayant le r\u00f4le <code>submission_customize</code> ont la capacit\u00e9 de modifier tous les param\u00e8tres, sachant qu'ils comprennent l'impact s\u00e9v\u00e8re que certains param\u00e8tres peuvent avoir sur le syst\u00e8me s'ils sont mal utilis\u00e9s.</p>"},{"location":"fr/user_manual/submitting_file/#parametres-de-soumission","title":"Param\u00e8tres de Soumission","text":"<ul> <li>Description : Fournissez \u00e9ventuellement une description pour l'analyse, ou laissez-la vide pour accepter la valeur par d\u00e9faut d\u00e9finie par le syst\u00e8me.</li> <li>Priorit\u00e9 : D\u00e9signez la priorit\u00e9 de traitement de la soumission.</li> <li>Dur\u00e9e de vie (jours) : Sp\u00e9cifiez combien de temps (en jours) le fichier doit \u00eatre conserv\u00e9 dans le syst\u00e8me.</li> <li>G\u00e9n\u00e9ration d'alerte : D\u00e9cidez si la soumission doit d\u00e9clencher une alerte \u00e0 l'ach\u00e8vement de l'analyse.</li> <li>Ignorer les services de filtrage : Choisissez de contourner tous les services de liste blanche.</li> <li>Ignorer le cache de r\u00e9sultats : Demandez au syst\u00e8me de r\u00e9analyser le fichier, ind\u00e9pendamment de toute analyse similaire r\u00e9cente.</li> <li>Ignorer la pr\u00e9vention de r\u00e9cursion : Supprimez les limites d'it\u00e9ration pour la soumission.</li> <li>Effectuer une analyse approfondie : Engagez un d\u00e9bogage approfondi, recommand\u00e9 pour des fichiers confirm\u00e9s malveillants ou hautement suspicieux.</li> </ul>"},{"location":"fr/user_manual/submitting_file/#donnees-de-soumission","title":"Donn\u00e9es de Soumission","text":"<ul> <li>Mot de passe de d\u00e9chiffrement : Entrez rapidement un mot de passe pour les fichiers chiffr\u00e9s, \u00e9liminant le besoin de le fournir \u00e0 chaque service individuel.</li> </ul>"},{"location":"fr/user_manual/submitting_file/#parametres-de-service","title":"Param\u00e8tres de Service","text":"<ul> <li>Cat\u00e9gories de service : Choisissez un groupe pr\u00e9d\u00e9fini de services.</li> <li>Service sp\u00e9cifique : S\u00e9lectionnez manuellement des services individuels pour l'analyse.</li> <li>Param\u00e8tres de service : Ajustez les param\u00e8tres sp\u00e9cifiques \u00e0 chaque service en d\u00e9ployant leurs menus individuels.</li> </ul>"},{"location":"fr/user_manual/submitting_file/#metadonnees-de-soumission","title":"M\u00e9tadonn\u00e9es de Soumission","text":"<ul> <li>M\u00e9tadonn\u00e9es syst\u00e8me : Remplissez les champs de m\u00e9tadonn\u00e9es g\u00e9n\u00e9r\u00e9s par le syst\u00e8me requis.</li> <li>M\u00e9tadonn\u00e9es suppl\u00e9mentaires : Pour ceux ayant des capacit\u00e9s de personnalisation totales, tous les champs de m\u00e9tadonn\u00e9es suppl\u00e9mentaires sont modifiables.</li> </ul>"},{"location":"fr/user_manual/submitting_file/#comprendre-lanalyse-de-fichiers","title":"Comprendre l'Analyse de Fichiers","text":"<p>Une fois soumis, Assemblyline effectue plusieurs \u00e9valuations pour d\u00e9terminer le chemin d'analyse optimal. Sa capacit\u00e9 d'analyse r\u00e9cursive est particuli\u00e8rement efficace pour d\u00e9cortiquer les couches d'obscurcissement souvent trouv\u00e9es dans les logiciels malveillants, r\u00e9v\u00e9lant finalement des scripts en clair ou des versions d\u00e9compress\u00e9es qui peuvent \u00eatre plus facilement identifi\u00e9es par les solutions antivirus traditionnelles.</p> <p></p>"},{"location":"fr/user_manual/submitting_sha256/","title":"Soumission d'un SHA256 pour Analyse","text":""},{"location":"fr/user_manual/submitting_sha256/#soumission-dun-sha256-pour-analyse","title":"Soumission d'un SHA256 pour Analyse","text":""},{"location":"fr/user_manual/submitting_sha256/#comment-soumettre-un-sha256","title":"Comment Soumettre un SHA256","text":"<p>Le processus de soumission d'un hash SHA256 est tr\u00e8s similaire \u00e0 celui d'une URL et peut \u00eatre r\u00e9alis\u00e9 directement via l'interface utilisateur Web d'Assemblyline. Pour ceux qui souhaitent automatiser le processus ou l'int\u00e9grer \u00e0 d'autres syst\u00e8mes, l'API REST est disponible. Il suffit de naviguer jusqu'\u00e0 l'onglet \"URL/SHA256\" pour commencer.</p> <p></p>"},{"location":"fr/user_manual/submitting_sha256/#niveau-de-partage-et-classification","title":"Niveau de Partage et Classification","text":"<p>Utilisez la banni\u00e8re de Classification pour attribuer le niveau de partage appropri\u00e9, tel que le Protocole de Feux de Circulation (TLP) ou un autre sch\u00e9ma de classification, \u00e0 votre soumission.</p>"},{"location":"fr/user_manual/submitting_sha256/#soumettre-un-hash-sha256","title":"Soumettre un Hash SHA256","text":"<p>Entrez le hash SHA256 que vous souhaitez analyser dans la zone de texte \"URL/SHA256 \u00e0 scanner\" et cliquez sur \"SCAN\" pour lancer le processus de soumission.</p>"},{"location":"fr/user_manual/submitting_sha256/#notes-importantes-sur-les-soumissions-de-sha256","title":"Notes Importantes sur les Soumissions de SHA256","text":"<p>Lorsque vous soumettez un hash SHA256, Assemblyline v\u00e9rifie si le hash correspond \u00e0 un fichier pr\u00e9c\u00e9demment rencontr\u00e9. Si un fichier correspondant est trouv\u00e9, il est resoumis pour une analyse plus approfondie. Dans le cas contraire, si le fichier n'est pas trouv\u00e9 dans le stockage local, Assemblyline tentera de le localiser sur une source externe, telle que Malware Bazaar, et le soumettra pour analyse s'il est trouv\u00e9.</p> <p>Ce comportement d\u00e9pend de votre configuration de d\u00e9ploiement \u2014 consultez <code>submission.file_sources</code> pour les d\u00e9tails de configuration.</p>"},{"location":"fr/user_manual/submitting_sha256/#options","title":"Options","text":"<p>Acc\u00e9dez \u00e0 des options de soumission avanc\u00e9es en cliquant sur l'ic\u00f4ne \"Ajuster\" pour ouvrir le panneau \"Options\". En haut, une banni\u00e8re indique le niveau de privil\u00e8ges de personnalisation disponibles. Les utilisateurs ayant le r\u00f4le <code>submission_customize</code> ont la capacit\u00e9 de modifier tous les param\u00e8tres, \u00e0 condition qu'ils comprennent l'impact s\u00e9v\u00e8re que certains param\u00e8tres peuvent avoir sur le syst\u00e8me s'ils sont mal utilis\u00e9s.</p>"},{"location":"fr/user_manual/submitting_sha256/#parametres-de-soumission","title":"Param\u00e8tres de Soumission","text":"<ul> <li>Description : Fournissez \u00e9ventuellement une description pour l'analyse, ou laissez-la vide pour accepter la valeur par d\u00e9faut d\u00e9finie par le syst\u00e8me.</li> <li>Priorit\u00e9 : D\u00e9signez la priorit\u00e9 de traitement de la soumission.</li> <li>Dur\u00e9e de vie (jours) : Sp\u00e9cifiez combien de temps (en jours) le fichier doit \u00eatre conserv\u00e9 dans le syst\u00e8me.</li> <li>G\u00e9n\u00e9rer une alerte : D\u00e9cidez si la soumission doit d\u00e9clencher une alerte \u00e0 la fin de l'analyse.</li> <li>Ignorer les services de filtrage : Choisissez de contourner tous les services de liste blanche.</li> <li>Ignorer le cache des r\u00e9sultats : Demandez au syst\u00e8me de r\u00e9analyser le fichier, sans tenir compte d'une analyse similaire r\u00e9cente.</li> <li>Ignorer la pr\u00e9vention de r\u00e9cursion : Supprimez les limites d'it\u00e9ration pour la soumission.</li> <li>Effectuer une analyse approfondie : Proc\u00e9dez \u00e0 une d\u00e9sobfuscation pouss\u00e9e, recommand\u00e9e pour des fichiers confirm\u00e9s malveillants ou hautement suspects.</li> </ul>"},{"location":"fr/user_manual/submitting_sha256/#donnees-de-soumission","title":"Donn\u00e9es de Soumission","text":"<ul> <li>Mot de passe de d\u00e9chiffrement : Entrez rapidement un mot de passe pour les fichiers chiffr\u00e9s, \u00e9liminant le besoin de le fournir \u00e0 chaque service individuel.</li> </ul>"},{"location":"fr/user_manual/submitting_sha256/#parametres-des-services","title":"Param\u00e8tres des Services","text":"<ul> <li>Cat\u00e9gories de service : Choisissez un groupe pr\u00e9d\u00e9fini de services.</li> <li>Service sp\u00e9cifique : S\u00e9lectionnez manuellement des services individuels pour l'analyse.</li> <li>Param\u00e8tres des services : Ajustez pr\u00e9cis\u00e9ment les param\u00e8tres sp\u00e9cifiques \u00e0 chaque service en d\u00e9veloppant leurs menus individuels.</li> </ul>"},{"location":"fr/user_manual/submitting_sha256/#metadonnees-de-soumission","title":"M\u00e9tadonn\u00e9es de Soumission","text":"<ul> <li>M\u00e9tadonn\u00e9es syst\u00e8me : Remplissez les champs de m\u00e9tadonn\u00e9es g\u00e9n\u00e9r\u00e9s par le syst\u00e8me requis.</li> <li>M\u00e9tadonn\u00e9es suppl\u00e9mentaires : Pour ceux ayant des capacit\u00e9s de personnalisation compl\u00e8tes, tous les champs de m\u00e9tadonn\u00e9es suppl\u00e9mentaires sont modifiables.</li> </ul>"},{"location":"fr/user_manual/submitting_url/","title":"Soumission d'une URL pour Analyse","text":""},{"location":"fr/user_manual/submitting_url/#soumission-dune-url-pour-analyse","title":"Soumission d'une URL pour Analyse","text":""},{"location":"fr/user_manual/submitting_url/#comment-soumettre-une-url","title":"Comment Soumettre une URL","text":"<p>La soumission d'une URL \u00e0 Assemblyline pour une analyse peut \u00eatre effectu\u00e9e directement par le biais de l'interface WebUI, tout comme la soumission d'un fichier. Pour l'automatisation et l'int\u00e9gration avec d'autres syst\u00e8mes, utilisez l'API REST.</p> <p></p>"},{"location":"fr/user_manual/submitting_url/#niveau-de-partage-et-de-classification","title":"Niveau de Partage et de Classification","text":"<p>Si un syst\u00e8me de classification tel que le Protocole TLP (Traffic Light Protocol) ou un syst\u00e8me similaire est configur\u00e9 sur votre plateforme, vous pouvez s\u00e9lectionner les restrictions de partage appropri\u00e9es \u00e0 l'aide de la banni\u00e8re de Classification.</p>"},{"location":"fr/user_manual/submitting_url/#details-de-la-soumission-durl","title":"D\u00e9tails de la Soumission d'URL","text":"<p>Entrez l'URL que vous souhaitez analyser dans la zone de texte \"URL/SHA256 \u00e0 scanner\" et cliquez sur \"SCAN\".</p>"},{"location":"fr/user_manual/submitting_url/#informations-importantes-concernant-les-soumissions-durl","title":"Informations Importantes Concernant les Soumissions d'URL","text":"<p>La soumission d'une URL cr\u00e9e un fichier URI qui sert de point d'entr\u00e9e pour l'analyse. Assemblyline peut \u00e9galement interagir avec des ressources externes, comme r\u00e9cup\u00e9rer une charge utile de seconde phase \u00e0 partir d'un lien HTTP/HTTPS trouv\u00e9 dans un fichier malveillant. Pour r\u00e9ussir la r\u00e9cup\u00e9ration d'un URI, des services pertinents tels que le service URLDownloader doivent \u00eatre s\u00e9lectionn\u00e9s.</p>"},{"location":"fr/user_manual/submitting_url/#structure-de-fichier-uri","title":"Structure de Fichier URI","text":"<p>Un fichier URI est un fichier YAML avec une structure de base comme celle-ci :</p> <pre><code># Fichier URI Assemblyline\nuri: &lt;scheme&gt;://&lt;host&gt;\n</code></pre> <p>C'est un fichier YAML qui peut contenir plus d'\u00e9l\u00e9ments pour des cas d'utilisation o\u00f9 les services peuvent les exploiter. Les parties les plus importantes sont la cl\u00e9 \"uri\" dans le fichier YAML, qui doit \u00eatre une URI valide avec un sch\u00e9ma et un h\u00f4te, et le commentaire en haut, pour aider \u00e0 l'identification. Le sch\u00e9ma sera utilis\u00e9 pour cr\u00e9er le type de fichier ; donc, si vous utilisez <code>uri: http://site.com</code>, cela sera un fichier de type <code>uri/http</code> et si vous utilisez <code>uri: ftp://site.com</code>, alors vous aurez un <code>uri/ftp</code>. Cela permettra de diriger vers diff\u00e9rents services en fonction du sch\u00e9ma. Dans le cas de <code>uri/ftp</code>, vous aurez probablement besoin de plus d'informations, telles que des cl\u00e9s YAML comme <code>passif:True</code>.</p> <p>Voici un exemple plus complet et complexe d'un fichier URI :</p> <pre><code># Fichier URI Assemblyline\nuri: https://mb-api.abuse.ch/api/v1/\ndata: query=get_info&amp;hash=52307f9ce784496218f2165be83c2486ad809da98026166b871dc279d40a4d1f\nheaders:\n  Content-Type: application/x-www-form-urlencoded\nmethod: POST\n</code></pre> <p>Le type de fichier serait <code>uri/https</code> et les autres cl\u00e9s YAML seront ignor\u00e9es lors de l'identification. Les cl\u00e9s suppl\u00e9mentaires dans le fichier YAML peuvent \u00eatre utilis\u00e9es par le service qui traite ce fichier sp\u00e9cifique pour fournir un comportement plus personnalis\u00e9, plus proche de ce que l'utilisateur demande. Un user-agent sp\u00e9cifique, un referer ou d'autres en-t\u00eates pourraient \u00eatre utilis\u00e9s pour r\u00e9cup\u00e9rer une seconde \u00e9tape d'un serveur qui n\u00e9cessiterait des valeurs sp\u00e9cifiques. Gr\u00e2ce \u00e0 ces valeurs suppl\u00e9mentaires, URLDownloader prend d\u00e9sormais en charge des m\u00e9thodes plus vari\u00e9es comme POST. Un changement simple de <code>query=get_info</code> \u00e0 <code>query=get_file</code> dans les donn\u00e9es et le service devrait \u00eatre en train de t\u00e9l\u00e9charger ce fichier depuis MalwareBazaar !</p> <p>\u00c9tant donn\u00e9 que les fichiers URI sont tr\u00e8s sp\u00e9cifiques \u00e0 Assemblyline, nous prenons le temps de r\u00e9\u00e9crire tout fichier entrant pour que le commentaire <code># Fichier URI Assemblyline</code> soit sur la premi\u00e8re ligne, suivi de la cl\u00e9 uri, puis de toutes les autres cl\u00e9s dans l'ordre alphab\u00e9tique. Ceci est fait pour d\u00e9dupliquer les fichiers \"identiques\" et utiliser la mise en cache. Une cl\u00e9 comme <code>extra_key: [\"premier\", \"deuxi\u00e8me\", \"troisi\u00e8me\", \"quatri\u00e8me\"]</code> conservera son ordre et sera convertie comme suit :</p> <pre><code>extra_key:\n- premier\n- deuxi\u00e8me\n- troisi\u00e8me\n- quatri\u00e8me\n</code></pre>"},{"location":"fr/user_manual/submitting_url/#fichiers-uri-et-proxies","title":"Fichiers URI et Proxies","text":"<p>Actuellement, chaque service qui n\u00e9cessite de passer par un proxy doit \u00eatre configur\u00e9 par un administrateur. URLDownloader prend en charge les proxies et peut m\u00eame \u00eatre configur\u00e9 pour permettre \u00e0 un utilisateur de choisir parmi plusieurs proxies configur\u00e9s. \u00c0 l'avenir, nous envisagerons de normaliser cette fonctionnalit\u00e9 afin que tous les services puissent utiliser une s\u00e9lection centrale de proxy.</p> <p>Cela est important car si vous avez une URL h\u00e9bergeant un logiciel malveillant et que vous ne souhaitez pas exposer votre syst\u00e8me Assemblyline \u00e0 ce serveur, alors nous vous recommandons de configurer un serveur proxy pour agir comme interm\u00e9diaire entre votre infrastructure Assemblyline et le serveur h\u00e9bergeant le logiciel malveillant. Vous pouvez configurer cela dans la configuration de votre d\u00e9ploiement sous le composant <code>ui</code>. L'\u00e9l\u00e9ment que vous recherchez est <code>url_submission_proxies</code>.</p>"},{"location":"fr/user_manual/submitting_url/#options","title":"Options","text":"<p>Acc\u00e9dez \u00e0 des options avanc\u00e9es de soumission en cliquant sur l'ic\u00f4ne \"Ajuster\" pour ouvrir le panneau \"R\u00e9glages\". En haut, une banni\u00e8re indique le niveau de privil\u00e8ges de personnalisation disponibles pour vous. Les utilisateurs avec le r\u00f4le <code>submission_customize</code> ont la capacit\u00e9 de modifier tous les param\u00e8tres, \u00e0 condition qu'ils comprennent l'impact s\u00e9v\u00e8re que certains param\u00e8tres peuvent avoir sur le syst\u00e8me s'ils sont mal utilis\u00e9s.</p>"},{"location":"fr/user_manual/submitting_url/#parametres-de-soumission","title":"Param\u00e8tres de Soumission","text":"<ul> <li>Description : Fournissez \u00e9ventuellement une description pour l'analyse, ou laissez-la vide pour accepter la valeur par d\u00e9faut d\u00e9finie par le syst\u00e8me.</li> <li>Priorit\u00e9 : D\u00e9signez la priorit\u00e9 de traitement de la soumission.</li> <li>Dur\u00e9e de vie (jours) : Sp\u00e9cifiez combien de temps (en jours) le fichier doit \u00eatre conserv\u00e9 dans le syst\u00e8me.</li> <li>G\u00e9n\u00e9rer une alerte : D\u00e9cidez si la soumission doit d\u00e9clencher une alerte \u00e0 la fin de l'analyse.</li> <li>Ignorer les services de filtrage : Choisissez de passer outre les services de liste blanche.</li> <li>Ignorer le cache de r\u00e9sultats : Demandez au syst\u00e8me de r\u00e9analyser le fichier, en ignorant toute analyse similaire r\u00e9cente.</li> <li>Ignorer la pr\u00e9vention de r\u00e9cursion : Supprimez les limites d'it\u00e9ration pour la soumission.</li> <li>Effectuer une analyse approfondie : Engagez une d\u00e9sobfuscation compl\u00e8te, recommand\u00e9e pour des fichiers confirm\u00e9s malveillants ou hautement suspects.</li> </ul>"},{"location":"fr/user_manual/submitting_url/#donnees-de-soumission","title":"Donn\u00e9es de Soumission","text":"<ul> <li>Mot de passe de d\u00e9chiffrement : Entrez rapidement un mot de passe pour les fichiers chiffr\u00e9s, \u00e9vitant la n\u00e9cessit\u00e9 de le fournir \u00e0 chaque service individuellement.</li> </ul>"},{"location":"fr/user_manual/submitting_url/#parametres-des-services","title":"Param\u00e8tres des Services","text":"<ul> <li>Cat\u00e9gories de services : Choisissez un groupe pr\u00e9d\u00e9fini de services.</li> <li>Service sp\u00e9cifique : S\u00e9lectionnez manuellement des services individuels pour l'analyse.</li> <li>Param\u00e8tres des services : Ajustez pr\u00e9cis\u00e9ment les param\u00e8tres sp\u00e9cifiques \u00e0 chaque service en d\u00e9veloppant leurs menus individuels.</li> </ul>"},{"location":"fr/user_manual/submitting_url/#metadonnees-de-soumission","title":"M\u00e9tadonn\u00e9es de Soumission","text":"<ul> <li>M\u00e9tadonn\u00e9es syst\u00e8me : Remplissez les champs m\u00e9tadonn\u00e9es g\u00e9n\u00e9r\u00e9s par le syst\u00e8me comme requis.</li> <li>M\u00e9tadonn\u00e9es suppl\u00e9mentaires : Pour ceux poss\u00e9dant des capacit\u00e9s de personnalisation totales, tous les champs de m\u00e9tadonn\u00e9es suppl\u00e9mentaires sont modifiables.</li> </ul>"}]}